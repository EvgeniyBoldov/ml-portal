
# ===== __init__.py =====

# ===== app/celery_app.py =====
from __future__ import annotations
import os
from celery import Celery
from kombu import Queue

BROKER_URL = os.getenv("CELERY_BROKER_URL") or "redis://redis:6379/0"
RESULT_BACKEND = os.getenv("CELERY_RESULT_BACKEND") or "redis://redis:6379/1"

app = Celery(
    "backend",
    broker=BROKER_URL,
    backend=RESULT_BACKEND,
    include=[
        "app.tasks.normalize",
        "app.tasks.chunk",
        "app.tasks.embed",
        "app.tasks.index",
        "app.tasks.analyze",
        "app.tasks.upload_watch",
        "app.tasks.ocr_tables",
        "app.tasks.chat",
        "app.tasks.embedding_worker",
        "app.services.embedding_dispatcher",
    ],
)

# Определение очередей с приоритетами
app.conf.task_queues = (
    # Критический приоритет - чат
    Queue('chat_critical', routing_key='chat_critical', priority=10),
    
    # Высокий приоритет - загрузка файлов
    Queue('upload_high', routing_key='upload_high', priority=8),
    
    # Средний приоритет - анализ документов
    Queue('analyze_medium', routing_key='analyze_medium', priority=5),
    Queue('ocr_medium', routing_key='ocr_medium', priority=5),
    
    # Низкий приоритет - RAG индексация
    Queue('rag_low', routing_key='rag_low', priority=2),
    Queue('cleanup_low', routing_key='cleanup_low', priority=1),
    
    # Очереди эмбеддингов (динамически создаются)
    Queue('embed.dispatch', routing_key='embed.dispatch', priority=8),
    Queue('embed.minilm.rt', routing_key='embed.minilm.rt', priority=8),
    Queue('embed.minilm.bulk', routing_key='embed.minilm.bulk', priority=3),
)

# Маршрутизация задач по очередям
app.conf.task_routes = {
    # Критический приоритет - чат
    "app.tasks.chat.*": {"queue": "chat_critical", "priority": 10},
    
    # Высокий приоритет - загрузка и нормализация
    "app.tasks.upload_watch.*": {"queue": "upload_high", "priority": 8},
    "app.tasks.normalize.normalize": {"queue": "upload_high", "priority": 8},
    "app.tasks.chunk.split": {"queue": "upload_high", "priority": 8},
    
    # Средний приоритет - анализ документов
    "app.tasks.analyze.*": {"queue": "analyze_medium", "priority": 5},
    "app.tasks.ocr_tables.*": {"queue": "ocr_medium", "priority": 5},
    
    # Низкий приоритет - RAG индексация
    "app.tasks.embed.*": {"queue": "rag_low", "priority": 2},
    "app.tasks.index.*": {"queue": "rag_low", "priority": 2},
    "app.tasks.delete.*": {"queue": "cleanup_low", "priority": 1},
    
    # Эмбеддинги
    "embedding_dispatcher.dispatch": {"queue": "embed.dispatch", "priority": 8},
    "embedding_worker.process_embedding": {"queue": "embed.minilm.rt", "priority": 8},
    "embedding_worker.health_check": {"queue": "embed.minilm.rt", "priority": 8},
}

app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    task_acks_late=True,
    worker_prefetch_multiplier=1,
    broker_heartbeat=20,
    broker_pool_limit=10,
    task_time_limit=60 * 60,
    task_soft_time_limit=55 * 60,
)

if os.getenv("BEAT") == "1":
    app.conf.beat_schedule = {
        "dummy-housekeeping-5m": {
            "task": "app.tasks.index.housekeeping",
            "schedule": 300.0,
        }
    }

# Инициализация задач эмбеддингов
from app.tasks.embedding_worker import create_embedding_worker_tasks
from app.services.embedding_dispatcher import create_dispatcher_tasks

# Регистрируем задачи
create_embedding_worker_tasks(app)
create_dispatcher_tasks(app)

# ===== app/__init__.py =====
# app package init

# ===== app/cli.py =====
#!/usr/bin/env python3
"""
CLI commands for the ML Portal application.
"""
import argparse
import sys
from typing import Optional
from sqlalchemy.orm import Session

from app.core.db import get_session
from app.core.security import hash_password
from app.repositories.users_repo import UsersRepo


def create_superuser(login: str, password: str, email: Optional[str] = None) -> None:
    """Create a superuser account."""
    session = next(get_session())
    repo = UsersRepo(session)
    
    try:
        # Check if user already exists
        existing_user = repo.by_login(login)
        if existing_user:
            print(f"❌ Error: User with login '{login}' already exists")
            sys.exit(1)
        
        # Create superuser
        password_hash = hash_password(password)
        user = repo.create_user(
            login=login,
            password_hash=password_hash,
            role="admin",
            email=email,
            is_active=True
        )
        
        print(f"✅ Superuser created successfully!")
        print(f"   Login: {user.login}")
        print(f"   Role: {user.role}")
        print(f"   Email: {user.email or 'Not set'}")
        print(f"   ID: {user.id}")
        
    except Exception as e:
        print(f"❌ Error creating superuser: {e}")
        sys.exit(1)
    finally:
        session.close()


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(description="ML Portal CLI")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Create superuser command
    create_superuser_parser = subparsers.add_parser(
        "create-superuser",
        help="Create a superuser account"
    )
    create_superuser_parser.add_argument(
        "--login",
        required=True,
        help="Login username"
    )
    create_superuser_parser.add_argument(
        "--password",
        required=True,
        help="Password (min 12 characters)"
    )
    create_superuser_parser.add_argument(
        "--email",
        help="Email address (optional)"
    )
    
    args = parser.parse_args()
    
    if args.command == "create-superuser":
        if len(args.password) < 12:
            print("❌ Error: Password must be at least 12 characters long")
            sys.exit(1)
        
        create_superuser(
            login=args.login,
            password=args.password,
            email=args.email
        )
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

# ===== app/main.py =====
from __future__ import annotations
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy import text
from app.core.metrics import prometheus_endpoint
from app.core.logging import setup_logging
from app.core.errors import install_exception_handlers
from app.core.config import settings
from app.core.db import engine
from app.core.redis import get_redis
from app.core.qdrant import get_qdrant
from app.core.s3 import get_minio
from app.core.idempotency import IdempotencyMiddleware
from app.core.request_id import RequestIDMiddleware
from app.core.security_headers import SecurityHeadersMiddleware
from app.api.routers.auth import router as auth_router
from app.api.routers.chats import router as chats_router
from app.api.routers.rag import router as rag_router
from app.api.routers.analyze import router as analyze_router
from app.api.routers.admin import router as admin_router
from app.api.routers.password_reset import router as password_reset_router

setup_logging()

app = FastAPI(title="API")

app.add_middleware(RequestIDMiddleware)
app.add_middleware(SecurityHeadersMiddleware, environment=os.getenv("ENVIRONMENT", "development"))
app.add_middleware(IdempotencyMiddleware)

if os.getenv("CORS_ENABLED", "1") not in {"0", "false", "False"}:
    origins = [o.strip() for o in os.getenv("CORS_ORIGINS", "*").split(",")]
    # В DEV разрешаем все origins, в PROD - только явно указанные
    is_dev = os.getenv("ENVIRONMENT", "development") == "development"
    
    if is_dev or origins == ["*"]:
        # DEV режим - разрешаем все origins, но без credentials
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=False,
            allow_methods=["*"],
            allow_headers=["*"],
        )
    else:
        # PROD режим - только указанные origins с credentials
        app.add_middleware(
            CORSMiddleware,
            allow_origins=origins,
            allow_credentials=True,
            allow_methods=["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"],
            allow_headers=["Authorization", "Content-Type", "X-Request-ID"],
        )

install_exception_handlers(app)

@app.get("/healthz")
@app.get("/health")  # Алиас для совместимости с тестами
async def healthz(deep: int | None = None):
    if settings.HEALTH_DEEP or deep == 1:
        try:
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            await get_redis().ping()
            get_qdrant().get_collections()
            get_minio().list_buckets()
        except Exception as e:
            return {"ok": False, "error": str(e)}
    return {"ok": True}

@app.get("/metrics")
def metrics():
    return prometheus_endpoint()

@app.get("/api/rag/metrics")
def rag_metrics():
    from app.api.deps import db_session
    from sqlalchemy import func
    from app.models.rag import RagDocuments, RagChunks
    
    session = next(db_session())
    try:
        # Подсчитываем документы по статусам
        status_counts = session.query(
            RagDocuments.status,
            func.count(RagDocuments.id)
        ).group_by(RagDocuments.status).all()
        
        # Общее количество документов
        total_documents = session.query(func.count(RagDocuments.id)).scalar()
        
        # Количество чанков
        total_chunks = session.query(func.count(RagChunks.id)).scalar()
        
        # Количество документов в обработке
        processing_documents = session.query(func.count(RagDocuments.id)).filter(
            RagDocuments.status.in_(['uploaded', 'normalizing', 'chunking', 'embedding', 'indexing'])
        ).scalar()
        
        # Размер хранилища (приблизительно)
        storage_size = session.query(func.sum(RagDocuments.size_bytes)).scalar() or 0
        
        return {
            "total_documents": total_documents,
            "total_chunks": total_chunks,
            "processing_documents": processing_documents,
            "storage_size_bytes": storage_size,
            "storage_size_mb": round(storage_size / (1024 * 1024), 2),
            "status_breakdown": {status: count for status, count in status_counts},
            "ready_documents": next((count for status, count in status_counts if status == 'ready'), 0),
            "error_documents": next((count for status, count in status_counts if status == 'error'), 0)
        }
    finally:
        session.close()

app.include_router(auth_router, prefix="/api")
app.include_router(chats_router, prefix="/api")
app.include_router(rag_router, prefix="/api")
app.include_router(analyze_router, prefix="/api")
app.include_router(admin_router)
app.include_router(password_reset_router)

# ===== tests/conftest.py =====
import pytest
import sys
import os

# Add the app directory to Python path
sys.path.insert(0, '/app')

from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.main import app
from app.core.db import get_db, Base
from app.core.config import settings

# Test database URL
SQLALCHEMY_DATABASE_URL = "sqlite:///./test.db"

engine = create_engine(
    SQLALCHEMY_DATABASE_URL, 
    connect_args={"check_same_thread": False}
)
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def override_get_db():
    try:
        db = TestingSessionLocal()
        yield db
    finally:
        db.close()

@pytest.fixture(scope="session")
def db_engine():
    """Create test database engine"""
    Base.metadata.create_all(bind=engine)
    yield engine
    Base.metadata.drop_all(bind=engine)

@pytest.fixture
def db_session(db_engine):
    """Create test database session"""
    connection = db_engine.connect()
    transaction = connection.begin()
    session = TestingSessionLocal(bind=connection)
    
    yield session
    
    session.close()
    transaction.rollback()
    connection.close()

@pytest.fixture
def client(db_session):
    """Create test client with database session override"""
    app.dependency_overrides[get_db] = lambda: db_session
    with TestClient(app) as test_client:
        yield test_client
    app.dependency_overrides.clear()

@pytest.fixture
def test_user():
    """Create test user data"""
    return {
        "id": "test-user-id",
        "email": "test@example.com",
        "role": "user"
    }

@pytest.fixture
def test_chat():
    """Create test chat data"""
    return {
        "id": "test-chat-id",
        "name": "Test Chat",
        "tags": ["test", "example"],
        "owner_id": "test-user-id"
    }

@pytest.fixture
def test_rag_document():
    """Create test RAG document data"""
    return {
        "id": "test-doc-id",
        "name": "test.pdf",
        "status": "ready",
        "tags": ["document", "test"]
    }

# ===== tests/test_chats.py =====
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, MagicMock

class TestChatsAPI:
    """Test cases for chats API endpoints"""
    
    def test_create_chat_success(self, client: TestClient, test_user):
        """Test successful chat creation"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            response = client.post("/api/chats", json={
                "name": "Test Chat",
                "tags": ["test", "example"]
            })
            
            assert response.status_code == 200
            data = response.json()
            assert "chat_id" in data
            assert isinstance(data["chat_id"], str)
    
    def test_create_chat_without_name(self, client: TestClient, test_user):
        """Test chat creation without name"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            response = client.post("/api/chats", json={
                "tags": ["test"]
            })
            
            assert response.status_code == 200
            data = response.json()
            assert "chat_id" in data
    
    def test_create_chat_invalid_tags(self, client: TestClient, test_user):
        """Test chat creation with invalid tags"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            response = client.post("/api/chats", json={
                "name": "Test Chat",
                "tags": ["a" * 100]  # Tag too long
            })
            
            assert response.status_code == 422  # Validation error
    
    def test_list_chats(self, client: TestClient, test_user):
        """Test listing chats"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            response = client.get("/api/chats")
            
            assert response.status_code == 200
            data = response.json()
            assert "items" in data
            assert isinstance(data["items"], list)
    
    def test_update_chat_tags(self, client: TestClient, test_user, test_chat):
        """Test updating chat tags"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            # First create a chat
            create_response = client.post("/api/chats", json={
                "name": "Test Chat",
                "tags": ["old"]
            })
            chat_id = create_response.json()["chat_id"]
            
            # Update tags
            response = client.put(f"/api/chats/{chat_id}/tags", json={
                "tags": ["new", "updated"]
            })
            
            assert response.status_code == 200
            data = response.json()
            assert data["tags"] == ["new", "updated"]
    
    def test_send_message(self, client: TestClient, test_user, test_chat):
        """Test sending message to chat"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            # Create a chat first
            create_response = client.post("/api/chats", json={"name": "Test Chat"})
            chat_id = create_response.json()["chat_id"]
            
            # Mock LLM response
            with patch('app.services.clients.llm_chat', return_value="Test response"):
                response = client.post(f"/api/chats/{chat_id}/messages", json={
                    "content": "Hello, world!",
                    "use_rag": False,
                    "response_stream": False
                })
                
                assert response.status_code == 200
                data = response.json()
                assert "content" in data
                assert data["content"] == "Test response"
    
    def test_send_message_stream(self, client: TestClient, test_user):
        """Test sending message with streaming response"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            # Create a chat first
            create_response = client.post("/api/chats", json={"name": "Test Chat"})
            chat_id = create_response.json()["chat_id"]
            
            # Mock streaming response
            async def mock_stream():
                yield "Test "
                yield "streaming "
                yield "response"
            
            with patch('app.api.routers.chats._stream_llm_response', return_value=mock_stream()):
                response = client.post(f"/api/chats/{chat_id}/messages", json={
                    "content": "Hello, world!",
                    "use_rag": False,
                    "response_stream": True
                })
                
                assert response.status_code == 200
                assert response.headers["content-type"] == "text/event-stream; charset=utf-8"
    
    def test_delete_chat(self, client: TestClient, test_user):
        """Test deleting a chat"""
        with patch('app.api.deps.get_current_user', return_value=test_user):
            # Create a chat first
            create_response = client.post("/api/chats", json={"name": "Test Chat"})
            chat_id = create_response.json()["chat_id"]
            
            # Delete the chat
            response = client.delete(f"/api/chats/{chat_id}")
            
            assert response.status_code == 200
            data = response.json()
            assert data["deleted"] is True

# ===== tests/test_rbac.py =====
import pytest
from fastapi.testclient import TestClient
from sqlalchemy.orm import Session
from unittest.mock import patch

from app.main import app
from app.core.security import hash_password
from app.repositories.users_repo import UsersRepo
from app.schemas.admin import UserRole


@pytest.fixture
def client():
    return TestClient(app)


@pytest.fixture
def admin_user(session: Session):
    """Create admin user for testing."""
    repo = UsersRepo(session)
    user = repo.create_user(
        login="admin",
        password_hash=hash_password("admin123456"),
        role="admin",
        email="admin@test.com",
        is_active=True
    )
    return user


@pytest.fixture
def editor_user(session: Session):
    """Create editor user for testing."""
    repo = UsersRepo(session)
    user = repo.create_user(
        login="editor",
        password_hash=hash_password("editor123456"),
        role="editor",
        email="editor@test.com",
        is_active=True
    )
    return user


@pytest.fixture
def reader_user(session: Session):
    """Create reader user for testing."""
    repo = UsersRepo(session)
    user = repo.create_user(
        login="reader",
        password_hash=hash_password("reader123456"),
        role="reader",
        email="reader@test.com",
        is_active=True
    )
    return user


def get_auth_headers(client: TestClient, login: str, password: str):
    """Get authorization headers for a user."""
    response = client.post("/api/auth/login", json={"login": login, "password": password})
    assert response.status_code == 200
    token = response.json()["access_token"]
    return {"Authorization": f"Bearer {token}"}


class TestRBAC:
    """Test RBAC functionality."""
    
    def test_admin_can_access_all_endpoints(self, client: TestClient, admin_user):
        """Test that admin can access all endpoints."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        # Test admin endpoints
        response = client.get("/api/admin/users", headers=headers)
        assert response.status_code == 200
        
        # Test RAG endpoints
        response = client.get("/api/rag/", headers=headers)
        assert response.status_code == 200
        
        response = client.get("/api/rag/stats", headers=headers)
        assert response.status_code == 200
    
    def test_editor_can_access_rag_write_operations(self, client: TestClient, editor_user):
        """Test that editor can access RAG write operations."""
        headers = get_auth_headers(client, "editor", "editor123456")
        
        # Test RAG read operations
        response = client.get("/api/rag/", headers=headers)
        assert response.status_code == 200
        
        response = client.get("/api/rag/stats", headers=headers)
        assert response.status_code == 200
        
        # Test RAG search
        response = client.post("/api/rag/search", json={"query": "test"}, headers=headers)
        assert response.status_code == 200
    
    def test_editor_cannot_access_admin_endpoints(self, client: TestClient, editor_user):
        """Test that editor cannot access admin endpoints."""
        headers = get_auth_headers(client, "editor", "editor123456")
        
        response = client.get("/api/admin/users", headers=headers)
        assert response.status_code == 403
    
    def test_reader_can_access_read_operations(self, client: TestClient, reader_user):
        """Test that reader can access read operations."""
        headers = get_auth_headers(client, "reader", "reader123456")
        
        # Test RAG read operations
        response = client.get("/api/rag/", headers=headers)
        assert response.status_code == 200
        
        response = client.get("/api/rag/stats", headers=headers)
        assert response.status_code == 200
        
        # Test RAG search
        response = client.post("/api/rag/search", json={"query": "test"}, headers=headers)
        assert response.status_code == 200
    
    def test_reader_cannot_access_write_operations(self, client: TestClient, reader_user):
        """Test that reader cannot access write operations."""
        headers = get_auth_headers(client, "reader", "reader123456")
        
        # Test RAG upload (should fail)
        with open("test_file.txt", "w") as f:
            f.write("test content")
        
        try:
            with open("test_file.txt", "rb") as f:
                response = client.post("/api/rag/upload", files={"file": f}, headers=headers)
                assert response.status_code == 403
        finally:
            import os
            if os.path.exists("test_file.txt"):
                os.remove("test_file.txt")
    
    def test_reader_cannot_access_admin_endpoints(self, client: TestClient, reader_user):
        """Test that reader cannot access admin endpoints."""
        headers = get_auth_headers(client, "reader", "reader123456")
        
        response = client.get("/api/admin/users", headers=headers)
        assert response.status_code == 403
    
    def test_inactive_user_cannot_access_anything(self, client: TestClient, session: Session):
        """Test that inactive user cannot access anything."""
        repo = UsersRepo(session)
        user = repo.create_user(
            login="inactive",
            password_hash=hash_password("inactive123456"),
            role="admin",
            is_active=False
        )
        
        headers = get_auth_headers(client, "inactive", "inactive123456")
        
        # Should get 401 for any endpoint
        response = client.get("/api/rag/", headers=headers)
        assert response.status_code == 401


class TestAdminAPI:
    """Test admin API functionality."""
    
    def test_create_user(self, client: TestClient, admin_user):
        """Test creating a new user."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        user_data = {
            "login": "newuser",
            "password": "newuser123456",
            "role": "reader",
            "email": "newuser@test.com"
        }
        
        response = client.post("/api/admin/users", json=user_data, headers=headers)
        assert response.status_code == 201
        
        data = response.json()
        assert data["login"] == "newuser"
        assert data["role"] == "reader"
        assert data["email"] == "newuser@test.com"
        assert data["is_active"] is True
    
    def test_list_users(self, client: TestClient, admin_user, editor_user, reader_user):
        """Test listing users."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        response = client.get("/api/admin/users", headers=headers)
        assert response.status_code == 200
        
        data = response.json()
        assert data["total"] >= 3  # At least our test users
        assert len(data["users"]) >= 3
    
    def test_get_user(self, client: TestClient, admin_user, editor_user):
        """Test getting a specific user."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        response = client.get(f"/api/admin/users/{editor_user.id}", headers=headers)
        assert response.status_code == 200
        
        data = response.json()
        assert data["id"] == str(editor_user.id)
        assert data["login"] == "editor"
        assert data["role"] == "editor"
    
    def test_update_user(self, client: TestClient, admin_user, editor_user):
        """Test updating a user."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        update_data = {
            "role": "reader",
            "is_active": False
        }
        
        response = client.patch(f"/api/admin/users/{editor_user.id}", json=update_data, headers=headers)
        assert response.status_code == 200
        
        data = response.json()
        assert data["role"] == "reader"
        assert data["is_active"] is False
    
    def test_reset_user_password(self, client: TestClient, admin_user, editor_user):
        """Test resetting user password."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        response = client.post(f"/api/admin/users/{editor_user.id}/password", json={}, headers=headers)
        assert response.status_code == 200
        
        data = response.json()
        assert "new_password" in data
        assert len(data["new_password"]) > 0
    
    def test_create_pat_token(self, client: TestClient, admin_user, editor_user):
        """Test creating a PAT token."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        token_data = {
            "name": "test-token",
            "scopes": ["api:read", "rag:read"]
        }
        
        response = client.post(f"/api/admin/users/{editor_user.id}/tokens", json=token_data, headers=headers)
        assert response.status_code == 201
        
        data = response.json()
        assert data["name"] == "test-token"
        assert "token_plain_once" in data
        assert data["scopes"] == ["api:read", "rag:read"]
    
    def test_list_user_tokens(self, client: TestClient, admin_user, editor_user):
        """Test listing user tokens."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        # Create a token first
        token_data = {"name": "test-token"}
        client.post(f"/api/admin/users/{editor_user.id}/tokens", json=token_data, headers=headers)
        
        response = client.get(f"/api/admin/users/{editor_user.id}/tokens", headers=headers)
        assert response.status_code == 200
        
        data = response.json()
        assert data["total"] >= 1
        assert len(data["tokens"]) >= 1
    
    def test_audit_logs(self, client: TestClient, admin_user, editor_user):
        """Test audit logs functionality."""
        headers = get_auth_headers(client, "admin", "admin123456")
        
        # Perform some actions that should be logged
        client.patch(f"/api/admin/users/{editor_user.id}", json={"role": "reader"}, headers=headers)
        
        response = client.get("/api/admin/audit-logs", headers=headers)
        assert response.status_code == 200
        
        data = response.json()
        assert data["total"] >= 1
        assert len(data["logs"]) >= 1

# ===== tests/__init__.py =====
# Tests package

# ===== tests/test_text_extractor.py =====
import io
from app.services.text_extractor import extract_text
from app.services.text_normalizer import normalize_text

def test_txt_basic():
    data = "Привет, мир!\nСтрока 2".encode("utf-8")
    res = extract_text(data, "note.txt")
    assert "Привет" in res.text
    assert res.kind.startswith("txt")

def test_csv_basic():
    data = b"a,b,c\n1,2,3\n4,5,6\n"
    res = extract_text(data, "table.csv")
    assert "1\t2\t3" in res.text

def test_normalize_hyphen_wrap():
    raw = "слово-\nперенос"
    norm = normalize_text(raw)
    assert norm == "словореренос" or "слово перенос" in norm  # depending on rules

# ===== tests/test_tz_compliance.py =====
"""
Тесты соответствия техническому заданию
"""
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch
import json

from app.main import app
from app.core.config import settings

client = TestClient(app)

class TestTZCompliance:
    """Тесты соответствия ТЗ"""
    
    def test_role_check_constraint(self):
        """Тест CHECK constraint для роли пользователя"""
        # Проверяем, что миграция создала правильный constraint
        from app.models.user import Users
        from sqlalchemy import text
        
        # В реальном тесте нужно подключиться к БД и проверить constraint
        # Пока проверяем, что модель определена правильно
        assert hasattr(Users, '__table_args__')
        constraints = Users.__table_args__
        assert any('ck_users_role' in str(constraint) for constraint in constraints)
    
    def test_cursor_pagination_format(self):
        """Тест cursor-based пагинации"""
        # Проверяем схему ответа
        from app.schemas.admin import UserListResponse
        
        # Схема должна содержать cursor поля
        fields = UserListResponse.__fields__
        assert 'has_more' in fields
        assert 'next_cursor' in fields
        assert 'users' in fields
        assert 'total' in fields
    
    def test_error_format_standardization(self):
        """Тест стандартизации формата ошибок"""
        # Проверяем, что все ошибки имеют стандартный формат
        response = client.get("/api/admin/users", headers={"Authorization": "Bearer invalid"})
        
        # Должен быть стандартный формат ошибки
        assert response.status_code in [401, 403]
        data = response.json()
        
        # Проверяем структуру ошибки
        if "error" in data:
            assert "code" in data["error"]
            assert "message" in data["error"]
        if "request_id" in data:
            assert isinstance(data["request_id"], str)
    
    def test_password_algorithm_argon2id(self):
        """Тест использования Argon2id для паролей"""
        from app.core.security import hash_password, verify_password
        
        password = "TestPassword123!"
        hash1 = hash_password(password)
        hash2 = hash_password(password)
        
        # Хеши должны быть разными (соль)
        assert hash1 != hash2
        
        # Оба должны верифицироваться
        assert verify_password(password, hash1)
        assert verify_password(password, hash2)
        
        # Неправильный пароль не должен верифицироваться
        assert not verify_password("WrongPassword123!", hash1)
    
    def test_refresh_token_rotation(self):
        """Тест ротации refresh токенов"""
        from app.core.config import settings
        
        # Проверяем, что ротация включена по умолчанию
        assert settings.REFRESH_ROTATING == True
        
        # Проверяем TTL настройки
        assert settings.ACCESS_TTL_SECONDS == 900  # 15 минут
        assert settings.REFRESH_TTL_DAYS == 7
    
    def test_cookie_auth_configuration(self):
        """Тест конфигурации cookie аутентификации"""
        # Проверяем, что настройки cookie auth присутствуют
        assert hasattr(settings, 'AUTH_MODE')
        assert hasattr(settings, 'COOKIE_AUTH_ENABLED')
        assert hasattr(settings, 'CSRF_ENABLED')
        
        # По умолчанию должен быть bearer режим
        assert settings.AUTH_MODE == "bearer"
        assert settings.COOKIE_AUTH_ENABLED == False
        assert settings.CSRF_ENABLED == False
    
    def test_reader_upload_permission(self):
        """Тест разрешения загрузок для reader"""
        # Проверяем настройку
        assert hasattr(settings, 'ALLOW_READER_UPLOADS')
        assert settings.ALLOW_READER_UPLOADS == False  # По умолчанию отключено
    
    def test_soft_delete_behavior(self):
        """Тест мягкого удаления пользователей"""
        # Проверяем, что есть endpoint для удаления
        response = client.delete("/api/admin/users/nonexistent", 
                               headers={"Authorization": "Bearer admin_token"})
        
        # Должен возвращать 404 для несуществующего пользователя
        # В реальном тесте нужно создать пользователя и проверить мягкое удаление
        assert response.status_code in [401, 404]  # 401 из-за отсутствия валидного токена
    
    def test_admin_metrics_availability(self):
        """Тест доступности метрик админ операций"""
        from app.core.metrics import (
            admin_operations_total,
            admin_user_operations_total,
            admin_token_operations_total,
            rate_limit_hits_total,
            auth_attempts_total
        )
        
        # Проверяем, что метрики определены
        assert admin_operations_total is not None
        assert admin_user_operations_total is not None
        assert admin_token_operations_total is not None
        assert rate_limit_hits_total is not None
        assert auth_attempts_total is not None
    
    def test_openapi_schemas(self):
        """Тест OpenAPI схем"""
        response = client.get("/openapi.json")
        assert response.status_code == 200
        
        openapi_spec = response.json()
        
        # Проверяем, что админ API присутствует
        paths = openapi_spec.get("paths", {})
        admin_paths = [path for path in paths.keys() if "/api/admin" in path]
        assert len(admin_paths) > 0
        
        # Проверяем схемы
        schemas = openapi_spec.get("components", {}).get("schemas", {})
        assert "UserResponse" in schemas
        assert "UserListResponse" in schemas
        assert "TokenResponse" in schemas
        assert "AuditLogResponse" in schemas
    
    def test_password_reset_security(self):
        """Тест безопасности password reset"""
        # Тестируем, что всегда возвращается 200
        test_cases = [
            "nonexistent@example.com",
            "invalid-email",
            "nonexistent_user"
        ]
        
        for test_case in test_cases:
            response = client.post("/auth/password/forgot", json={
                "login_or_email": test_case
            })
            assert response.status_code == 200
    
    def test_pat_scope_validation(self):
        """Тест валидации PAT scopes"""
        from app.core.pat_validation import validate_scopes, check_scope_permission
        
        # Валидные scopes
        valid_scopes = ["api:read", "rag:write", "chat:admin"]
        validated = validate_scopes(valid_scopes)
        
        assert "api:read" in validated
        assert "rag:write" in validated
        assert "chat:admin" in validated
        assert "chat:read" in validated  # Должен быть расширен
        assert "chat:write" in validated  # Должен быть расширен
        
        # Проверка разрешений
        user_scopes = ["api:admin"]
        assert check_scope_permission(user_scopes, "api:read")
        assert check_scope_permission(user_scopes, "api:write")
        assert not check_scope_permission(user_scopes, "chat:read")
    
    def test_cors_configuration(self):
        """Тест CORS конфигурации"""
        # Проверяем настройки
        assert hasattr(settings, 'CORS_ENABLED')
        assert hasattr(settings, 'CORS_ORIGINS')
        assert hasattr(settings, 'CORS_ALLOW_CREDENTIALS')
        
        # Тестируем OPTIONS запрос
        response = client.options("/api/auth/login")
        assert "access-control-allow-origin" in response.headers
    
    def test_sse_heartbeat_configuration(self):
        """Тест конфигурации SSE heartbeat"""
        from app.api.sse import sse_heartbeat_response
        
        # Тестируем создание SSE ответа
        response = sse_heartbeat_response(heartbeat_interval=1)
        assert response.media_type == "text/event-stream"
        assert "Cache-Control" in response.headers
        assert response.headers["Cache-Control"] == "no-cache"
    
    def test_audit_logging_structure(self):
        """Тест структуры audit logging"""
        from app.services.audit_service import AuditService
        
        # Проверяем методы
        assert hasattr(AuditService, 'log_action')
        assert hasattr(AuditService, 'log_user_action')
        assert hasattr(AuditService, 'log_token_action')
        assert hasattr(AuditService, 'log_auth_action')
    
    def test_rate_limiting_configuration(self):
        """Тест конфигурации rate limiting"""
        # Проверяем настройки
        assert hasattr(settings, 'RATE_LIMIT_LOGIN_ATTEMPTS')
        assert hasattr(settings, 'RATE_LIMIT_LOGIN_WINDOW')
        
        assert settings.RATE_LIMIT_LOGIN_ATTEMPTS == 10
        assert settings.RATE_LIMIT_LOGIN_WINDOW == 60
    
    def test_password_policy_configuration(self):
        """Тест конфигурации политики паролей"""
        # Проверяем настройки
        assert hasattr(settings, 'PASSWORD_MIN_LENGTH')
        assert hasattr(settings, 'PASSWORD_REQUIRE_UPPERCASE')
        assert hasattr(settings, 'PASSWORD_REQUIRE_LOWERCASE')
        assert hasattr(settings, 'PASSWORD_REQUIRE_DIGITS')
        assert hasattr(settings, 'PASSWORD_REQUIRE_SPECIAL')
        assert hasattr(settings, 'PASSWORD_PEPPER')
        
        # Проверяем значения по умолчанию
        assert settings.PASSWORD_MIN_LENGTH == 12
        assert settings.PASSWORD_REQUIRE_UPPERCASE == True
        assert settings.PASSWORD_REQUIRE_LOWERCASE == True
        assert settings.PASSWORD_REQUIRE_DIGITS == True
        assert settings.PASSWORD_REQUIRE_SPECIAL == True
        assert len(settings.PASSWORD_PEPPER) > 0

if __name__ == "__main__":
    pytest.main([__file__])

# ===== tests/test_rag.py =====
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, MagicMock
import io

class TestRAGAPI:
    """Test cases for RAG API endpoints"""
    
    def test_upload_rag_file_success(self, client: TestClient):
        """Test successful RAG file upload"""
        # Create a test file
        test_file = io.BytesIO(b"Test document content")
        test_file.name = "test.txt"
        
        with patch('app.core.s3_helpers.put_object'), \
             patch('app.tasks.upload_watch.watch'):
            
            response = client.post(
                "/api/rag/upload",
                files={"file": ("test.txt", test_file, "text/plain")},
                data={"tags": '["test", "document"]'}
            )
            
            assert response.status_code == 200
            data = response.json()
            assert "id" in data
            assert data["status"] == "uploaded"
            assert data["tags"] == ["test", "document"]
    
    def test_upload_rag_file_invalid_type(self, client: TestClient):
        """Test upload with invalid file type"""
        test_file = io.BytesIO(b"Test content")
        test_file.name = "test.exe"  # Invalid extension
        
        response = client.post(
            "/api/rag/upload",
            files={"file": ("test.exe", test_file, "application/octet-stream")}
        )
        
        assert response.status_code == 400
        assert "Unsupported file type" in response.json()["detail"]
    
    def test_upload_rag_file_too_large(self, client: TestClient):
        """Test upload with file too large"""
        # Create a large file (simulate)
        large_content = b"x" * (51 * 1024 * 1024)  # 51MB
        test_file = io.BytesIO(large_content)
        test_file.name = "large.txt"
        test_file.size = 51 * 1024 * 1024
        
        response = client.post(
            "/api/rag/upload",
            files={"file": ("large.txt", test_file, "text/plain")}
        )
        
        assert response.status_code == 413
        assert "File too large" in response.json()["detail"]
    
    def test_list_rag_documents(self, client: TestClient):
        """Test listing RAG documents"""
        response = client.get("/api/rag/?page=1&size=10")
        
        assert response.status_code == 200
        data = response.json()
        assert "items" in data
        assert "pagination" in data
        assert isinstance(data["items"], list)
    
    def test_list_rag_documents_with_filters(self, client: TestClient):
        """Test listing RAG documents with filters"""
        response = client.get("/api/rag/?page=1&size=10&status=ready&search=test")
        
        assert response.status_code == 200
        data = response.json()
        assert "items" in data
        assert "pagination" in data
    
    def test_get_rag_document(self, client: TestClient):
        """Test getting specific RAG document"""
        # First upload a document
        test_file = io.BytesIO(b"Test document content")
        test_file.name = "test.txt"
        
        with patch('app.core.s3_helpers.put_object'), \
             patch('app.tasks.upload_watch.watch'):
            
            upload_response = client.post(
                "/api/rag/upload",
                files={"file": ("test.txt", test_file, "text/plain")}
            )
            
            doc_id = upload_response.json()["id"]
            
            # Get the document
            response = client.get(f"/api/rag/{doc_id}")
            
            assert response.status_code == 200
            data = response.json()
            assert data["id"] == doc_id
    
    def test_update_rag_document_tags(self, client: TestClient):
        """Test updating RAG document tags"""
        # First upload a document
        test_file = io.BytesIO(b"Test document content")
        test_file.name = "test.txt"
        
        with patch('app.core.s3_helpers.put_object'), \
             patch('app.tasks.upload_watch.watch'):
            
            upload_response = client.post(
                "/api/rag/upload",
                files={"file": ("test.txt", test_file, "text/plain")}
            )
            
            doc_id = upload_response.json()["id"]
            
            # Update tags
            response = client.put(f"/api/rag/{doc_id}/tags", json=["new", "tags"])
            
            assert response.status_code == 200
            data = response.json()
            assert data["tags"] == ["new", "tags"]
    
    def test_search_rag(self, client: TestClient):
        """Test RAG search with real implementation"""
        # Mock only the external dependencies
        with patch('app.services.clients.embed_texts', return_value=[[0.1] * 8]), \
             patch('app.services.clients.qdrant_search', return_value=[
                 {
                     "score": 0.9,
                     "id": "chunk1",
                     "payload": {
                         "text": "Test content",
                         "document_id": "doc1",
                         "chunk_idx": 0,
                         "tags": ["test"]
                     }
                 }
             ]):
            
            response = client.post("/api/rag/search", json={
                "text": "test query",
                "top_k": 10
            })
            
            assert response.status_code == 200
            data = response.json()
            assert "items" in data
            assert len(data["items"]) == 1
            assert data["items"][0]["score"] == 0.9
            assert data["items"][0]["text"] == "Test content"
    
    def test_rag_metrics(self, client: TestClient):
        """Test RAG metrics endpoint"""
        response = client.get("/api/rag/metrics")
        
        assert response.status_code == 200
        data = response.json()
        assert "total_documents" in data
        assert "total_chunks" in data
        assert "storage_size_bytes" in data
        assert isinstance(data["total_documents"], int)
    
    def test_archive_rag_document(self, client: TestClient):
        """Test archiving RAG document"""
        # First upload a document
        test_file = io.BytesIO(b"Test document content")
        test_file.name = "test.txt"
        
        with patch('app.core.s3_helpers.put_object'), \
             patch('app.tasks.upload_watch.watch'):
            
            upload_response = client.post(
                "/api/rag/upload",
                files={"file": ("test.txt", test_file, "text/plain")}
            )
            
            doc_id = upload_response.json()["id"]
            
            # Archive the document
            response = client.post(f"/api/rag/{doc_id}/archive")
            
            assert response.status_code == 200
            data = response.json()
            assert data["status"] == "archived"
    
    def test_delete_rag_document(self, client: TestClient):
        """Test deleting RAG document"""
        # First upload a document
        test_file = io.BytesIO(b"Test document content")
        test_file.name = "test.txt"
        
        with patch('app.core.s3_helpers.put_object'), \
             patch('app.tasks.upload_watch.watch'), \
             patch('app.core.s3.get_minio'):
            
            upload_response = client.post(
                "/api/rag/upload",
                files={"file": ("test.txt", test_file, "text/plain")}
            )
            
            doc_id = upload_response.json()["id"]
            
            # Delete the document
            response = client.delete(f"/api/rag/{doc_id}")
            
            assert response.status_code == 200
            data = response.json()
            assert data["deleted"] is True

# ===== adapters/llm_proxy.py =====
"""
LLM Proxy - заглушка для тестирования
Имитирует работу реального LLM сервиса
"""
import os
import time
import random
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="LLM Proxy", version="0.1.0")

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: int = 1000

class ChatResponse(BaseModel):
    content: str
    usage: Dict[str, int]

# Заглушка для ответов
MOCK_RESPONSES = [
    "Привет! Как дела? Чем могу помочь?",
    "Интересный вопрос! Давайте разберем его подробнее.",
    "Понимаю вашу проблему. Вот что я думаю по этому поводу...",
    "Отличная идея! Это действительно может сработать.",
    "Хм, это сложный вопрос. Нужно подумать...",
    "Я готов помочь вам с этим вопросом!",
    "Спасибо за вопрос! Вот мой ответ...",
    "Это очень важная тема. Давайте обсудим её детально.",
]

def generate_mock_response(messages: List[ChatMessage]) -> str:
    """Генерирует заглушку ответа на основе сообщений"""
    if not messages:
        return "Привет! Как дела?"
    
    last_message = messages[-1].content.lower()
    
    # Простые паттерны для более реалистичных ответов
    if "привет" in last_message or "hello" in last_message:
        return "Привет! Как дела? Чем могу помочь?"
    elif "как дела" in last_message:
        return "У меня все отлично! А у вас как дела?"
    elif "спасибо" in last_message:
        return "Пожалуйста! Рад был помочь!"
    elif "?" in last_message:
        return "Интересный вопрос! Вот что я думаю по этому поводу..."
    else:
        return random.choice(MOCK_RESPONSES)

@app.get("/healthz")
async def healthz():
    """Health check endpoint"""
    return {"status": "ok", "mode": "mock", "target": "localhost"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """Chat endpoint - заглушка"""
    try:
        # Имитируем задержку обработки
        delay = random.uniform(0.5, 2.0)
        time.sleep(delay)
        
        # Генерируем ответ
        content = generate_mock_response(request.messages)
        
        # Имитируем токены
        token_count = len(content.split()) + random.randint(10, 50)
        
        return ChatResponse(
            content=content,
            usage={
                "prompt_tokens": sum(len(msg.content.split()) for msg in request.messages),
                "completion_tokens": token_count,
                "total_tokens": token_count + sum(len(msg.content.split()) for msg in request.messages)
            }
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Mock LLM error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8002))
    uvicorn.run(app, host="0.0.0.0", port=port)

# ===== adapters/__init__.py =====
# Adapters package

# ===== scripts/create_admin_hash.py =====
#!/usr/bin/env python3
"""
Скрипт для создания хеша пароля admin123
"""
import sys
from pathlib import Path

# Добавляем путь к приложению
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.security import hash_password

if __name__ == "__main__":
    password = "admin123"
    hashed = hash_password(password)
    print(f"Password: {password}")
    print(f"Hash: {hashed}")


# ===== scripts/create_superuser.py =====
#!/usr/bin/env python3
"""
Script to create a superuser account.
Usage: python scripts/create_superuser.py --login admin --password 'secure_password123'
"""
import sys
import os
import argparse

# Add the parent directory to the path so we can import from app
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from app.core.db import get_session
from app.core.security import hash_password
from app.repositories.users_repo import UsersRepo


def create_superuser(login: str, password: str, email: str = None):
    """Create a superuser account."""
    session = next(get_session())
    repo = UsersRepo(session)
    
    try:
        # Check if user already exists
        existing_user = repo.by_login(login)
        if existing_user:
            print(f"❌ Error: User with login '{login}' already exists")
            return False
        
        # Validate password
        if len(password) < 12:
            print("❌ Error: Password must be at least 12 characters long")
            return False
        
        # Create superuser
        password_hash = hash_password(password)
        user = repo.create_user(
            login=login,
            password_hash=password_hash,
            role="admin",
            email=email,
            is_active=True
        )
        
        print(f"✅ Superuser created successfully!")
        print(f"   Login: {user.login}")
        print(f"   Role: {user.role}")
        print(f"   Email: {user.email or 'Not set'}")
        print(f"   ID: {user.id}")
        
        return True
        
    except Exception as e:
        print(f"❌ Error creating superuser: {e}")
        return False
    finally:
        session.close()


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Create a superuser account")
    parser.add_argument("--login", required=True, help="Login username")
    parser.add_argument("--password", required=True, help="Password (min 12 characters)")
    parser.add_argument("--email", help="Email address (optional)")
    
    args = parser.parse_args()
    
    success = create_superuser(
        login=args.login,
        password=args.password,
        email=args.email
    )
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

# ===== scripts/bootstrap_models_bucket.py =====
#!/usr/bin/env python3
"""
Скрипт для создания бакета моделей в MinIO
"""
import os
import boto3
from botocore.exceptions import ClientError

def create_models_bucket():
    """Создает бакет для моделей в MinIO"""
    
    # Настройки MinIO
    endpoint_url = os.getenv("S3_ENDPOINT", "http://localhost:9000")
    access_key = os.getenv("S3_ACCESS_KEY", "minioadmin")
    secret_key = os.getenv("S3_SECRET_KEY", "minioadmin")
    bucket_name = os.getenv("MODELS_BUCKET", "models")
    
    try:
        # Создаем S3 клиент
        s3_client = boto3.client(
            's3',
            endpoint_url=endpoint_url,
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key
        )
        
        # Проверяем, существует ли бакет
        try:
            s3_client.head_bucket(Bucket=bucket_name)
            print(f"Bucket '{bucket_name}' already exists")
            return True
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                # Бакет не существует, создаем
                pass
            else:
                raise
        
        # Создаем бакет
        s3_client.create_bucket(Bucket=bucket_name)
        print(f"Created bucket '{bucket_name}'")
        
        # Создаем тестовую структуру
        test_structure = [
            "sentence-transformers/all-MiniLM-L6-v2/default/config.json",
            "sentence-transformers/all-MiniLM-L6-v2/default/pytorch_model.bin",
            "sentence-transformers/all-MiniLM-L6-v2/default/tokenizer.json",
            "sentence-transformers/all-MiniLM-L6-v2/default/tokenizer_config.json",
            "sentence-transformers/all-MiniLM-L6-v2/default/vocab.txt",
        ]
        
        for key in test_structure:
            # Создаем пустой файл для тестирования
            s3_client.put_object(
                Bucket=bucket_name,
                Key=key,
                Body=b'{}'  # Пустой JSON
            )
            print(f"Created test object: {key}")
        
        print(f"Models bucket '{bucket_name}' initialized successfully")
        return True
        
    except Exception as e:
        print(f"Failed to create models bucket: {e}")
        return False

if __name__ == "__main__":
    create_models_bucket()

# ===== scripts/__init__.py =====

# ===== scripts/seed_users.py =====
# backend/scripts/seed_users.py
import os, sys
from datetime import datetime, timezone
from sqlalchemy import create_engine, MetaData, select, insert, update
from sqlalchemy.exc import SQLAlchemyError

DB_URL = os.environ.get("DB_URL") or os.environ.get("DB.URL")
if not DB_URL:
    print("seed_users.py: DB_URL is required", file=sys.stderr); sys.exit(2)

ADMIN_LOGIN = os.environ.get("ADMIN_LOGIN", "admin")
ADMIN_PASSWORD = os.environ.get("ADMIN_PASSWORD", "admin123")
USER_LOGIN = os.environ.get("USER_LOGIN", "user")
USER_PASSWORD = os.environ.get("USER_PASSWORD", "user123")

try:
    from passlib.hash import bcrypt
    def hash_pw(pw: str) -> str: return bcrypt.hash(pw)
except Exception:
    def hash_pw(pw: str) -> str: return pw  # plaintext fallback

engine = create_engine(DB_URL, future=True, pool_pre_ping=True)
md = MetaData()

with engine.begin() as conn:
    # No create_all here — we rely on Alembic migrations.
    md.reflect(conn)
    if "users" not in md.tables:
        print("seed_users.py: users table not found — run migrations first.", file=sys.stderr)
        sys.exit(3)
    users_tbl = md.tables["users"]

    def col(name, alt=None):
        c = users_tbl.c.get(name)
        if c is None and alt:
            c = users_tbl.c.get(alt)
        if c is None:
            raise RuntimeError(f"seed_users.py: column not found: {name}")
        return c

    login_col = col("login", "username")
    pwd_col = users_tbl.c.get("password_hash") or users_tbl.c.get("password") or users_tbl.c.get("hashed_password")
    if pwd_col is None:
        print("seed_users.py: cannot identify password column", file=sys.stderr); sys.exit(4)
    is_active_col = users_tbl.c.get("is_active")
    role_col = users_tbl.c.get("role")
    fio_col = users_tbl.c.get("fio") or users_tbl.c.get("full_name") or users_tbl.c.get("name")

    def upsert_user(login: str, password: str, is_admin: bool, fio: str | None):
        existing = conn.execute(select(users_tbl).where(login_col == login)).first()
        if existing:
            upd = {pwd_col.key: hash_pw(password)}
            if is_active_col is not None: upd[is_active_col.key] = True
            if role_col is not None and is_admin: upd[role_col.key] = "admin"
            conn.execute(update(users_tbl).where(login_col == login).values(**upd))
        else:
            ins = {login_col.key: login, pwd_col.key: hash_pw(password)}
            if is_active_col is not None: ins[is_active_col.key] = True
            if role_col is not None: ins[role_col.key] = ("admin" if is_admin else "reader")
            if fio_col is not None: ins[fio_col.key] = fio
            conn.execute(insert(users_tbl).values(**ins))

    try:
        upsert_user(ADMIN_LOGIN, ADMIN_PASSWORD, True, "Admin")
        upsert_user(USER_LOGIN, USER_PASSWORD, False, "User")
        print("seed_users.py: users ready")
    except SQLAlchemyError as e:
        print(f"seed_users.py: error: {e}", file=sys.stderr); sys.exit(5)

# ===== scripts/bootstrap_qdrant.py =====
from app.core.qdrant import get_qdrant

def main():
    q = get_qdrant()
    c = q.get_collections()
    print("Qdrant reachable. Collections:", getattr(c, 'collections', c))

if __name__ == "__main__":
    main()

# ===== scripts/bootstrap_minio.py =====
from app.core.s3 import ensure_bucket
from app.core.config import settings

def main():
    for b in (settings.S3_BUCKET_RAG, settings.S3_BUCKET_ANALYSIS):
        ensure_bucket(b)
        print(f"ensured bucket: {b}")

if __name__ == "__main__":
    main()

# ===== scripts/run_migrations.py =====
# backend/scripts/run_migrations.py
"""Programmatic Alembic upgrade to head.
Use: python scripts/run_migrations.py
Requires: ALEMBIC_CONFIG (optional) or uses app/migrations as script_location.
"""
import os, sys
from alembic import command
from alembic.config import Config

here = os.path.dirname(os.path.abspath(__file__))
# We expect migrations under app/migrations
cfg = Config()
cfg.set_main_option("script_location", "app/migrations")
# DB URL read by env.py via app.core.config.settings, so no need to set here.
command.upgrade(cfg, "head")
print("Migrations applied: head")

# ===== tests/api/__init__.py =====

# ===== tests/api/test_rag_progress.py =====
from fastapi.testclient import TestClient
from app.main import app

def test_rag_progress_endpoint(monkeypatch):
    from app.services import rag_service
    def fake_progress(session, doc_id):
        return {"id": doc_id, "status": "indexing", "chunks_total": 10, "vectors_total": 8, "updated_at": None}
    monkeypatch.setattr(rag_service, "progress", fake_progress)
    client = TestClient(app)
    r = client.get("/api/rag/123/progress")
    assert r.status_code == 200
    body = r.json()
    assert body["id"] == "123" and body["status"] == "indexing"

# ===== tests/api/test_rag_search_next_offset.py =====
from fastapi.testclient import TestClient
from app.main import app

def test_rag_search_next_offset(monkeypatch):
    from app.services import rag_service
    def fake_search(session, query, top_k, offset=0, doc_id=None, tags=None, sort_by="score_desc"):
        return {"results":[{"score":0.5,"text":"hello","doc_id":"d1","chunk_idx":0,"tags":["a"]}], "next_offset": offset+1}
    monkeypatch.setattr(rag_service, "search", fake_search)
    client = TestClient(app)
    r = client.post("/api/rag/search", json={"query":"q","top_k":1,"offset":0})
    assert r.status_code == 200
    body = r.json()
    assert "next_offset" in body and body["next_offset"] == 1

# ===== tests/api/test_rag_search_payload.py =====
from fastapi.testclient import TestClient
from app.main import app

def test_rag_search_payload_shape(monkeypatch):
    from app.services import rag_service
    def fake_search(session, query, top_k, offset=0, doc_id=None, tags=None, sort_by="score_desc"):
        return {"results":[{"score":0.5,"text":"hello","doc_id":"d1","chunk_idx":0,"tags":["a"]}], "next_offset": offset+1}
    monkeypatch.setattr(rag_service, "search", fake_search)
    client = TestClient(app)
    r = client.post("/api/rag/search", json={"query":"q","top_k":3,"offset":1,"doc_id":"d1","tags":["a"]})
    assert r.status_code == 200
    body = r.json()
    assert "results" in body and body["results"][0]["doc_id"] == "d1"

# ===== tests/e2e/test_full_system.py =====
#!/usr/bin/env python3
"""
Комплексные E2E тесты всей системы ML Portal
"""
import asyncio
import json
import time
import uuid
from pathlib import Path
from typing import Dict, Any, List
import httpx
import pytest

# Конфигурация тестов
API_BASE_URL = "http://localhost:8000"
TEST_TIMEOUT = 300  # 5 минут на весь тест

class TestFullSystem:
    """Комплексные тесты всей системы"""
    
    @pytest.fixture(autouse=True)
    async def setup(self):
        """Настройка перед каждым тестом"""
        self.client = httpx.AsyncClient(timeout=30.0)
        self.test_data = {
            "chat_id": None,
            "document_id": None,
            "analysis_id": None,
            "tags": []
        }
        yield
        await self.client.aclose()
    
    async def wait_for_condition(self, check_func, timeout: int = 60, interval: int = 2):
        """Ждет выполнения условия с таймаутом"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                if await check_func():
                    return True
            except Exception as e:
                print(f"Condition check failed: {e}")
            await asyncio.sleep(interval)
        return False
    
    async def test_chat_workflow(self):
        """Тест полного workflow чата"""
        print("\n🧪 Тестирование workflow чата...")
        
        # 1. Создание чата
        print("1. Создание чата...")
        chat_data = {
            "title": f"Test Chat {uuid.uuid4().hex[:8]}",
            "tags": ["test", "e2e"]
        }
        
        response = await self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
        assert response.status_code == 200
        chat = response.json()
        self.test_data["chat_id"] = chat["id"]
        print(f"✅ Чат создан: {chat['id']}")
        
        # 2. Отправка сообщения
        print("2. Отправка сообщения...")
        message_data = {
            "content": "Привет! Как дела? Расскажи что-нибудь интересное.",
            "use_rag": False
        }
        
        response = await self.client.post(
            f"{API_BASE_URL}/api/chats/{chat['id']}/messages", 
            json=message_data
        )
        assert response.status_code == 200
        message = response.json()
        print(f"✅ Сообщение отправлено: {message['id']}")
        
        # 3. Получение ответа (стрим)
        print("3. Получение ответа...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/{chat['id']}/messages/{message['id']}/stream")
        assert response.status_code == 200
        
        # Читаем стрим
        content = ""
        async for line in response.aiter_lines():
            if line.startswith("data: "):
                data = line[6:]
                if data.strip() == "[DONE]":
                    break
                try:
                    chunk = json.loads(data)
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta = chunk["choices"][0].get("delta", {})
                        chunk_content = delta.get("content", "")
                        if chunk_content:
                            content += chunk_content
                except:
                    continue
        
        assert len(content) > 0
        print(f"✅ Ответ получен: {len(content)} символов")
        
        # 4. Добавление тега
        print("4. Добавление тега...")
        tag_data = {"tag": "important"}
        response = await self.client.post(f"{API_BASE_URL}/api/chats/{chat['id']}/tags", json=tag_data)
        assert response.status_code == 200
        print("✅ Тег добавлен")
        
        # 5. Переименование чата
        print("5. Переименование чата...")
        new_title = f"Renamed Chat {uuid.uuid4().hex[:8]}"
        update_data = {"title": new_title}
        response = await self.client.put(f"{API_BASE_URL}/api/chats/{chat['id']}", json=update_data)
        assert response.status_code == 200
        print("✅ Чат переименован")
        
        # 6. Удаление чата
        print("6. Удаление чата...")
        response = await self.client.delete(f"{API_BASE_URL}/api/chats/{chat['id']}")
        assert response.status_code == 200
        print("✅ Чат удален")
        
        print("🎉 Тест чата завершен успешно!")
    
    async def test_rag_workflow(self):
        """Тест полного workflow RAG"""
        print("\n🧪 Тестирование workflow RAG...")
        
        # 1. Создание документа
        print("1. Создание документа...")
        doc_data = {
            "name": f"test_document_{uuid.uuid4().hex[:8]}.txt",
            "uploaded_by": "test_user"
        }
        
        response = await self.client.post(f"{API_BASE_URL}/api/rag/documents", json=doc_data)
        assert response.status_code == 200
        doc = response.json()
        self.test_data["document_id"] = doc["id"]
        print(f"✅ Документ создан: {doc['id']}")
        
        # 2. Загрузка файла
        print("2. Загрузка файла...")
        test_content = "Это тестовый документ для проверки RAG системы. Содержит информацию о машинном обучении и обработке естественного языка."
        put_url = doc["put_url"]
        
        # Загружаем файл через presigned URL
        upload_response = await self.client.put(put_url, content=test_content)
        assert upload_response.status_code == 200
        print("✅ Файл загружен")
        
        # 3. Ожидание обработки
        print("3. Ожидание обработки...")
        async def check_processing():
            response = await self.client.get(f"{API_BASE_URL}/api/rag/documents/{doc['id']}/progress")
            if response.status_code == 200:
                progress = response.json()
                return progress.get("status") == "processed"
            return False
        
        processing_ok = await self.wait_for_condition(check_processing, timeout=120)
        assert processing_ok, "Документ не был обработан в течение 2 минут"
        print("✅ Документ обработан")
        
        # 4. Проверка статусов
        print("4. Проверка статусов...")
        response = await self.client.get(f"{API_BASE_URL}/api/rag/documents/{doc['id']}/progress")
        assert response.status_code == 200
        progress = response.json()
        assert progress["chunks_total"] > 0
        assert progress["vectors_total"] > 0
        print(f"✅ Статус: {progress['chunks_total']} чанков, {progress['vectors_total']} векторов")
        
        # 5. Скачивание оригинала
        print("5. Скачивание оригинала...")
        response = await self.client.get(f"{API_BASE_URL}/api/rag/documents/{doc['id']}/download?type=original")
        assert response.status_code == 200
        assert len(response.content) > 0
        print("✅ Оригинал скачан")
        
        # 6. Скачивание канонического файла
        print("6. Скачивание канонического файла...")
        response = await self.client.get(f"{API_BASE_URL}/api/rag/documents/{doc['id']}/download?type=canonical")
        assert response.status_code == 200
        print("✅ Канонический файл скачан")
        
        # 7. Поиск в RAG
        print("7. Поиск в RAG...")
        search_data = {
            "query": "машинное обучение",
            "top_k": 5
        }
        response = await self.client.post(f"{API_BASE_URL}/api/rag/search", json=search_data)
        assert response.status_code == 200
        results = response.json()
        assert len(results["results"]) > 0
        print(f"✅ Найдено {len(results['results'])} результатов")
        
        # 8. Пересчет на новую модель (если доступно)
        print("8. Пересчет на новую модель...")
        response = await self.client.post(f"{API_BASE_URL}/api/rag/documents/{doc['id']}/reprocess")
        if response.status_code == 200:
            print("✅ Пересчет запущен")
            # Ждем завершения пересчета
            reprocess_ok = await self.wait_for_condition(check_processing, timeout=120)
            assert reprocess_ok, "Пересчет не завершился в течение 2 минут"
            print("✅ Пересчет завершен")
        else:
            print("⚠️  Пересчет недоступен")
        
        # 9. Архивирование документа
        print("9. Архивирование документа...")
        response = await self.client.delete(f"{API_BASE_URL}/api/rag/documents/{doc['id']}")
        assert response.status_code == 200
        print("✅ Документ архивирован")
        
        # 10. Удаление документа
        print("10. Удаление документа...")
        response = await self.client.delete(f"{API_BASE_URL}/api/rag/documents/{doc['id']}?hard=true")
        assert response.status_code == 200
        print("✅ Документ удален")
        
        print("🎉 Тест RAG завершен успешно!")
    
    async def test_analysis_workflow(self):
        """Тест полного workflow анализа"""
        print("\n🧪 Тестирование workflow анализа...")
        
        # 1. Создание документа для анализа
        print("1. Создание документа для анализа...")
        doc_data = {
            "name": f"analysis_document_{uuid.uuid4().hex[:8]}.txt",
            "uploaded_by": "test_user"
        }
        
        response = await self.client.post(f"{API_BASE_URL}/api/rag/documents", json=doc_data)
        assert response.status_code == 200
        doc = response.json()
        
        # 2. Загрузка файла
        print("2. Загрузка файла...")
        test_content = "Это документ для анализа. Содержит важную информацию о проекте и его результатах."
        put_url = doc["put_url"]
        
        upload_response = await self.client.put(put_url, content=test_content)
        assert upload_response.status_code == 200
        print("✅ Файл загружен")
        
        # 3. Запуск анализа
        print("3. Запуск анализа...")
        analysis_data = {
            "document_id": doc["id"],
            "analysis_type": "summary"
        }
        
        response = await self.client.post(f"{API_BASE_URL}/api/analyze", json=analysis_data)
        assert response.status_code == 200
        analysis = response.json()
        self.test_data["analysis_id"] = analysis["id"]
        print(f"✅ Анализ запущен: {analysis['id']}")
        
        # 4. Ожидание результата
        print("4. Ожидание результата...")
        async def check_analysis():
            response = await self.client.get(f"{API_BASE_URL}/api/analyze/{analysis['id']}")
            if response.status_code == 200:
                result = response.json()
                return result.get("status") in ["completed", "failed"]
            return False
        
        analysis_ok = await self.wait_for_condition(check_analysis, timeout=120)
        assert analysis_ok, "Анализ не завершился в течение 2 минут"
        print("✅ Анализ завершен")
        
        # 5. Получение результата
        print("5. Получение результата...")
        response = await self.client.get(f"{API_BASE_URL}/api/analyze/{analysis['id']}")
        assert response.status_code == 200
        result = response.json()
        assert result["status"] == "completed"
        assert "result" in result
        print("✅ Результат получен")
        
        # 6. Очистка
        print("6. Очистка...")
        await self.client.delete(f"{API_BASE_URL}/api/rag/documents/{doc['id']}?hard=true")
        print("✅ Очистка завершена")
        
        print("🎉 Тест анализа завершен успешно!")
    
    async def test_system_health(self):
        """Тест здоровья системы"""
        print("\n🧪 Тестирование здоровья системы...")
        
        # 1. Проверка API
        print("1. Проверка API...")
        response = await self.client.get(f"{API_BASE_URL}/healthz")
        assert response.status_code == 200
        print("✅ API работает")
        
        # 2. Проверка эмбеддингов
        print("2. Проверка эмбеддингов...")
        try:
            response = await self.client.get("http://localhost:8001/healthz")
            if response.status_code == 200:
                print("✅ Эмбеддинги работают")
            else:
                print("⚠️  Эмбеддинги недоступны")
        except:
            print("⚠️  Эмбеддинги недоступны")
        
        # 3. Проверка LLM
        print("3. Проверка LLM...")
        try:
            response = await self.client.get("http://localhost:8002/healthz")
            if response.status_code == 200:
                print("✅ LLM работает")
            else:
                print("⚠️  LLM недоступен")
        except:
            print("⚠️  LLM недоступен")
        
        print("🎉 Тест здоровья системы завершен!")
    
    async def test_error_handling(self):
        """Тест обработки ошибок"""
        print("\n🧪 Тестирование обработки ошибок...")
        
        # 1. Несуществующий чат
        print("1. Несуществующий чат...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/nonexistent")
        assert response.status_code == 404
        print("✅ 404 для несуществующего чата")
        
        # 2. Несуществующий документ
        print("2. Несуществующий документ...")
        response = await self.client.get(f"{API_BASE_URL}/api/rag/documents/nonexistent")
        assert response.status_code == 404
        print("✅ 404 для несуществующего документа")
        
        # 3. Неверные данные
        print("3. Неверные данные...")
        response = await self.client.post(f"{API_BASE_URL}/api/chats", json={})
        assert response.status_code == 422
        print("✅ 422 для неверных данных")
        
        print("🎉 Тест обработки ошибок завершен!")

# Дополнительные тесты
class TestAdditionalFeatures:
    """Дополнительные тесты функций"""
    
    @pytest.fixture(autouse=True)
    async def setup(self):
        self.client = httpx.AsyncClient(timeout=30.0)
        yield
        await self.client.aclose()
    
    async def test_chat_with_rag(self):
        """Тест чата с RAG"""
        print("\n🧪 Тестирование чата с RAG...")
        
        # Создаем документ
        doc_data = {
            "name": f"rag_test_{uuid.uuid4().hex[:8]}.txt",
            "uploaded_by": "test_user"
        }
        response = await self.client.post(f"{API_BASE_URL}/api/rag/documents", json=doc_data)
        assert response.status_code == 200
        doc = response.json()
        
        # Загружаем файл
        test_content = "Это документ о машинном обучении. Содержит информацию о нейронных сетях и алгоритмах."
        put_url = doc["put_url"]
        await self.client.put(put_url, content=test_content)
        
        # Ждем обработки
        async def check_processing():
            response = await self.client.get(f"{API_BASE_URL}/api/rag/documents/{doc['id']}/progress")
            if response.status_code == 200:
                progress = response.json()
                return progress.get("status") == "processed"
            return False
        
        await self.wait_for_condition(check_processing, timeout=120)
        
        # Создаем чат
        chat_data = {"title": f"RAG Chat {uuid.uuid4().hex[:8]}"}
        response = await self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
        assert response.status_code == 200
        chat = response.json()
        
        # Отправляем сообщение с RAG
        message_data = {
            "content": "Расскажи о машинном обучении",
            "use_rag": True
        }
        response = await self.client.post(f"{API_BASE_URL}/api/chats/{chat['id']}/messages", json=message_data)
        assert response.status_code == 200
        
        # Очистка
        await self.client.delete(f"{API_BASE_URL}/api/chats/{chat['id']}")
        await self.client.delete(f"{API_BASE_URL}/api/rag/documents/{doc['id']}?hard=true")
        
        print("✅ Чат с RAG работает")
    
    async def test_batch_operations(self):
        """Тест пакетных операций"""
        print("\n🧪 Тестирование пакетных операций...")
        
        # Создаем несколько чатов
        chat_ids = []
        for i in range(3):
            chat_data = {"title": f"Batch Chat {i}"}
            response = await self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
            assert response.status_code == 200
            chat_ids.append(response.json()["id"])
        
        # Получаем список чатов
        response = await self.client.get(f"{API_BASE_URL}/api/chats")
        assert response.status_code == 200
        chats = response.json()
        assert len(chats) >= 3
        
        # Удаляем все чаты
        for chat_id in chat_ids:
            await self.client.delete(f"{API_BASE_URL}/api/chats/{chat_id}")
        
        print("✅ Пакетные операции работают")
    
    async def test_performance(self):
        """Тест производительности"""
        print("\n🧪 Тестирование производительности...")
        
        start_time = time.time()
        
        # Создаем чат
        chat_data = {"title": "Performance Test"}
        response = await self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
        assert response.status_code == 200
        chat = response.json()
        
        # Отправляем несколько сообщений
        for i in range(5):
            message_data = {"content": f"Сообщение {i}"}
            response = await self.client.post(f"{API_BASE_URL}/api/chats/{chat['id']}/messages", json=message_data)
            assert response.status_code == 200
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"✅ 5 сообщений за {duration:.2f} секунд")
        assert duration < 30, "Слишком медленно"
        
        # Очистка
        await self.client.delete(f"{API_BASE_URL}/api/chats/{chat['id']}")

if __name__ == "__main__":
    # Запуск тестов
    pytest.main([__file__, "-v", "-s"])

# ===== tests/e2e/test_additional_features.py =====
#!/usr/bin/env python3
"""
Дополнительные E2E тесты для функций, которые могли быть забыты
"""
import asyncio
import json
import time
import uuid
from typing import Dict, Any
import httpx
import pytest

API_BASE_URL = "http://localhost:8000"

class TestAdditionalFeatures:
    """Дополнительные тесты функций"""
    
    @pytest.fixture(autouse=True)
    async def setup(self):
        self.client = httpx.AsyncClient(timeout=30.0)
        yield
        await self.client.aclose()
    
    async def wait_for_condition(self, check_func, timeout: int = 60, interval: int = 2):
        """Ждет выполнения условия с таймаутом"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                if await check_func():
                    return True
            except Exception as e:
                print(f"Condition check failed: {e}")
            await asyncio.sleep(interval)
        return False
    
    async def test_user_authentication(self):
        """Тест аутентификации пользователей"""
        print("\n🧪 Тестирование аутентификации...")
        
        # 1. Регистрация пользователя
        print("1. Регистрация пользователя...")
        user_data = {
            "username": f"test_user_{uuid.uuid4().hex[:8]}",
            "email": f"test_{uuid.uuid4().hex[:8]}@example.com",
            "password": "test_password_123"
        }
        
        response = await self.client.post(f"{API_BASE_URL}/api/auth/register", json=user_data)
        if response.status_code == 201:
            print("✅ Пользователь зарегистрирован")
        elif response.status_code == 409:
            print("⚠️  Пользователь уже существует")
        else:
            print(f"⚠️  Ошибка регистрации: {response.status_code}")
        
        # 2. Вход пользователя
        print("2. Вход пользователя...")
        login_data = {
            "username": user_data["username"],
            "password": user_data["password"]
        }
        
        response = await self.client.post(f"{API_BASE_URL}/api/auth/login", json=login_data)
        if response.status_code == 200:
            tokens = response.json()
            assert "access_token" in tokens
            print("✅ Пользователь вошел в систему")
            
            # Сохраняем токен для дальнейших запросов
            self.client.headers.update({"Authorization": f"Bearer {tokens['access_token']}"})
        else:
            print(f"⚠️  Ошибка входа: {response.status_code}")
        
        print("🎉 Тест аутентификации завершен!")
    
    async def test_chat_search_and_filtering(self):
        """Тест поиска и фильтрации чатов"""
        print("\n🧪 Тестирование поиска и фильтрации чатов...")
        
        # Создаем несколько чатов с разными тегами
        chat_ids = []
        for i in range(3):
            chat_data = {
                "title": f"Search Test Chat {i}",
                "tags": [f"tag{i}", "search_test"]
            }
            response = await self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
            assert response.status_code == 200
            chat_ids.append(response.json()["id"])
        
        # 1. Поиск по названию
        print("1. Поиск по названию...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats?search=Search Test")
        assert response.status_code == 200
        chats = response.json()
        assert len(chats) >= 3
        print(f"✅ Найдено {len(chats)} чатов по названию")
        
        # 2. Фильтрация по тегам
        print("2. Фильтрация по тегам...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats?tags=search_test")
        assert response.status_code == 200
        chats = response.json()
        assert len(chats) >= 3
        print(f"✅ Найдено {len(chats)} чатов по тегам")
        
        # 3. Пагинация
        print("3. Пагинация...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats?limit=2&offset=0")
        assert response.status_code == 200
        chats = response.json()
        assert len(chats) <= 2
        print(f"✅ Пагинация работает: {len(chats)} чатов на странице")
        
        # Очистка
        for chat_id in chat_ids:
            await self.client.delete(f"{API_BASE_URL}/api/chats/{chat_id}")
        
        print("🎉 Тест поиска и фильтрации завершен!")
    
    async def test_document_metadata(self):
        """Тест метаданных документов"""
        print("\n🧪 Тестирование метаданных документов...")
        
        # Создаем документ
        doc_data = {
            "name": f"metadata_test_{uuid.uuid4().hex[:8]}.txt",
            "uploaded_by": "test_user",
            "tags": ["metadata", "test"]
        }
        
        response = await self.client.post(f"{API_BASE_URL}/api/rag/documents", json=doc_data)
        assert response.status_code == 200
        doc = response.json()
        
        # 1. Получение метаданных
        print("1. Получение метаданных...")
        response = await self.client.get(f"{API_BASE_URL}/api/rag/documents/{doc['id']}")
        assert response.status_code == 200
        metadata = response.json()
        assert metadata["name"] == doc_data["name"]
        assert metadata["uploaded_by"] == doc_data["uploaded_by"]
        print("✅ Метаданные получены")
        
        # 2. Обновление метаданных
        print("2. Обновление метаданных...")
        update_data = {
            "tags": ["metadata", "test", "updated"]
        }
        response = await self.client.put(f"{API_BASE_URL}/api/rag/documents/{doc['id']}", json=update_data)
        assert response.status_code == 200
        print("✅ Метаданные обновлены")
        
        # 3. Получение списка документов
        print("3. Получение списка документов...")
        response = await self.client.get(f"{API_BASE_URL}/api/rag/documents")
        assert response.status_code == 200
        documents = response.json()
        assert len(documents) >= 1
        print(f"✅ Получено {len(documents)} документов")
        
        # Очистка
        await self.client.delete(f"{API_BASE_URL}/api/rag/documents/{doc['id']}?hard=true")
        
        print("🎉 Тест метаданных завершен!")
    
    async def test_chat_export_import(self):
        """Тест экспорта и импорта чатов"""
        print("\n🧪 Тестирование экспорта и импорта чатов...")
        
        # Создаем чат с сообщениями
        chat_data = {"title": "Export Test Chat"}
        response = await self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
        assert response.status_code == 200
        chat = response.json()
        
        # Добавляем сообщения
        for i in range(3):
            message_data = {"content": f"Test message {i}"}
            response = await self.client.post(f"{API_BASE_URL}/api/chats/{chat['id']}/messages", json=message_data)
            assert response.status_code == 200
        
        # 1. Экспорт в JSON
        print("1. Экспорт в JSON...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/{chat['id']}/export?format=json")
        assert response.status_code == 200
        export_data = response.json()
        assert "chat" in export_data
        assert "messages" in export_data
        print("✅ Экспорт в JSON работает")
        
        # 2. Экспорт в TXT
        print("2. Экспорт в TXT...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/{chat['id']}/export?format=txt")
        assert response.status_code == 200
        assert len(response.content) > 0
        print("✅ Экспорт в TXT работает")
        
        # 3. Экспорт в Markdown
        print("3. Экспорт в Markdown...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/{chat['id']}/export?format=md")
        assert response.status_code == 200
        assert len(response.content) > 0
        print("✅ Экспорт в Markdown работает")
        
        # Очистка
        await self.client.delete(f"{API_BASE_URL}/api/chats/{chat['id']}")
        
        print("🎉 Тест экспорта и импорта завершен!")
    
    async def test_analytics_and_metrics(self):
        """Тест аналитики и метрик"""
        print("\n🧪 Тестирование аналитики и метрик...")
        
        # 1. Статистика чатов
        print("1. Статистика чатов...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/stats")
        if response.status_code == 200:
            stats = response.json()
            assert "total_chats" in stats
            print("✅ Статистика чатов получена")
        else:
            print("⚠️  Статистика чатов недоступна")
        
        # 2. Статистика документов
        print("2. Статистика документов...")
        response = await self.client.get(f"{API_BASE_URL}/api/rag/stats")
        if response.status_code == 200:
            stats = response.json()
            assert "total_docs" in stats
            print("✅ Статистика документов получена")
        else:
            print("⚠️  Статистика документов недоступна")
        
        # 3. Метрики системы
        print("3. Метрики системы...")
        response = await self.client.get(f"{API_BASE_URL}/metrics")
        if response.status_code == 200:
            print("✅ Метрики системы получены")
        else:
            print("⚠️  Метрики системы недоступны")
        
        print("🎉 Тест аналитики завершен!")
    
    async def test_error_recovery(self):
        """Тест восстановления после ошибок"""
        print("\n🧪 Тестирование восстановления после ошибок...")
        
        # 1. Тест с неверным форматом файла
        print("1. Тест с неверным форматом файла...")
        doc_data = {
            "name": "invalid_file.exe",  # Неподдерживаемый формат
            "uploaded_by": "test_user"
        }
        response = await self.client.post(f"{API_BASE_URL}/api/rag/documents", json=doc_data)
        if response.status_code == 400:
            print("✅ Неверный формат файла отклонен")
        else:
            print("⚠️  Неверный формат файла не обработан")
        
        # 2. Тест с очень большим файлом
        print("2. Тест с очень большим файлом...")
        doc_data = {
            "name": "large_file.txt",
            "uploaded_by": "test_user"
        }
        response = await self.client.post(f"{API_BASE_URL}/api/rag/documents", json=doc_data)
        if response.status_code == 200:
            doc = response.json()
            # Пытаемся загрузить очень большой контент
            large_content = "x" * (100 * 1024 * 1024)  # 100MB
            put_url = doc["put_url"]
            try:
                response = await self.client.put(put_url, content=large_content)
                if response.status_code == 413:
                    print("✅ Большой файл отклонен")
                else:
                    print("⚠️  Большой файл принят")
            except:
                print("✅ Большой файл отклонен (исключение)")
            
            # Очистка
            await self.client.delete(f"{API_BASE_URL}/api/rag/documents/{doc['id']}?hard=true")
        
        print("🎉 Тест восстановления завершен!")
    
    async def test_concurrent_operations(self):
        """Тест параллельных операций"""
        print("\n🧪 Тестирование параллельных операций...")
        
        # Создаем несколько чатов параллельно
        print("1. Создание чатов параллельно...")
        tasks = []
        for i in range(5):
            chat_data = {"title": f"Concurrent Chat {i}"}
            task = self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        success_count = sum(1 for r in responses if isinstance(r, httpx.Response) and r.status_code == 200)
        print(f"✅ Создано {success_count}/5 чатов параллельно")
        
        # Получаем ID успешных чатов
        chat_ids = []
        for response in responses:
            if isinstance(response, httpx.Response) and response.status_code == 200:
                chat_ids.append(response.json()["id"])
        
        # Отправляем сообщения параллельно
        print("2. Отправка сообщений параллельно...")
        tasks = []
        for chat_id in chat_ids:
            message_data = {"content": f"Concurrent message for {chat_id}"}
            task = self.client.post(f"{API_BASE_URL}/api/chats/{chat_id}/messages", json=message_data)
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        success_count = sum(1 for r in responses if isinstance(r, httpx.Response) and r.status_code == 200)
        print(f"✅ Отправлено {success_count} сообщений параллельно")
        
        # Очистка
        for chat_id in chat_ids:
            await self.client.delete(f"{API_BASE_URL}/api/chats/{chat_id}")
        
        print("🎉 Тест параллельных операций завершен!")
    
    async def test_data_consistency(self):
        """Тест консистентности данных"""
        print("\n🧪 Тестирование консистентности данных...")
        
        # Создаем чат
        chat_data = {"title": "Consistency Test Chat"}
        response = await self.client.post(f"{API_BASE_URL}/api/chats", json=chat_data)
        assert response.status_code == 200
        chat = response.json()
        
        # Добавляем сообщение
        message_data = {"content": "Consistency test message"}
        response = await self.client.post(f"{API_BASE_URL}/api/chats/{chat['id']}/messages", json=message_data)
        assert response.status_code == 200
        message = response.json()
        
        # 1. Проверяем, что сообщение появилось в списке
        print("1. Проверка появления сообщения...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/{chat['id']}/messages")
        assert response.status_code == 200
        messages = response.json()
        assert len(messages) >= 1
        print("✅ Сообщение появилось в списке")
        
        # 2. Проверяем, что чат обновился
        print("2. Проверка обновления чата...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/{chat['id']}")
        assert response.status_code == 200
        updated_chat = response.json()
        assert updated_chat["id"] == chat["id"]
        print("✅ Чат обновился")
        
        # 3. Проверяем, что данные не потерялись
        print("3. Проверка сохранности данных...")
        response = await self.client.get(f"{API_BASE_URL}/api/chats/{chat['id']}/messages/{message['id']}")
        assert response.status_code == 200
        retrieved_message = response.json()
        assert retrieved_message["content"] == message_data["content"]
        print("✅ Данные не потерялись")
        
        # Очистка
        await self.client.delete(f"{API_BASE_URL}/api/chats/{chat['id']}")
        
        print("🎉 Тест консистентности завершен!")

if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])

# ===== tests/e2e/test_ingest_chain_apply.py =====
def test_ingest_chain_calls_apply_async(monkeypatch):
    from app.services import rag_service
    called = {"ok": False}
    class DummySign:
        def __or__(self, other): return self
        def apply_async(self_inner): called["ok"] = True
    # monkeypatch each task signature .s to return DummySign
    from app.tasks import normalize, chunk, embed, index
    monkeypatch.setattr(normalize.process, "s", lambda *a, **k: DummySign())
    monkeypatch.setattr(chunk.split, "s", lambda *a, **k: DummySign())
    monkeypatch.setattr(embed.compute, "s", lambda *a, **k: DummySign())
    monkeypatch.setattr(index.finalize, "s", lambda *a, **k: DummySign())
    rag_service.start_ingest_chain("doc-1")
    assert called["ok"] is True

# ===== tests/services/test_rag_service_pagination.py =====
from app.services import rag_service

def test_next_offset_and_sort(monkeypatch):
    # monkeypatch clients: embed_texts -> fixed vector; qdrant_search -> fake hits
    from app.services import clients
    monkeypatch.setattr(clients, "embed_texts", lambda texts: [[0.1, 0.2]])
    def fake_search(vec, top_k, offset=0, doc_id=None, tags=None, sort_by="score_desc"):
        base = [
            {"score": 0.9, "payload": {"text":"A","document_id":"d","chunk_idx":0,"tags":["x"]}},
            {"score": 0.8, "payload": {"text":"B","document_id":"d","chunk_idx":1,"tags":["y"]}},
        ][:top_k]
        return list(reversed(base)) if sort_by=="score_asc" else base
    monkeypatch.setattr(clients, "qdrant_search", fake_search)
    class S: pass
    out = rag_service.search(S(), "q", top_k=2, offset=10, sort_by="score_desc")
    assert out["next_offset"] == 12
    assert out["results"][0]["score"] == 0.9
    out2 = rag_service.search(S(), "q", top_k=2, offset=10, sort_by="score_asc")
    assert out2["results"][0]["score"] == 0.8

# ===== tests/services/__init__.py =====

# ===== tests/services/test_clients.py =====
import types
from app.services import clients

def test_qdrant_filter_build(monkeypatch):
    # monkeypatch search to capture arguments
    captured = {}
    class DummyH: 
        def __init__(self): self.score=0.9; self.id="1"; self.payload={"text":"t","document_id":"d","chunk_idx":0,"tags":["a"]}
    def fake_search(collection_name, query_vector, limit, offset, with_payload, query_filter):
        captured['kwargs'] = {'collection_name': collection_name, 'limit': limit, 'offset': offset, 'with_payload': with_payload, 'query_filter': query_filter}
        return [DummyH()]
    monkeypatch.setattr(clients.get_qdrant(), "search", fake_search)
    out = clients.qdrant_search([0.1,0.2], 5, offset=10, doc_id="doc-1", tags=["a","b"])
    assert out and out[0]["payload"]["document_id"] == "d"
    k = captured['kwargs']
    assert k['limit'] == 5 and k['offset'] == 10 and k['with_payload'] is True
    assert k['query_filter'] is not None

# ===== app/migrations/env.py =====
# backend/app/migrations/env.py
from __future__ import annotations
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# Add the app directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from app.core.config import settings
from app.models.base import Base  # provides Base.metadata
# Don't import models to avoid ENUM creation conflicts

# Alembic Config
config = context.config
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata

def get_url() -> str:
    # Use application settings (supports DB_URL or DB.URL env names)
    return settings.DB_URL

def run_migrations_offline() -> None:
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        include_schemas=False,
    )
    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    configuration = config.get_section(config.config_ini_section) or {}
    configuration["sqlalchemy.url"] = get_url()
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
        future=True,
    )
    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            include_schemas=False,
        )
        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

# ===== app/tasks/delete.py =====
from __future__ import annotations
from celery import shared_task
from app.core.s3 import get_minio
from app.core.qdrant import get_qdrant
from app.core.db import SessionLocal
from app.core.config import settings
from app.models.rag import RagDocuments, RagChunks
from .shared import log, task_metrics

COLLECTION = "rag_chunks"


@shared_task(name="app.tasks.delete.hard_delete", bind=True)
def hard_delete(self, document_id: str) -> dict:
    """Hard delete: removes MinIO objects, Qdrant points and DB rows."""
    with task_metrics("delete.hard_delete", "delete"):
        session = SessionLocal()
        s3 = get_minio()
        qdrant = get_qdrant()
        
        try:
            doc = session.get(RagDocuments, document_id)
            if not doc:
                return {"document_id": document_id, "status": "not_found"}
            
            # Delete MinIO objects
            if doc.url_file:
                try:
                    s3.remove_object(settings.S3_BUCKET_RAG, doc.url_file)
                    log.info(f"Deleted raw file: {doc.url_file}")
                except Exception as e:
                    log.error(f"Failed to delete raw file {doc.url_file}: {e}")
            
            if doc.url_canonical_file:
                try:
                    s3.remove_object(settings.S3_BUCKET_RAG, doc.url_canonical_file)
                    log.info(f"Deleted canonical file: {doc.url_canonical_file}")
                except Exception as e:
                    log.error(f"Failed to delete canonical file {doc.url_canonical_file}: {e}")
            
            # Delete Qdrant points
            try:
                from qdrant_client.http.models import Filter, FieldCondition, MatchValue
                f = Filter(must=[FieldCondition(key="document_id", match=MatchValue(value=document_id))])
                qdrant.delete(collection_name=COLLECTION, points_selector=f)
                log.info(f"Deleted Qdrant points for document: {document_id}")
            except Exception as e:
                log.error(f"Failed to delete Qdrant points for {document_id}: {e}")
            
            # Delete DB chunks
            session.query(RagChunks).filter(RagChunks.document_id == doc.id).delete()
            
            # Delete document
            session.delete(doc)
            session.commit()
            
            log.info(f"Hard deleted document: {document_id}")
            return {"document_id": document_id, "status": "deleted"}
            
        except Exception as e:
            log.error(f"Error during hard delete of {document_id}: {e}")
            session.rollback()
            raise
        finally:
            session.close()
# ===== app/tasks/ocr_tables.py =====
from __future__ import annotations

import json
from typing import Dict, List, Any, Optional
from celery import shared_task
from app.core.config import settings
from app.core.s3 import get_object, put_object
from app.core.db import SessionLocal
from app.core.metrics import rag_ingest_stage_duration, rag_ingest_errors_total
from .shared import log, RetryableError, task_metrics

@shared_task(name="app.tasks.ocr_tables.process", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 3})
def process_ocr_tables(self, document_id: str, source_key: str, original_filename: str) -> Dict[str, Any]:
    """
    Process PDF for OCR and table extraction
    """
    with task_metrics("ocr_tables.process", "ocr"):
        session = SessionLocal()
        try:
            # Load PDF from S3
            obj = get_object(settings.S3_BUCKET_RAG, source_key)
            pdf_content = obj.read()
            
            # Process OCR if needed
            ocr_text = ""
            ocr_meta = {}
            try:
                ocr_text, ocr_meta = _extract_ocr_text(pdf_content)
                log.info(f"OCR extracted {len(ocr_text)} characters")
            except Exception as e:
                log.warning(f"OCR extraction failed: {e}")
                rag_ingest_errors_total.labels(stage="ocr", error_type="extraction_failed").inc()
            
            # Extract tables
            tables = []
            try:
                tables = _extract_tables(pdf_content)
                log.info(f"Extracted {len(tables)} tables")
            except Exception as e:
                log.warning(f"Table extraction failed: {e}")
                rag_ingest_errors_total.labels(stage="tables", error_type="extraction_failed").inc()
            
            # Combine results
            result = {
                "document_id": document_id,
                "ocr_text": ocr_text,
                "ocr_meta": ocr_meta,
                "tables": tables,
                "processing_method": "ocr_tables"
            }
            
            # Save enhanced canonical
            canonical_key = f"{document_id}/canonical_enhanced.json"
            canonical_data = json.dumps(result, ensure_ascii=False).encode("utf-8")
            put_object(settings.S3_BUCKET_RAG, canonical_key, canonical_data, content_type="application/json; charset=utf-8")
            
            return result
            
        except Exception as e:
            log.error(f"OCR/tables processing failed: {e}")
            rag_ingest_errors_total.labels(stage="ocr_tables", error_type="processing_failed").inc()
            raise RetryableError(f"OCR/tables processing failed: {e}")
        finally:
            session.close()

def _extract_ocr_text(pdf_content: bytes) -> tuple[str, Dict[str, Any]]:
    """Extract text using OCR from PDF images"""
    try:
        from pdf2image import convert_from_bytes
        import pytesseract
        
        # Convert PDF to images
        images = convert_from_bytes(pdf_content, dpi=300)
        
        text_parts = []
        meta = {
            "pages": len(images),
            "method": "pytesseract",
            "dpi": 300,
            "lang": "rus+eng"
        }
        
        for i, image in enumerate(images):
            # OCR each page
            page_text = pytesseract.image_to_string(image, lang='rus+eng')
            text_parts.append(page_text)
            
            # Log progress
            if i % 10 == 0:
                log.info(f"OCR processed page {i+1}/{len(images)}")
        
        full_text = '\n\n'.join(text_parts)
        
        return full_text, meta
        
    except ImportError:
        log.warning("OCR dependencies not available")
        return "", {"error": "OCR dependencies not available"}
    except Exception as e:
        log.error(f"OCR extraction failed: {e}")
        return "", {"error": str(e)}

def _extract_tables(pdf_content: bytes) -> List[Dict[str, Any]]:
    """Extract tables from PDF using multiple methods"""
    import io
    tables = []
    
    # Method 1: pdfplumber (simple tables)
    try:
        import pdfplumber
        with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_tables = page.extract_tables()
                for table_num, table in enumerate(page_tables):
                    if table and len(table) > 1:
                        # Convert to structured format
                        table_data = {
                            "page": page_num + 1,
                            "table": table_num + 1,
                            "rows": len(table),
                            "cols": len(table[0]) if table else 0,
                            "data": table,
                            "method": "pdfplumber"
                        }
                        tables.append(table_data)
    except ImportError:
        log.warning("pdfplumber not available for table extraction")
    except Exception as e:
        log.warning(f"pdfplumber table extraction failed: {e}")
    
    # Method 2: camelot (advanced tables)
    try:
        import camelot
        import io
        
        # Extract tables using camelot
        camelot_tables = camelot.read_pdf(io.BytesIO(pdf_content), pages='all')
        
        for i, table in enumerate(camelot_tables):
            if table.df is not None and not table.df.empty:
                table_data = {
                    "page": table.page,
                    "table": i + 1,
                    "rows": len(table.df),
                    "cols": len(table.df.columns),
                    "data": table.df.values.tolist(),
                    "method": "camelot",
                    "accuracy": table.accuracy
                }
                tables.append(table_data)
    except ImportError:
        log.warning("camelot not available for table extraction")
    except Exception as e:
        log.warning(f"camelot table extraction failed: {e}")
    
    # Method 3: tabula (Java-based)
    try:
        import tabula
        import io
        
        # Extract tables using tabula
        tabula_tables = tabula.read_pdf(io.BytesIO(pdf_content), pages='all', multiple_tables=True)
        
        for i, table in enumerate(tabula_tables):
            if table is not None and not table.empty:
                table_data = {
                    "page": 1,  # tabula doesn't provide page info easily
                    "table": i + 1,
                    "rows": len(table),
                    "cols": len(table.columns),
                    "data": table.values.tolist(),
                    "method": "tabula"
                }
                tables.append(table_data)
    except ImportError:
        log.warning("tabula not available for table extraction")
    except Exception as e:
        log.warning(f"tabula table extraction failed: {e}")
    
    return tables

def _convert_table_to_markdown(table_data: List[List[str]]) -> str:
    """Convert table data to Markdown format"""
    if not table_data or len(table_data) < 2:
        return ""
    
    # Create header
    header = "| " + " | ".join(str(cell or "") for cell in table_data[0]) + " |"
    separator = "| " + " | ".join("---" for _ in table_data[0]) + " |"
    
    # Create rows
    rows = []
    for row in table_data[1:]:
        row_str = "| " + " | ".join(str(cell or "") for cell in row) + " |"
        rows.append(row_str)
    
    return "\n".join([header, separator] + rows)

@shared_task(name="app.tasks.ocr_tables.enhance_canonical", bind=True)
def enhance_canonical_with_ocr_tables(self, document_id: str, canonical_key: str) -> Dict[str, Any]:
    """
    Enhance existing canonical file with OCR and table data
    """
    with task_metrics("ocr_tables.enhance_canonical", "enhance"):
        try:
            # Load existing canonical
            obj = get_object(settings.S3_BUCKET_RAG, canonical_key)
            canonical_data = json.loads(obj.read().decode("utf-8"))
            
            # Get original PDF
            source_key = f"{document_id}/origin.pdf"  # Assuming PDF extension
            pdf_obj = get_object(settings.S3_BUCKET_RAG, source_key)
            pdf_content = pdf_obj.read()
            
            # Extract OCR and tables
            ocr_text, ocr_meta = _extract_ocr_text(pdf_content)
            tables = _extract_tables(pdf_content)
            
            # Enhance canonical data
            enhanced_data = {
                **canonical_data,
                "ocr_text": ocr_text,
                "ocr_meta": ocr_meta,
                "extracted_tables": tables,
                "enhanced_at": "2024-01-01T00:00:00Z"  # Would use actual timestamp
            }
            
            # Save enhanced canonical
            enhanced_key = f"{document_id}/canonical_enhanced.json"
            enhanced_json = json.dumps(enhanced_data, ensure_ascii=False).encode("utf-8")
            put_object(settings.S3_BUCKET_RAG, enhanced_key, enhanced_json, content_type="application/json; charset=utf-8")
            
            return {
                "document_id": document_id,
                "enhanced": True,
                "ocr_chars": len(ocr_text),
                "tables_count": len(tables)
            }
            
        except Exception as e:
            log.error(f"Canonical enhancement failed: {e}")
            rag_ingest_errors_total.labels(stage="enhance", error_type="enhancement_failed").inc()
            raise RetryableError(f"Canonical enhancement failed: {e}")

# ===== app/tasks/embed.py =====
from __future__ import annotations
import os, uuid
from datetime import datetime
import httpx
from celery import shared_task
from qdrant_client.http.models import VectorParams, Distance, PointStruct
from app.core.config import settings
from app.core.qdrant import get_qdrant
from app.core.db import SessionLocal
from app.core.metrics import rag_vectors_upserted_total
from app.models.rag import RagDocuments, RagChunks
from .shared import log, RetryableError, task_metrics, env_int
from app.core.metrics import embedding_batch_inflight

EMB_URL = os.getenv("EMBEDDINGS_URL", "http://emb:8001")
COLLECTION = os.getenv("QDRANT_COLLECTION", "rag_chunks")

def _embed_sync(texts: list[str]) -> list[list[float]]:
    with httpx.Client(timeout=60) as client:
        r = client.post(f"{EMB_URL}/embed", json={"inputs": texts})
        r.raise_for_status()
        payload = r.json()
        return payload.get("vectors", [])

@shared_task(name="app.tasks.embed.compute", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 5})
def compute(self, result: dict) -> dict:
    with task_metrics("embed.compute", "embed"):
        session = SessionLocal()
        qdrant = get_qdrant()
        try:
            # Получаем document_id из результата предыдущей задачи
            document_id = result.get("document_id")
            if not document_id:
                raise RetryableError("no_document_id")
            
            from uuid import UUID
            doc = session.get(RagDocuments, UUID(document_id))
            if not doc:
                raise RetryableError("document_not_found")
            doc_tags = doc.tags or []
            BATCH = env_int("EMBEDDING_BATCH_SIZE", 8)
            rows = session.query(RagChunks).filter(RagChunks.document_id==doc.id, RagChunks.qdrant_point_id==None).order_by(RagChunks.chunk_idx.asc()).all()
            total = 0
            for i in range(0, len(rows), BATCH):
                batch = rows[i:i+BATCH]
                texts = [c.text for c in batch]
                embedding_batch_inflight.inc()
                try:
                    vectors = _embed_sync(texts)
                finally:
                    embedding_batch_inflight.dec()
                if not vectors:
                    raise RetryableError("empty_vectors")

                dim = len(vectors[0])
                try:
                    qdrant.get_collection(COLLECTION)
                except Exception:
                    from qdrant_client.http.models import OptimizersConfigDiff
                    qdrant.recreate_collection(
                        collection_name=COLLECTION,
                        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
                        optimizers_config=OptimizersConfigDiff(memmap_threshold=20000)
                    )

                points = []
                now = datetime.utcnow()
                for chunk, vec in zip(batch, vectors):
                    pid = uuid.uuid4()
                    points.append(PointStruct(id=str(pid), vector=vec, payload={
                        "document_id": str(doc.id),
                        "chunk_idx": chunk.chunk_idx,
                        "text": chunk.text,
                        "tags": doc_tags,
                    }))
                    chunk.qdrant_point_id = pid
                    chunk.embedding_model = os.getenv("MODEL_ID","BAAI/bge-m3")
                    chunk.embedding_version = "v1"
                    chunk.date_embedding = now
                qdrant.upsert(collection_name=COLLECTION, points=points)
                session.commit()
                rag_vectors_upserted_total.inc(len(points))
                total += len(batch)
            doc.status = "indexing"; doc.updated_at = datetime.utcnow()
            session.commit()
            return {"document_id": str(doc.id), "embedded": total, "status": doc.status}
        finally:
            session.close()

# ===== app/tasks/index.py =====
from __future__ import annotations
from datetime import datetime
from celery import shared_task
from app.core.qdrant import get_qdrant
from app.core.db import SessionLocal
from app.models.rag import RagDocuments, RagChunks
from .shared import log, task_metrics, RetryableError

COLLECTION = "rag_chunks"

@shared_task(name="app.tasks.index.finalize", bind=True)
def finalize(self, result: dict) -> dict:
    with task_metrics("index.finalize", "index"):
        session = SessionLocal()
        qdrant = get_qdrant()
        try:
            # Получаем document_id из результата предыдущей задачи
            document_id = result.get("document_id")
            if not document_id:
                raise RetryableError("no_document_id")
            
            from uuid import UUID
            doc = session.get(RagDocuments, UUID(document_id))
            if not doc:
                return {"document_id": document_id, "status": "not_found"}
            # простая проверка наличия поинтов документа
            # (в реале — count по payload фильтру)
            doc.status = "ready"
            doc.updated_at = datetime.utcnow()
            session.commit()
            return {"document_id": str(doc.id), "status": doc.status}
        finally:
            session.close()

@shared_task(name="app.tasks.index.housekeeping", bind=True)
def housekeeping(self) -> dict:
    with task_metrics("index.housekeeping", "index"):
        return {"ok": True}

# ===== app/tasks/normalize.py =====
from __future__ import annotations

import json
from typing import Optional
from celery import shared_task

from app.core.config import settings  # expects settings.S3_BUCKET_RAG
from app.core.s3 import get_object, put_object  # S3 functions
from app.core.metrics import rag_ingest_stage_duration, rag_ingest_errors_total

from app.services.enhanced_text_extractor import extract_text_enhanced
from app.services.text_normalizer import normalize_text
from .shared import log, RetryableError, task_metrics

@shared_task(name="app.tasks.normalize.normalize", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 3})
def normalize(self, document_id: str, source_key: Optional[str] = None, original_filename: Optional[str] = None) -> dict:
    """
    Read original file from S3, extract + normalize text, and write canonical JSON to
    f"{document_id}/canonical.txt" in the same RAG bucket.
    Returns a dict with canonical path and counters for logging/metrics.
    """
    with task_metrics("normalize.normalize", "normalize"):
        try:
            # 1) Determine keys
            bucket = settings.S3_BUCKET_RAG
            if source_key is None:
                raise ValueError("source_key is required to locate the original file in S3")
            canonical_key = f"{document_id}/canonical.txt"

            # 2) Load original bytes
            obj = get_object(bucket, source_key)
            content = obj.read()

            # 3) Extract text by format, normalize
            filename = original_filename or source_key.split("/")[-1]
            result = extract_text_enhanced(content, filename=filename)
            cleaned = normalize_text(result.text)

            # 4) Build canonical JSON payload with tables
            canonical_payload = {
                "text": cleaned,
                "tables": [{"name": t.name, "csv": t.csv_data, "rows": t.rows, "cols": t.cols} for t in result.tables],
                "meta": result.meta,
                "original_filename": filename,
                "extractor": result.kind,
                "warnings": result.warnings,
            }
            data = json.dumps(canonical_payload, ensure_ascii=False).encode("utf-8")

            # 5) Store canonical alongside original (same bucket)
            put_object(bucket, canonical_key, data, content_type="application/json; charset=utf-8")

            # Обновляем метрики
            rag_ingest_stage_duration.labels(stage="normalize").observe(0.1)  # Примерное время

            return {
                "document_id": document_id,
                "source_key": source_key,
                "canonical_key": canonical_key,
                "text_chars": len(cleaned),
                "extractor": result.kind,
                "warnings": result.warnings,
            }
            
        except Exception as e:
            log.error(f"Normalization failed for document {document_id}: {e}")
            rag_ingest_errors_total.labels(stage="normalize", error_type="processing_failed").inc()
            raise RetryableError(f"Normalization failed: {e}")

# Оставляем старую функцию для обратной совместимости
def process(document_id: str, source_key: Optional[str] = None, original_filename: Optional[str] = None) -> dict:
    """Legacy function for backward compatibility"""
    return normalize.delay(document_id, source_key, original_filename).get()

# ===== app/tasks/__init__.py =====


# ===== app/tasks/embedding_worker.py =====
"""
Embedding Worker - воркер для конкретной модели эмбеддинга
Загружает модель из MinIO, кэширует локально, обрабатывает батчи
"""
from __future__ import annotations
import os
import time
import hashlib
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional
import boto3
from botocore.exceptions import ClientError
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from celery import Celery
from app.core.model_registry import get_model_registry, ModelConfig
from app.core.config import settings

class EmbeddingWorker:
    """Воркер эмбеддингов для одной модели"""
    
    def __init__(self):
        self.model_alias = os.getenv("EMB_MODEL_ALIAS", "minilm")
        self.model_id = os.getenv("EMB_MODEL_ID", "sentence-transformers/all-MiniLM-L6-v2")
        self.model_rev = os.getenv("EMB_MODEL_REV", "default")
        self.dim = int(os.getenv("EMB_DIM", "384"))
        self.max_seq = int(os.getenv("EMB_MAX_SEQ", "256"))
        self.device = os.getenv("EMB_DEVICE", "cpu")
        
        # MinIO настройки
        self.s3_endpoint = os.getenv("S3_ENDPOINT", "http://minio:9000")
        self.s3_access_key = os.getenv("S3_ACCESS_KEY", "minioadmin")
        self.s3_secret_key = os.getenv("S3_SECRET_KEY", "minioadmin")
        self.models_bucket = os.getenv("MODELS_BUCKET", "models")
        self.models_cache_dir = os.getenv("MODELS_CACHE_DIR", "/models-cache")
        
        # Батчинг настройки
        self.batch_max_tokens_rt = int(os.getenv("BATCH_MAX_TOKENS_RT", "4096"))
        self.batch_max_tokens_bulk = int(os.getenv("BATCH_MAX_TOKENS_BULK", "16384"))
        self.batch_max_wait_ms_rt = int(os.getenv("BATCH_MAX_WAIT_MS_RT", "25"))
        self.batch_max_wait_ms_bulk = int(os.getenv("BATCH_MAX_WAIT_MS_BULK", "200"))
        
        self.model = None
        self.tokenizer = None
        self.registry = get_model_registry()
        self.s3_client = None
        self._setup_s3()
        self._load_model()
    
    def _setup_s3(self):
        """Настраивает S3 клиент для MinIO"""
        try:
            self.s3_client = boto3.client(
                's3',
                endpoint_url=self.s3_endpoint,
                aws_access_key_id=self.s3_access_key,
                aws_secret_access_key=self.s3_secret_key
            )
        except Exception as e:
            print(f"Failed to setup S3 client: {e}")
            self.s3_client = None
    
    def _get_model_cache_path(self) -> Path:
        """Получает путь к кэшу модели"""
        cache_dir = Path(self.models_cache_dir) / self.model_alias / self.model_rev
        cache_dir.mkdir(parents=True, exist_ok=True)
        return cache_dir
    
    def _download_model_from_s3(self) -> bool:
        """Скачивает модель из MinIO в локальный кэш"""
        if not self.s3_client:
            print("S3 client not available, using local model")
            return False
        
        cache_path = self._get_model_cache_path()
        
        try:
            # Проверяем, есть ли уже модель в кэше
            if (cache_path / "config.json").exists():
                print(f"Model already cached at {cache_path}")
                return True
            
            # Скачиваем из MinIO
            s3_prefix = f"{self.model_id}/{self.model_rev}/"
            print(f"Downloading model from s3://{self.models_bucket}/{s3_prefix}")
            
            # Список файлов для скачивания
            files_to_download = [
                "config.json",
                "pytorch_model.bin",
                "tokenizer.json",
                "tokenizer_config.json",
                "vocab.txt"
            ]
            
            for filename in files_to_download:
                s3_key = f"{s3_prefix}{filename}"
                local_path = cache_path / filename
                
                try:
                    self.s3_client.download_file(self.models_bucket, s3_key, str(local_path))
                    print(f"Downloaded {filename}")
                except ClientError as e:
                    if e.response['Error']['Code'] == '404':
                        print(f"File {filename} not found in S3, skipping")
                    else:
                        print(f"Failed to download {filename}: {e}")
            
            return True
            
        except Exception as e:
            print(f"Failed to download model from S3: {e}")
            return False
    
    def _load_model(self):
        """Загружает модель и токенайзер"""
        try:
            # Проверяем, нужно ли использовать локальные модели
            use_local = os.getenv("USE_LOCAL_MODELS", "false").lower() == "true"
            local_models_dir = os.getenv("LOCAL_MODELS_DIR", "/local-models")
            
            if use_local:
                # Используем локальные модели
                model_dir_name = self.model_id.replace("/", "--")
                local_model_path = Path(local_models_dir) / model_dir_name
                
                if local_model_path.exists():
                    model_path = str(local_model_path)
                    print(f"Loading model from local directory: {model_path}")
                else:
                    print(f"Local model not found at {local_model_path}, falling back to HuggingFace")
                    model_path = self.model_id
            else:
                # Пробуем скачать из S3
                s3_success = self._download_model_from_s3()
                
                cache_path = self._get_model_cache_path()
                
                if s3_success and (cache_path / "config.json").exists():
                    # Загружаем из кэша
                    model_path = str(cache_path)
                    print(f"Loading model from cache: {model_path}")
                else:
                    # Fallback на загрузку из HuggingFace (только для разработки)
                    model_path = self.model_id
                    print(f"Loading model from HuggingFace: {model_path}")
            
            # Загружаем модель
            self.model = SentenceTransformer(model_path, device=self.device)
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # Обновляем статус в реестре
            self.registry.update_health(self.model_alias, "ready")
            
            print(f"Model {self.model_alias} loaded successfully")
            
        except Exception as e:
            print(f"Failed to load model {self.model_alias}: {e}")
            self.registry.update_health(self.model_alias, "down")
            raise
    
    def process_embedding_batch(self, texts: List[str], profile: str = "rt") -> Dict[str, Any]:
        """Обрабатывает батч текстов"""
        if not self.model:
            raise Exception("Model not loaded")
        
        start_time = time.perf_counter()
        
        try:
            # Токенизация и проверка длины
            max_tokens = self.batch_max_tokens_rt if profile == "rt" else self.batch_max_tokens_bulk
            
            # Простая проверка длины (можно улучшить)
            processed_texts = []
            warnings = []
            
            for i, text in enumerate(texts):
                if len(text) > max_tokens * 4:  # Примерная оценка
                    # Обрезаем текст
                    truncated = text[:max_tokens * 4]
                    processed_texts.append(truncated)
                    warnings.append(f"Text {i} truncated due to length")
                else:
                    processed_texts.append(text)
            
            # Генерируем эмбеддинги
            vectors = self.model.encode(processed_texts, convert_to_tensor=False)
            vectors_list = vectors.tolist() if hasattr(vectors, 'tolist') else vectors
            
            duration_ms = int((time.perf_counter() - start_time) * 1000)
            
            return {
                "model_alias": self.model_alias,
                "dim": self.dim,
                "vectors": vectors_list,
                "warnings": warnings,
                "duration_ms": duration_ms
            }
            
        except Exception as e:
            print(f"Error processing batch: {e}")
            raise

# Celery задачи для воркера
def create_embedding_worker_tasks(celery_app: Celery):
    """Создает Celery задачи для воркера эмбеддингов"""
    
    worker = None
    
    @celery_app.task(name="embedding_worker.process_embedding")
    def process_embedding_task(task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Основная задача воркера эмбеддингов"""
        nonlocal worker
        
        if worker is None:
            worker = EmbeddingWorker()
        
        try:
            texts = task_data.get("texts", [])
            profile = task_data.get("profile", "rt")
            
            if not texts:
                return {
                    "error": "No texts provided",
                    "model_alias": worker.model_alias
                }
            
            result = worker.process_embedding_batch(texts, profile)
            return result
            
        except Exception as e:
            return {
                "error": str(e),
                "model_alias": worker.model_alias if worker else "unknown"
            }
    
    @celery_app.task(name="embedding_worker.health_check")
    def health_check_task() -> Dict[str, Any]:
        """Проверка здоровья воркера"""
        nonlocal worker
        
        if worker is None:
            try:
                worker = EmbeddingWorker()
                return {
                    "status": "ready",
                    "model_alias": worker.model_alias,
                    "dim": worker.dim
                }
            except Exception as e:
                return {
                    "status": "down",
                    "error": str(e)
                }
        
        return {
            "status": "ready" if worker.model else "down",
            "model_alias": worker.model_alias,
            "dim": worker.dim
        }
    
    return process_embedding_task, health_check_task

# ===== app/tasks/chunk.py =====
from __future__ import annotations
from celery import shared_task
from datetime import datetime
from app.core.config import settings
from app.core.s3 import get_minio
from app.core.db import SessionLocal
from app.core.metrics import rag_chunks_created_total
from app.models.rag import RagDocuments, RagChunks
from .shared import log, RetryableError, task_metrics, env_int
from app.services.adaptive_chunker import chunk_text_adaptive

def _split_text(txt: str, max_chars: int, overlap: int):
    i = 0; n = len(txt)
    while i < n:
        j = min(i + max_chars, n)
        yield txt[i:j]
        if j >= n: break
        i = j - overlap if j - overlap > i else j

@shared_task(name="app.tasks.chunk.split", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 5})
def split(self, result: dict, *, max_chars: int | None = None, overlap: int | None = None) -> dict:
    with task_metrics("chunk.split", "chunk"):
        session = SessionLocal()
        s3 = get_minio()
        try:
            # Получаем document_id из результата предыдущей задачи
            document_id = result.get("document_id")
            if not document_id:
                raise RetryableError("no_document_id")
            
            from uuid import UUID
            doc = session.get(RagDocuments, UUID(document_id))
            if not doc or not doc.url_canonical_file:
                raise RetryableError("canonical_not_ready")
            max_chars = max_chars or env_int("CHUNK_MAX_CHARS", 1200)
            overlap   = overlap   or env_int("CHUNK_OVERLAP", 100)
            try:
                obj = s3.get_object(settings.S3_BUCKET_RAG, doc.url_canonical_file)
                json_data = obj.read().decode("utf-8") or "{}"
                import json
                parsed_data = json.loads(json_data)
                # Извлекаем текст из нормализованного JSON
                data = parsed_data.get("text", "") if isinstance(parsed_data, dict) else ""
                document_meta = parsed_data.get("meta", {}) if isinstance(parsed_data, dict) else {}
            except Exception as e:
                log.error(f"Error reading canonical file: {e}")
                data = ""
                document_meta = {}
            
            # Используем адаптивный chunker
            adaptive_chunks = chunk_text_adaptive(data, max_chars=max_chars, overlap=overlap, document_meta=document_meta)
            
            if not adaptive_chunks:
                # Fallback to simple splitting
                chunks = list(_split_text(data, max_chars=max_chars, overlap=overlap)) or [""]
                for idx, text in enumerate(chunks):
                    session.add(RagChunks(document_id=doc.id, chunk_idx=idx, text=text))
            else:
                # Use adaptive chunks
                for chunk in adaptive_chunks:
                    chunk_metadata = {
                        'is_header': chunk.is_header,
                        'is_table': chunk.is_table,
                        'parent_section': chunk.parent_section,
                        **chunk.metadata
                    }
                    session.add(RagChunks(
                        document_id=doc.id, 
                        chunk_idx=chunk.chunk_idx, 
                        text=chunk.text,
                        metadata=chunk_metadata
                    ))
            rag_chunks_created_total.inc(len(adaptive_chunks) if adaptive_chunks else len(chunks))
            doc.status = "embedding"; doc.updated_at = datetime.utcnow()
            session.commit()
            return {"document_id": str(doc.id), "chunks": len(adaptive_chunks) if adaptive_chunks else len(chunks), "status": doc.status}
        finally:
            session.close()

# ===== app/tasks/shared.py =====
from __future__ import annotations
import logging, time, uuid, contextlib, os
from datetime import datetime
from app.core.metrics import tasks_started_total, tasks_failed_total, task_duration_seconds

log = logging.getLogger("tasks")
def new_id() -> str: return str(uuid.uuid4())

class RetryableError(RuntimeError): ...
class FatalError(RuntimeError): ...

@contextlib.contextmanager
def task_metrics(task: str, queue: str):
    tasks_started_total.labels(queue=queue, task=task).inc()
    start = time.perf_counter()
    try:
        yield
    except Exception:
        tasks_failed_total.labels(queue=queue, task=task).inc()
        raise
    finally:
        task_duration_seconds.labels(task=task).observe(time.perf_counter() - start)

def env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, str(default)))
    except Exception:
        return default

# ===== app/tasks/upload_watch.py =====
from __future__ import annotations
from celery import shared_task
from app.core.s3 import get_minio
from app.core.db import SessionLocal
from app.core.config import settings
from app.models.rag import RagDocuments
from .shared import log, task_metrics, RetryableError
# Avoid circular import by importing inside the task function

@shared_task(name="app.tasks.upload.watch", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 100})
def watch(self, document_id: str, *, key: str) -> dict:
    with task_metrics("upload.watch", "watch"):
        s3 = get_minio()
        session = SessionLocal()
        try:
            doc = session.get(RagDocuments, document_id)
            if not doc:
                return {"ok": False, "error": "doc_not_found"}
            raw_bucket = settings.S3_BUCKET_RAG
            rel = key.split(raw_bucket + "/", 1)[-1] if key.startswith(raw_bucket + "/") else key
            try:
                s3.stat_object(raw_bucket, rel)
            except Exception:
                raise RetryableError("not_uploaded_yet")
            from app.services.rag_service import start_ingest_chain
            start_ingest_chain(document_id)
            return {"ok": True, "started": "ingest"}
        finally:
            session.close()

# ===== app/tasks/chat.py =====
from __future__ import annotations

import json
from typing import Dict, Any, List
from celery import shared_task
from app.core.config import settings
from app.core.db import SessionLocal
from app.core.metrics import chat_rag_usage_total, chat_rag_fallback_total
from .shared import log, RetryableError, task_metrics

@shared_task(name="app.tasks.chat.process_message", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 3})
def process_chat_message(self, chat_id: str, message: str, use_rag: bool = False) -> Dict[str, Any]:
    """
    Обработка сообщения чата (критический приоритет)
    """
    with task_metrics("chat.process_message", "chat"):
        session = SessionLocal()
        try:
            # Логика обработки сообщения чата
            log.info(f"Processing chat message for chat_id={chat_id}, use_rag={use_rag}")
            
            # Если используется RAG, выполняем поиск
            rag_context = ""
            if use_rag:
                try:
                    from app.services.rag_service import search
                    results = search(message, limit=5)
                    if results:
                        rag_context = "\n".join([r.get("text", "") for r in results])
                        log.info(f"RAG context found: {len(rag_context)} characters")
                    chat_rag_usage_total.labels(model="gpt-4", has_context="true").inc()
                except Exception as e:
                    log.warning(f"RAG search failed: {e}")
                    chat_rag_fallback_total.labels(reason="search_failed").inc()
                    chat_rag_usage_total.labels(model="gpt-4", has_context="false").inc()
            else:
                chat_rag_usage_total.labels(model="gpt-4", has_context="false").inc()
            
            # Формируем ответ
            response = {
                "chat_id": chat_id,
                "message": message,
                "rag_context": rag_context,
                "use_rag": use_rag,
                "processed": True
            }
            
            return response
            
        except Exception as e:
            log.error(f"Chat message processing failed: {e}")
            chat_rag_fallback_total.labels(reason="processing_failed").inc()
            raise RetryableError(f"Chat message processing failed: {e}")
        finally:
            session.close()

@shared_task(name="app.tasks.chat.generate_response", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 3})
def generate_chat_response(self, chat_id: str, message: str, rag_context: str = "") -> Dict[str, Any]:
    """
    Генерация ответа чата (критический приоритет)
    """
    with task_metrics("chat.generate_response", "chat"):
        try:
            # Логика генерации ответа через LLM
            log.info(f"Generating response for chat_id={chat_id}")
            
            # Здесь будет вызов LLM API
            response_text = f"Ответ на сообщение: {message}"
            if rag_context:
                response_text += f"\n\nКонтекст из RAG: {rag_context[:200]}..."
            
            response = {
                "chat_id": chat_id,
                "response": response_text,
                "rag_used": bool(rag_context),
                "generated": True
            }
            
            return response
            
        except Exception as e:
            log.error(f"Chat response generation failed: {e}")
            chat_rag_fallback_total.labels(reason="generation_failed").inc()
            raise RetryableError(f"Chat response generation failed: {e}")

@shared_task(name="app.tasks.chat.process_rag_query", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 3})
def process_rag_query(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Обработка RAG запроса (средний приоритет)
    """
    with task_metrics("chat.process_rag_query", "rag"):
        try:
            from app.services.rag_service import search
            
            log.info(f"Processing RAG query: {query[:100]}...")
            
            results = search(query, limit=limit)
            
            log.info(f"RAG query returned {len(results)} results")
            return results
            
        except Exception as e:
            log.error(f"RAG query processing failed: {e}")
            raise RetryableError(f"RAG query processing failed: {e}")

@shared_task(name="app.tasks.chat.cleanup_old_messages", bind=True)
def cleanup_old_messages(self, days_old: int = 30) -> Dict[str, Any]:
    """
    Очистка старых сообщений чата (низкий приоритет)
    """
    with task_metrics("chat.cleanup_old_messages", "cleanup"):
        session = SessionLocal()
        try:
            from datetime import datetime, timedelta
            from app.models.chat import ChatMessage
            
            cutoff_date = datetime.utcnow() - timedelta(days=days_old)
            
            # Удаляем старые сообщения
            old_messages = session.query(ChatMessage).filter(
                ChatMessage.created_at < cutoff_date
            ).all()
            
            count = len(old_messages)
            for message in old_messages:
                session.delete(message)
            
            session.commit()
            
            log.info(f"Cleaned up {count} old chat messages")
            
            return {
                "cleaned_messages": count,
                "cutoff_date": cutoff_date.isoformat(),
                "success": True
            }
            
        except Exception as e:
            log.error(f"Chat cleanup failed: {e}")
            session.rollback()
            raise RetryableError(f"Chat cleanup failed: {e}")
        finally:
            session.close()

# ===== app/tasks/analyze.py =====
from __future__ import annotations
from datetime import datetime
from celery import shared_task
from app.core.db import SessionLocal
from app.models.analyze import AnalysisDocuments
from .shared import log, RetryableError, task_metrics
from app.services.clients import llm_chat

@shared_task(name="app.tasks.analyze.run", bind=True, autoretry_for=(RetryableError,), retry_backoff=True, retry_kwargs={"max_retries": 5})
def run(self, job_id: str, *, pipeline: dict | None = None) -> dict:
    with task_metrics("analyze.run", "analyze"):
        session = SessionLocal()
        try:
            job = session.get(AnalysisDocuments, job_id)
            if not job:
                raise RetryableError("job_not_found")
            # Сбор сообщений для LLM
            system = {"role": "system", "content": "Ты — аналитик. Кратко резюмируй данные и выдай ключевые пункты."}
            user_msg = {"role": "user", "content": (pipeline or {}).get("prompt", "Проанализируй документ и верни краткое резюме.")}
            content = llm_chat([system, user_msg], temperature=float((pipeline or {}).get("temperature", 0.2)))
            job.status = "done"
            job.updated_at = datetime.utcnow()
            job.result = {"summary": content, "pipeline": pipeline or {}}
            session.commit()
            return {"job_id": str(job.id), "status": job.status}
        finally:
            session.close()

# ===== app/core/logging.py =====
import logging
import sys
from typing import Any, Dict, Optional
from datetime import datetime
import json
import traceback

class JSONFormatter(logging.Formatter):
    """Custom JSON formatter for structured logging"""
    
    def format(self, record: logging.LogRecord) -> str:
        log_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # Add exception info if present
        if record.exc_info:
            log_entry["exception"] = {
                "type": record.exc_info[0].__name__,
                "message": str(record.exc_info[1]),
                "traceback": traceback.format_exception(*record.exc_info)
            }
        
        # Add extra fields
        if hasattr(record, 'extra'):
            log_entry.update(record.extra)
        
        return json.dumps(log_entry, ensure_ascii=False)

def setup_logging(level: str = "INFO") -> None:
    """Setup structured logging configuration"""
    
    # Create logger
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, level.upper()))
    
    # Remove existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Create console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(getattr(logging, level.upper()))
    
    # Set formatter
    formatter = JSONFormatter()
    console_handler.setFormatter(formatter)
    
    # Add handler to logger
    logger.addHandler(console_handler)
    
    # Set specific loggers
    logging.getLogger("uvicorn").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("fastapi").setLevel(logging.INFO)
    logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)

def get_logger(name: str) -> logging.Logger:
    """Get logger instance"""
    return logging.getLogger(name)

class LoggerMixin:
    """Mixin class for adding logging to any class"""
    
    @property
    def logger(self) -> logging.Logger:
        return get_logger(f"{self.__class__.__module__}.{self.__class__.__name__}")

def log_api_call(
    logger: logging.Logger,
    method: str,
    endpoint: str,
    user_id: Optional[str] = None,
    status_code: Optional[int] = None,
    duration_ms: Optional[float] = None,
    error: Optional[str] = None
) -> None:
    """Log API call with structured data"""
    extra = {
        "api_call": {
            "method": method,
            "endpoint": endpoint,
            "user_id": user_id,
            "status_code": status_code,
            "duration_ms": duration_ms,
            "error": error
        }
    }
    
    if error:
        logger.error(f"API call failed: {method} {endpoint}", extra=extra)
    else:
        logger.info(f"API call: {method} {endpoint}", extra=extra)

def log_database_operation(
    logger: logging.Logger,
    operation: str,
    table: str,
    record_id: Optional[str] = None,
    duration_ms: Optional[float] = None,
    error: Optional[str] = None
) -> None:
    """Log database operation with structured data"""
    extra = {
        "db_operation": {
            "operation": operation,
            "table": table,
            "record_id": record_id,
            "duration_ms": duration_ms,
            "error": error
        }
    }
    
    if error:
        logger.error(f"Database operation failed: {operation} on {table}", extra=extra)
    else:
        logger.info(f"Database operation: {operation} on {table}", extra=extra)

def log_external_service(
    logger: logging.Logger,
    service: str,
    operation: str,
    duration_ms: Optional[float] = None,
    error: Optional[str] = None,
    **kwargs
) -> None:
    """Log external service call with structured data"""
    extra = {
        "external_service": {
            "service": service,
            "operation": operation,
            "duration_ms": duration_ms,
            "error": error,
            **kwargs
        }
    }
    
    if error:
        logger.error(f"External service call failed: {service}.{operation}", extra=extra)
    else:
        logger.info(f"External service call: {service}.{operation}", extra=extra)
# ===== app/core/metrics.py =====
from __future__ import annotations
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from fastapi.responses import Response

# API metrics
logins_total = Counter("logins_total", "Logins", ["result"])
refresh_total = Counter("refresh_total", "Refresh", ["result"])
chat_requests_total = Counter("chat_requests_total", "Total chat requests", ["use_rag", "model"])
chat_latency_seconds = Histogram("chat_latency_seconds", "Chat latency", ["use_rag"])
rag_search_total = Counter("rag_search_total", "Total RAG searches", ["model"])
rag_documents_total = Counter("rag_documents_total", "RAG documents by status", ["status"])

# Pipeline counters
rag_ingest_started_total = Counter("rag_ingest_started_total", "RAG ingest pipelines started")
rag_chunks_created_total = Counter("rag_chunks_created_total", "Total chunks created")
rag_vectors_upserted_total = Counter("rag_vectors_upserted_total", "Total vectors upserted to Qdrant")

# Tasks metrics
tasks_started_total = Counter("tasks_started_total", "Tasks started", ["queue", "task"])
tasks_failed_total  = Counter("tasks_failed_total",  "Tasks failed",  ["queue", "task"])
task_duration_seconds = Histogram("task_duration_seconds", "Task duration", ["task"])
embedding_batch_inflight = Gauge("embedding_batch_inflight", "Embedding batches in flight")

# External calls metrics
external_request_total = Counter("external_request_total", "External requests total", ["target", "status"])
external_request_seconds = Histogram("external_request_seconds", "External request latency", ["target"])

# Enhanced metrics for observability
llm_request_total = Counter("llm_request_total", "LLM requests total", ["model", "status"])
llm_latency_seconds = Histogram("llm_latency_seconds", "LLM request latency", ["model"], buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0])
llm_tokens_total = Counter("llm_tokens_total", "LLM tokens processed", ["model", "type"])  # type: input/output

embedding_request_total = Counter("embedding_request_total", "Embedding requests total", ["model", "status"])
embedding_latency_seconds = Histogram("embedding_latency_seconds", "Embedding request latency", ["model"], buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0])

document_processing_total = Counter("document_processing_total", "Document processing", ["format", "status"])
document_processing_seconds = Histogram("document_processing_seconds", "Document processing time", ["format"], buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 300.0])

chunking_quality = Histogram("chunking_quality", "Chunk quality metrics", ["metric"], buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])

reranking_total = Counter("reranking_total", "Reranking operations", ["method", "status"])
reranking_latency_seconds = Histogram("reranking_latency_seconds", "Reranking latency", ["method"])

pipeline_stage_duration = Histogram("pipeline_stage_duration_seconds", "Pipeline stage duration", ["stage"], buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 300.0, 600.0])
pipeline_errors_total = Counter("pipeline_errors_total", "Pipeline errors", ["stage", "error_type"])

qdrant_points = Gauge("qdrant_points", "Qdrant points", ["collection"])
qdrant_operations_total = Counter("qdrant_operations_total", "Qdrant operations", ["operation", "status"])
qdrant_latency_seconds = Histogram("qdrant_latency_seconds", "Qdrant operation latency", ["operation"])

# RAG-specific metrics
rag_ingest_stage_duration = Histogram("rag_ingest_stage_duration_seconds", "RAG ingest stage duration", ["stage"], buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 300.0])
rag_ingest_errors_total = Counter("rag_ingest_errors_total", "RAG ingest errors", ["stage", "error_type"])

rag_vectors_total = Gauge("rag_vectors_total", "Total vectors in Qdrant", ["collection"])
rag_chunks_total = Gauge("rag_chunks_total", "Total chunks in Qdrant", ["collection"])

rag_search_latency_seconds = Histogram("rag_search_latency_seconds", "RAG search latency", ["model"], buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0])
rag_search_top_k = Histogram("rag_search_top_k", "RAG search top_k distribution", ["model"], buckets=[1, 3, 5, 10, 20, 50])
rag_search_scores = Histogram("rag_search_scores", "RAG search score distribution", ["model"], buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
rag_search_coverage = Histogram("rag_search_coverage", "RAG search document coverage", ["model"], buckets=[1, 2, 3, 5, 10, 20])

rag_quality_mrr = Histogram("rag_quality_mrr", "RAG quality MRR@K", ["k"], buckets=[1, 3, 5, 10])
rag_quality_ndcg = Histogram("rag_quality_ndcg", "RAG quality nDCG@K", ["k"], buckets=[1, 3, 5, 10])

chat_rag_usage_total = Counter("chat_rag_usage_total", "Chat RAG usage", ["model", "has_context"])
chat_rag_fallback_total = Counter("chat_rag_fallback_total", "Chat RAG fallback", ["reason"])

# System health metrics
memory_usage_bytes = Gauge("memory_usage_bytes", "Memory usage", ["type"])
cpu_usage_percent = Gauge("cpu_usage_percent", "CPU usage percentage")
disk_usage_bytes = Gauge("disk_usage_bytes", "Disk usage", ["path"])

# Admin operations metrics
admin_operations_total = Counter("admin_operations_total", "Admin operations", ["operation", "status"])
admin_user_operations_total = Counter("admin_user_operations_total", "Admin user operations", ["action", "status"])
admin_token_operations_total = Counter("admin_token_operations_total", "Admin token operations", ["action", "status"])
admin_audit_operations_total = Counter("admin_audit_operations_total", "Admin audit operations", ["action", "status"])

# Rate limiting metrics
rate_limit_hits_total = Counter("rate_limit_hits_total", "Rate limit hits", ["endpoint", "ip"])
rate_limit_blocks_total = Counter("rate_limit_blocks_total", "Rate limit blocks", ["endpoint", "ip"])

# Authentication metrics
auth_attempts_total = Counter("auth_attempts_total", "Authentication attempts", ["method", "result"])
password_reset_requests_total = Counter("password_reset_requests_total", "Password reset requests", ["status"])
password_reset_completions_total = Counter("password_reset_completions_total", "Password reset completions", ["status"])

# Email metrics (if enabled)
email_sent_total = Counter("email_sent_total", "Emails sent", ["type", "status"])
email_errors_total = Counter("email_errors_total", "Email errors", ["type", "error"])

# CSRF metrics
csrf_tokens_generated_total = Counter("csrf_tokens_generated_total", "CSRF tokens generated")
csrf_validation_failures_total = Counter("csrf_validation_failures_total", "CSRF validation failures", ["reason"])

def prometheus_endpoint() -> Response:
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

# ===== app/core/db.py =====
from __future__ import annotations
from contextlib import contextmanager
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from .config import settings

engine = create_engine(settings.DB_URL, pool_pre_ping=True, future=True)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False, future=True)

def get_session():
    db = SessionLocal()
    try:
        yield db
        db.commit()
    except Exception:
        db.rollback()
        raise
    finally:
        db.close()

@contextmanager
def session_scope():
    """Provide a transactional scope around a series of operations."""
    db = SessionLocal()
    try:
        yield db
        db.commit()
    except Exception:
        db.rollback()
        raise
    finally:
        db.close()

# ===== app/core/pat_validation.py =====
"""
PAT (Personal Access Token) scope validation
"""
from __future__ import annotations
from typing import List, Set, Optional
from fastapi import HTTPException, status

# Valid scope patterns
VALID_SCOPE_PATTERNS = {
    "api:read",      # Read API access
    "api:write",     # Write API access
    "api:admin",     # Admin API access
    "rag:read",      # Read RAG documents
    "rag:write",     # Write RAG documents
    "rag:admin",     # Admin RAG operations
    "chat:read",     # Read chat history
    "chat:write",    # Send messages
    "chat:admin",    # Admin chat operations
    "users:read",    # Read user data
    "users:write",   # Write user data
    "users:admin",   # Admin user operations
}

# Scope hierarchy (higher scopes include lower ones)
SCOPE_HIERARCHY = {
    "api:admin": ["api:read", "api:write"],
    "api:write": ["api:read"],
    "rag:admin": ["rag:read", "rag:write"],
    "rag:write": ["rag:read"],
    "chat:admin": ["chat:read", "chat:write"],
    "chat:write": ["chat:read"],
    "users:admin": ["users:read", "users:write"],
    "users:write": ["users:read"],
}

def validate_scopes(scopes: Optional[List[str]]) -> List[str]:
    """
    Validate and normalize PAT scopes
    
    Args:
        scopes: List of scope strings
        
    Returns:
        Normalized list of valid scopes
        
    Raises:
        HTTPException: If scopes are invalid
    """
    if not scopes:
        return []
    
    if not isinstance(scopes, list):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Scopes must be a list"
        )
    
    # Check for invalid scopes
    invalid_scopes = []
    for scope in scopes:
        if not isinstance(scope, str):
            invalid_scopes.append(str(scope))
        elif scope not in VALID_SCOPE_PATTERNS:
            invalid_scopes.append(scope)
    
    if invalid_scopes:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid scopes: {', '.join(invalid_scopes)}. Valid scopes: {', '.join(sorted(VALID_SCOPE_PATTERNS))}"
        )
    
    # Expand hierarchical scopes
    expanded_scopes = set(scopes)
    for scope in scopes:
        if scope in SCOPE_HIERARCHY:
            expanded_scopes.update(SCOPE_HIERARCHY[scope])
    
    return sorted(list(expanded_scopes))

def check_scope_permission(user_scopes: List[str], required_scope: str) -> bool:
    """
    Check if user has permission for required scope
    
    Args:
        user_scopes: List of user's scopes
        required_scope: Required scope to check
        
    Returns:
        True if user has permission
    """
    if not user_scopes:
        return False
    
    # Direct scope match
    if required_scope in user_scopes:
        return True
    
    # Check if user has a higher-level scope that includes this one
    for user_scope in user_scopes:
        if user_scope in SCOPE_HIERARCHY and required_scope in SCOPE_HIERARCHY[user_scope]:
            return True
    
    return False

def get_scope_description(scope: str) -> str:
    """Get human-readable description for a scope"""
    descriptions = {
        "api:read": "Read API access",
        "api:write": "Write API access", 
        "api:admin": "Admin API access",
        "rag:read": "Read RAG documents",
        "rag:write": "Write RAG documents",
        "rag:admin": "Admin RAG operations",
        "chat:read": "Read chat history",
        "chat:write": "Send messages",
        "chat:admin": "Admin chat operations",
        "users:read": "Read user data",
        "users:write": "Write user data",
        "users:admin": "Admin user operations",
    }
    return descriptions.get(scope, scope)

def format_scopes_for_display(scopes: List[str]) -> List[dict]:
    """Format scopes for API response with descriptions"""
    return [
        {
            "scope": scope,
            "description": get_scope_description(scope)
        }
        for scope in sorted(scopes)
    ]

# ===== app/core/rate_limiting.py =====
"""
Rate limiting middleware for authentication endpoints
"""
from __future__ import annotations
import time
from typing import Dict, Optional
from fastapi import Request, HTTPException, status
from fastapi.responses import JSONResponse
import redis
from .config import settings

# Redis connection for rate limiting
redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)

def get_client_ip(request: Request) -> str:
    """Extract client IP from request, considering X-Forwarded-For header"""
    # Check for forwarded IP first (for load balancers/proxies)
    forwarded_for = request.headers.get("X-Forwarded-For")
    if forwarded_for:
        # Take the first IP in the chain
        return forwarded_for.split(",")[0].strip()
    
    # Fall back to direct connection IP
    if hasattr(request, "client") and request.client:
        return request.client.host
    
    return "unknown"

def check_rate_limit(
    request: Request,
    key_prefix: str = "rate_limit",
    max_attempts: int = None,
    window_seconds: int = None
) -> bool:
    """
    Check if request is within rate limits
    
    Args:
        request: FastAPI request object
        key_prefix: Prefix for Redis key
        max_attempts: Maximum attempts per window (uses config if None)
        window_seconds: Time window in seconds (uses config if None)
    
    Returns:
        True if request is allowed, False if rate limited
    """
    if max_attempts is None:
        max_attempts = settings.RATE_LIMIT_LOGIN_ATTEMPTS
    if window_seconds is None:
        window_seconds = settings.RATE_LIMIT_LOGIN_WINDOW
    
    client_ip = get_client_ip(request)
    current_time = int(time.time())
    window_start = current_time - window_seconds
    
    # Create rate limit key
    rate_key = f"{key_prefix}:{client_ip}"
    
    try:
        # Use Redis pipeline for atomic operations
        pipe = redis_client.pipeline()
        
        # Remove expired entries
        pipe.zremrangebyscore(rate_key, 0, window_start)
        
        # Count current attempts
        pipe.zcard(rate_key)
        
        # Add current attempt
        pipe.zadd(rate_key, {str(current_time): current_time})
        
        # Set expiration
        pipe.expire(rate_key, window_seconds)
        
        results = pipe.execute()
        current_attempts = results[1]
        
        return current_attempts < max_attempts
        
    except Exception as e:
        # If Redis is unavailable, allow the request (fail open)
        print(f"Rate limiting error: {e}")
        return True

def rate_limit_middleware(
    max_attempts: int = None,
    window_seconds: int = None,
    key_prefix: str = "rate_limit"
):
    """
    Rate limiting decorator for endpoints
    
    Usage:
        @app.post("/auth/login")
        @rate_limit_middleware(max_attempts=5, window_seconds=300)
        async def login(...):
            ...
    """
    def decorator(func):
        async def wrapper(*args, **kwargs):
            # Find request object in args
            request = None
            for arg in args:
                if isinstance(arg, Request):
                    request = arg
                    break
            
            if not request:
                return await func(*args, **kwargs)
            
            if not check_rate_limit(request, key_prefix, max_attempts, window_seconds):
                return JSONResponse(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    content={
                        "error": "rate_limit_exceeded",
                        "message": f"Too many requests. Try again in {window_seconds or settings.RATE_LIMIT_LOGIN_WINDOW} seconds.",
                        "retry_after": window_seconds or settings.RATE_LIMIT_LOGIN_WINDOW
                    }
                )
            
            return await func(*args, **kwargs)
        return wrapper
    return decorator

def clear_rate_limit(request: Request, key_prefix: str = "rate_limit") -> None:
    """Clear rate limit for a specific client (e.g., after successful login)"""
    client_ip = get_client_ip(request)
    rate_key = f"{key_prefix}:{client_ip}"
    
    try:
        redis_client.delete(rate_key)
    except Exception as e:
        print(f"Error clearing rate limit: {e}")

# ===== app/core/config.py =====
from __future__ import annotations
import os
from typing import Optional

def _env(key: str, default: Optional[str] = None) -> Optional[str]:
    """Fetch env var trying both UNDER_SCORE and DOT.SEPARATED names."""
    return os.getenv(key) or os.getenv(key.replace("_", "."), default)

class settings:
    """Central config.
    Important S3 envs:
      - S3_ENDPOINT: internal endpoint for SDK (e.g. http://minio:9000) — used by the app in Docker network.
      - S3_PUBLIC_ENDPOINT: external/public base URL for links (e.g. https://10.4.4.2). Can include scheme http/https.
        If not set, falls back to S3_ENDPOINT.
      Buckets can be set via either S3_BUCKET_* or S3.BUCKET_* names.
    """
    # Core
    # Use psycopg3 driver by default to match dependencies
    DB_URL = _env("DB_URL") or _env("DB.URL") or "postgresql+psycopg://postgres:postgres@postgres:5432/app"
    REDIS_URL = _env("REDIS_URL") or _env("REDIS.URL") or "redis://redis:6379/0"
    QDRANT_URL = _env("QDRANT_URL") or _env("QDRANT.URL") or "http://qdrant:6333"
    LLM_URL = _env("LLM_URL") or _env("LLM.URL") or "http://llm:8002"
    EMB_URL = _env("EMB_URL") or _env("EMB.URL") or "http://emb:8001"

    # Auth
    JWT_SECRET = os.getenv("JWT_SECRET")
    if not JWT_SECRET:
        import secrets
        JWT_SECRET = secrets.token_urlsafe(32)
        print("⚠️  WARNING: JWT_SECRET not set, using random secret. Set JWT_SECRET env var for production!")
    
    PASSWORD_PEPPER = os.getenv("PASSWORD_PEPPER", "")
    if not PASSWORD_PEPPER:
        import secrets
        PASSWORD_PEPPER = secrets.token_urlsafe(32)
        print("⚠️  WARNING: PASSWORD_PEPPER not set, using random pepper. Set PASSWORD_PEPPER env var for production!")
    
    ACCESS_TTL_SECONDS = int(os.getenv("ACCESS_TTL_SECONDS", "900"))  # 15 минут вместо 1 часа
    REFRESH_TTL_DAYS = int(os.getenv("REFRESH_TTL_DAYS", "7"))  # 7 дней вместо 30
    REFRESH_ROTATING = (str(os.getenv("REFRESH_ROTATING", "true")).lower() in ("1","true","yes"))

    # S3 / MinIO
    S3_ENDPOINT = _env("S3_ENDPOINT") or _env("S3.ENDPOINT") or "http://minio:9000"
    S3_PUBLIC_ENDPOINT = _env("S3_PUBLIC_ENDPOINT") or _env("S3.PUBLIC_ENDPOINT") or S3_ENDPOINT
    S3_ACCESS_KEY = _env("S3_ACCESS_KEY") or _env("S3.ACCESS_KEY") or "minio"
    S3_SECRET_KEY = _env("S3_SECRET_KEY") or _env("S3.SECRET_KEY") or "minio123"

    S3_BUCKET_RAG = _env("S3_BUCKET_RAG") or _env("S3.BUCKET_RAG") or "rag"
    S3_BUCKET_ANALYSIS = _env("S3_BUCKET_ANALYSIS") or _env("S3.BUCKET_ANALYSIS") or "analysis"

    # Email (optional)
    EMAIL_ENABLED = (os.getenv("EMAIL_ENABLED", "false").lower() in ("1", "true", "yes"))
    SMTP_HOST = os.getenv("SMTP_HOST", "localhost")
    SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
    SMTP_USER = os.getenv("SMTP_USER", "")
    SMTP_PASSWORD = os.getenv("SMTP_PASSWORD", "")
    SMTP_USE_TLS = (os.getenv("SMTP_USE_TLS", "true").lower() in ("1", "true", "yes"))
    FROM_EMAIL = os.getenv("FROM_EMAIL", "noreply@ml-portal.local")
    
    # Password policy
    PASSWORD_MIN_LENGTH = int(os.getenv("PASSWORD_MIN_LENGTH", "12"))
    PASSWORD_REQUIRE_UPPERCASE = (os.getenv("PASSWORD_REQUIRE_UPPERCASE", "true").lower() in ("1", "true", "yes"))
    PASSWORD_REQUIRE_LOWERCASE = (os.getenv("PASSWORD_REQUIRE_LOWERCASE", "true").lower() in ("1", "true", "yes"))
    PASSWORD_REQUIRE_DIGITS = (os.getenv("PASSWORD_REQUIRE_DIGITS", "true").lower() in ("1", "true", "yes"))
    PASSWORD_REQUIRE_SPECIAL = (os.getenv("PASSWORD_REQUIRE_SPECIAL", "true").lower() in ("1", "true", "yes"))
    PASSWORD_PEPPER = os.getenv("PASSWORD_PEPPER", "")
    
    # Rate limiting
    RATE_LIMIT_LOGIN_ATTEMPTS = int(os.getenv("RATE_LIMIT_LOGIN_ATTEMPTS", "10"))
    RATE_LIMIT_LOGIN_WINDOW = int(os.getenv("RATE_LIMIT_LOGIN_WINDOW", "60"))
    
    # CORS
    CORS_ENABLED = (os.getenv("CORS_ENABLED", "true").lower() in ("1", "true", "yes"))
    CORS_ORIGINS = os.getenv("CORS_ORIGINS", "*").split(",")
    CORS_ALLOW_CREDENTIALS = (os.getenv("CORS_ALLOW_CREDENTIALS", "false").lower() in ("1", "true", "yes"))
    
    # Authentication modes
    AUTH_MODE = os.getenv("AUTH_MODE", "bearer")  # "bearer" or "cookie"
    COOKIE_AUTH_ENABLED = (os.getenv("COOKIE_AUTH_ENABLED", "false").lower() in ("1", "true", "yes"))
    CSRF_ENABLED = (os.getenv("CSRF_ENABLED", "false").lower() in ("1", "true", "yes"))
    
    # Reader permissions
    ALLOW_READER_UPLOADS = (os.getenv("ALLOW_READER_UPLOADS", "false").lower() in ("1", "true", "yes"))
    
    # Debug mode
    DEBUG = (os.getenv("DEBUG", "false").lower() in ("1", "true", "yes"))
    
    # Health
    HEALTH_DEEP = (os.getenv("HEALTH_DEEP", "0") == "1")

# ===== app/core/qdrant.py =====
from __future__ import annotations
from qdrant_client import QdrantClient
from .config import settings

_client: QdrantClient | None = None

def get_qdrant() -> QdrantClient:
    global _client
    if _client is None:
        _client = QdrantClient(url=settings.QDRANT_URL, prefer_grpc=False)
    return _client

# ===== app/core/cookie_auth.py =====
"""
Cookie-based authentication with CSRF protection
"""
from __future__ import annotations
import secrets
from typing import Optional
from fastapi import Request, Response, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from starlette.middleware.base import BaseHTTPMiddleware

from .config import settings
from .security import decode_jwt

# CSRF token storage (in production, use Redis)
_csrf_tokens = {}

class CSRFMiddleware(BaseHTTPMiddleware):
    """CSRF protection middleware"""
    
    def __init__(self, app, secret_key: str = None):
        super().__init__(app)
        self.secret_key = secret_key or settings.JWT_SECRET
    
    async def dispatch(self, request: Request, call_next):
        # Skip CSRF for safe methods and auth endpoints
        if request.method in ["GET", "HEAD", "OPTIONS"]:
            return await call_next(request)
        
        # Skip CSRF for auth endpoints
        if request.url.path.startswith("/api/auth/"):
            return await call_next(request)
        
        # Check CSRF token
        csrf_token = request.headers.get("X-CSRF-Token")
        if not csrf_token:
            return HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail={"error": {"code": "csrf_missing", "message": "CSRF token required"}}
            )
        
        # Validate CSRF token
        if not self._validate_csrf_token(csrf_token, request):
            return HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail={"error": {"code": "csrf_invalid", "message": "Invalid CSRF token"}}
            )
        
        return await call_next(request)
    
    def _validate_csrf_token(self, token: str, request: Request) -> bool:
        """Validate CSRF token"""
        # In production, implement proper CSRF token validation
        # For now, just check if token exists in our storage
        return token in _csrf_tokens

def generate_csrf_token() -> str:
    """Generate CSRF token"""
    token = secrets.token_urlsafe(32)
    _csrf_tokens[token] = True
    return token

def set_auth_cookies(response: Response, access_token: str, refresh_token: str, csrf_token: str):
    """Set authentication cookies"""
    # Access token cookie (short-lived)
    response.set_cookie(
        key="access_token",
        value=access_token,
        max_age=settings.ACCESS_TTL_SECONDS,
        httponly=True,
        secure=not settings.DEBUG,  # HTTPS only in production
        samesite="lax"
    )
    
    # Refresh token cookie (long-lived)
    response.set_cookie(
        key="refresh_token", 
        value=refresh_token,
        max_age=settings.REFRESH_TTL_DAYS * 86400,
        httponly=True,
        secure=not settings.DEBUG,
        samesite="lax"
    )
    
    # CSRF token cookie (not httpOnly for JS access)
    response.set_cookie(
        key="csrf_token",
        value=csrf_token,
        max_age=settings.REFRESH_TTL_DAYS * 86400,
        httponly=False,  # Allow JS access
        secure=not settings.DEBUG,
        samesite="lax"
    )

def clear_auth_cookies(response: Response):
    """Clear authentication cookies"""
    response.delete_cookie("access_token")
    response.delete_cookie("refresh_token") 
    response.delete_cookie("csrf_token")

def get_token_from_cookie(request: Request) -> Optional[str]:
    """Get access token from cookie"""
    return request.cookies.get("access_token")

def get_refresh_token_from_cookie(request: Request) -> Optional[str]:
    """Get refresh token from cookie"""
    return request.cookies.get("refresh_token")

def get_csrf_token_from_cookie(request: Request) -> Optional[str]:
    """Get CSRF token from cookie"""
    return request.cookies.get("csrf_token")

class CookieAuthBearer(HTTPBearer):
    """Cookie-based authentication with Bearer fallback"""
    
    def __init__(self, auto_error: bool = True):
        super().__init__(auto_error=auto_error)
    
    async def __call__(self, request: Request) -> Optional[HTTPAuthorizationCredentials]:
        # Try cookie first
        token = get_token_from_cookie(request)
        if token:
            return HTTPAuthorizationCredentials(scheme="Bearer", credentials=token)
        
        # Fallback to header
        return await super().__call__(request)

# Cookie auth dependency
cookie_auth = CookieAuthBearer(auto_error=False)

def get_cookie_token(credentials: Optional[HTTPAuthorizationCredentials] = None) -> Optional[str]:
    """Get token from cookie or header"""
    if credentials and credentials.scheme.lower() == "bearer":
        return credentials.credentials
    return None

# ===== app/core/idempotency.py =====
from __future__ import annotations
import os, json, base64, hashlib
from typing import List, Tuple, Optional, Dict, Any
from starlette.types import ASGIApp, Scope, Receive, Send, Message
from app.core.redis import get_redis

SAFE_METHODS = {"GET", "HEAD", "OPTIONS"}
DEFAULT_TTL = int(os.getenv("IDEMPOTENCY_TTL_SECONDS", "86400"))
ENABLED = os.getenv("IDEMPOTENCY_ENABLED", "1") not in {"0", "false", "False"}
MAX_CAPTURE_BYTES = int(os.getenv("IDEMPOTENCY_MAX_BYTES", "1048576"))  # 1 MiB

def _get_header(headers: List[Tuple[bytes, bytes]], name: str) -> Optional[bytes]:
    name_b = name.lower().encode()
    for k, v in headers:
        if k.lower() == name_b:
            return v
    return None

class IdempotencyMiddleware:
    def __init__(self, app: ASGIApp):
        self.app = app

    async def __call__(self, scope: Scope, receive: Receive, send: Send):
        if not ENABLED or scope.get("type") != "http":
            return await self.app(scope, receive, send)

        method = scope.get("method") or "GET"
        if method in SAFE_METHODS:
            return await self.app(scope, receive, send)

        headers: List[Tuple[bytes, bytes]] = scope.get("headers") or []
        idem_key_b = _get_header(headers, "idempotency-key")
        if not idem_key_b:
            return await self.app(scope, receive, send)

        path = scope.get("path") or "/"
        auth_b = _get_header(headers, "authorization") or b""
        user_hash = hashlib.sha256(auth_b).hexdigest()[:16] if auth_b else "anon"
        raw_key = b"|".join([method.encode(), path.encode(), idem_key_b, user_hash.encode()])
        key_hash = hashlib.sha256(raw_key).hexdigest()
        rkey = f"idemp:v1:{key_hash}"

        redis = get_redis()
        try:
            cached = await redis.get(rkey)  # type: ignore[attr-defined]
        except Exception:
            cached = None

        if cached:
            try:
                data = json.loads(cached)
                body = base64.b64decode(data.get("body_b64", ""))
                from starlette.responses import Response
                headers_dict = data.get("headers") or {}
                headers_dict["content-length"] = str(len(body))
                resp = Response(content=body, status_code=int(data.get("status", 200)), headers=headers_dict, media_type=None)
                return await resp(scope, receive, send)
            except Exception:
                pass

        started: Dict[str, Any] = {"status": None, "headers": []}
        chunks: list[bytes] = []
        total = 0
        is_streaming = False

        async def send_wrapper(message: Message):
            nonlocal total, is_streaming
            if message["type"] == "http.response.start":
                started["status"] = message["status"]
                started["headers"] = message.get("headers", [])
                ctype = _get_header(started["headers"], "content-type") or b""
                if b"text/event-stream" in ctype or b"stream" in ctype:
                    is_streaming = True
                return await send(message)
            elif message["type"] == "http.response.body":
                body = message.get("body", b"") or b""
                if body:
                    total += len(body)
                    if total <= MAX_CAPTURE_BYTES:
                        chunks.append(body)
                    else:
                        is_streaming = True
                return await send(message)
            else:
                return await send(message)

        await self.app(scope, receive, send_wrapper)

        if not is_streaming and started["status"] is not None:
            try:
                body_bytes = b"".join(chunks)
                headers_dict = {k.decode().lower(): v.decode() for k, v in (started["headers"] or [])}
                payload = {
                    "status": int(started["status"]),
                    "headers": headers_dict,
                    "body_b64": base64.b64encode(body_bytes).decode(),
                }
                await redis.setex(rkey, DEFAULT_TTL, json.dumps(payload))  # type: ignore[attr-defined]
            except Exception:
                pass

# ===== app/core/security_headers.py =====
"""
Security headers middleware для защиты от различных атак
"""
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
import os

class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """Middleware для добавления защитных HTTP-заголовков"""
    
    def __init__(self, app, environment: str = "development"):
        super().__init__(app)
        self.environment = environment
    
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        
        # X-Frame-Options - защита от clickjacking
        response.headers["X-Frame-Options"] = "DENY"
        
        # X-Content-Type-Options - защита от MIME-sniffing
        response.headers["X-Content-Type-Options"] = "nosniff"
        
        # Referrer-Policy - контроль реферальной информации
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
        
        # X-XSS-Protection (устаревший, но для совместимости)
        response.headers["X-XSS-Protection"] = "1; mode=block"
        
        # Strict-Transport-Security (только для HTTPS)
        if request.url.scheme == "https":
            response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        
        # Content-Security-Policy (только в production)
        if self.environment == "production":
            csp = (
                "default-src 'self'; "
                "script-src 'self' 'unsafe-inline'; "
                "style-src 'self' 'unsafe-inline'; "
                "img-src 'self' data: https:; "
                "font-src 'self'; "
                "connect-src 'self'; "
                "frame-ancestors 'none';"
            )
            response.headers["Content-Security-Policy"] = csp
        else:
            # В development более мягкая политика
            response.headers["Content-Security-Policy"] = "default-src 'self' 'unsafe-inline' 'unsafe-eval'"
        
        # Permissions-Policy (новый стандарт)
        permissions = (
            "camera=(), "
            "microphone=(), "
            "geolocation=(), "
            "payment=(), "
            "usb=(), "
            "magnetometer=(), "
            "gyroscope=(), "
            "speaker=()"
        )
        response.headers["Permissions-Policy"] = permissions
        
        return response

# ===== app/core/security.py =====
from __future__ import annotations
import time
import jwt
import re
from typing import Any, Dict, Optional
from argon2 import PasswordHasher
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from .config import settings

ph = PasswordHasher()
http_bearer = HTTPBearer(auto_error=False)

def hash_password(password: str) -> str:
    """Hash password with Argon2id and pepper"""
    # Add pepper if configured
    if hasattr(settings, 'PASSWORD_PEPPER') and settings.PASSWORD_PEPPER:
        password = password + settings.PASSWORD_PEPPER
    return ph.hash(password)

def verify_password(password: str, password_hash: str) -> bool:
    """Verify password with Argon2id and pepper"""
    try:
        # Add pepper if configured
        if hasattr(settings, 'PASSWORD_PEPPER') and settings.PASSWORD_PEPPER:
            password = password + settings.PASSWORD_PEPPER
        return ph.verify(password_hash, password)
    except Exception:
        return False

def validate_password_strength(password: str) -> tuple[bool, str]:
    """Validate password strength according to policy"""
    errors = []
    
    if len(password) < settings.PASSWORD_MIN_LENGTH:
        errors.append(f"Password must be at least {settings.PASSWORD_MIN_LENGTH} characters long")
    
    if settings.PASSWORD_REQUIRE_UPPERCASE and not re.search(r'[A-Z]', password):
        errors.append("Password must contain at least one uppercase letter")
    
    if settings.PASSWORD_REQUIRE_LOWERCASE and not re.search(r'[a-z]', password):
        errors.append("Password must contain at least one lowercase letter")
    
    if settings.PASSWORD_REQUIRE_DIGITS and not re.search(r'\d', password):
        errors.append("Password must contain at least one digit")
    
    if settings.PASSWORD_REQUIRE_SPECIAL and not re.search(r'[!@#$%^&*(),.?":{}|<>]', password):
        errors.append("Password must contain at least one special character")
    
    if errors:
        return False, "; ".join(errors)
    
    return True, ""

def encode_jwt(payload: Dict[str, Any], *, ttl_seconds: int) -> str:
    now = int(time.time())
    to_encode = {"iat": now, "exp": now + ttl_seconds, **payload}
    return jwt.encode(to_encode, settings.JWT_SECRET, algorithm="HS256")

def decode_jwt(token: str) -> Dict[str, Any]:
    try:
        return jwt.decode(token, settings.JWT_SECRET, algorithms=["HS256"])
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="token_expired")
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="invalid_token")

# FastAPI dependency
def get_bearer_token(credentials: HTTPAuthorizationCredentials | None = Depends(http_bearer)) -> str:
    if not credentials or credentials.scheme.lower() != "bearer":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="missing_bearer")
    return credentials.credentials

# ===== app/core/cache.py =====
import json
import pickle
from typing import Any, Optional, Union
from datetime import datetime, timedelta
import redis
from app.core.config import settings
from app.core.logging import get_logger

logger = get_logger(__name__)

class CacheManager:
    """Redis cache manager with JSON serialization"""
    
    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=settings.REDIS_DB,
            decode_responses=True
        )
        self.default_ttl = 3600  # 1 hour
    
    def _serialize(self, value: Any) -> str:
        """Serialize value to JSON string"""
        try:
            return json.dumps(value, default=str)
        except (TypeError, ValueError):
            # Fallback to pickle for complex objects
            return pickle.dumps(value).hex()
    
    def _deserialize(self, value: str) -> Any:
        """Deserialize JSON string to value"""
        try:
            return json.loads(value)
        except (TypeError, ValueError, json.JSONDecodeError):
            # Fallback to pickle for complex objects
            return pickle.loads(bytes.fromhex(value))
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        try:
            value = self.redis_client.get(key)
            if value is None:
                return None
            return self._deserialize(value)
        except Exception as e:
            logger.error(f"Cache get error for key {key}: {e}")
            return None
    
    def set(
        self, 
        key: str, 
        value: Any, 
        ttl: Optional[int] = None
    ) -> bool:
        """Set value in cache with TTL"""
        try:
            serialized_value = self._serialize(value)
            ttl = ttl or self.default_ttl
            return self.redis_client.setex(key, ttl, serialized_value)
        except Exception as e:
            logger.error(f"Cache set error for key {key}: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """Delete key from cache"""
        try:
            return bool(self.redis_client.delete(key))
        except Exception as e:
            logger.error(f"Cache delete error for key {key}: {e}")
            return False
    
    def exists(self, key: str) -> bool:
        """Check if key exists in cache"""
        try:
            return bool(self.redis_client.exists(key))
        except Exception as e:
            logger.error(f"Cache exists error for key {key}: {e}")
            return False
    
    def get_or_set(
        self, 
        key: str, 
        factory_func, 
        ttl: Optional[int] = None,
        *args,
        **kwargs
    ) -> Any:
        """Get value from cache or set it using factory function"""
        value = self.get(key)
        if value is not None:
            return value
        
        # Generate value using factory function
        value = factory_func(*args, **kwargs)
        self.set(key, value, ttl)
        return value
    
    def invalidate_pattern(self, pattern: str) -> int:
        """Invalidate all keys matching pattern"""
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                return self.redis_client.delete(*keys)
            return 0
        except Exception as e:
            logger.error(f"Cache pattern invalidation error for {pattern}: {e}")
            return 0

# Global cache instance
cache = CacheManager()

# Cache decorators
def cached(ttl: int = 3600, key_prefix: str = ""):
    """Decorator for caching function results"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # Generate cache key
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(sorted(kwargs.items())))}"
            
            # Try to get from cache
            result = cache.get(cache_key)
            if result is not None:
                return result
            
            # Execute function and cache result
            result = func(*args, **kwargs)
            cache.set(cache_key, result, ttl)
            return result
        
        return wrapper
    return decorator

def cache_invalidate(pattern: str):
    """Decorator for invalidating cache after function execution"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            cache.invalidate_pattern(pattern)
            return result
        return wrapper
    return decorator

# Cache key generators
def chat_key(chat_id: str) -> str:
    return f"chat:{chat_id}"

def chat_messages_key(chat_id: str, limit: int = 50, cursor: str = None) -> str:
    cursor_part = f":{cursor}" if cursor else ""
    return f"chat_messages:{chat_id}:{limit}{cursor_part}"

def rag_document_key(doc_id: str) -> str:
    return f"rag_document:{doc_id}"

def rag_documents_key(page: int, size: int, status: str = None, search: str = None) -> str:
    status_part = f":{status}" if status else ""
    search_part = f":{search}" if search else ""
    return f"rag_documents:{page}:{size}{status_part}{search_part}"

def rag_metrics_key() -> str:
    return "rag_metrics"

def user_chats_key(user_id: str) -> str:
    return f"user_chats:{user_id}"

# ===== app/core/__init__.py =====
# core package init

# ===== app/core/request_id.py =====
"""
Request ID middleware для трассировки запросов
"""
import uuid
from contextvars import ContextVar
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware

# Context variable для хранения request ID
request_id_ctx: ContextVar[str] = ContextVar('request_id', default=None)

class RequestIDMiddleware(BaseHTTPMiddleware):
    """Middleware для генерации и передачи request ID"""
    
    async def dispatch(self, request: Request, call_next):
        # Генерируем или получаем request ID
        request_id = request.headers.get('X-Request-ID') or str(uuid.uuid4())
        
        # Сохраняем в контекст
        request_id_ctx.set(request_id)
        
        # Обрабатываем запрос
        response = await call_next(request)
        
        # Добавляем request ID в заголовки ответа
        response.headers['X-Request-ID'] = request_id
        
        return response

def get_request_id() -> str:
    """Получить текущий request ID"""
    return request_id_ctx.get() or "unknown"

# ===== app/core/redis.py =====
from __future__ import annotations
from typing import Optional
from redis.asyncio import Redis
from .config import settings

_redis: Optional[Redis] = None

def get_redis() -> Redis:
    global _redis
    if _redis is None:
        _redis = Redis.from_url(settings.REDIS_URL, decode_responses=True)
    return _redis

# ===== app/core/errors.py =====
from __future__ import annotations
from fastapi import Request
from fastapi.responses import JSONResponse
from fastapi import status
from typing import Any, Optional, Dict
from .request_id import get_request_id

class APIError(Exception):
    def __init__(self, code: str, message: str, *, http_status: int = 400, details: Optional[Dict[str, Any]] = None):
        super().__init__(message)
        self.code = code
        self.http_status = http_status
        self.details = details or {}

def format_error_payload(code: str, message: str, details: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    return {
        "error": {"code": code, "message": message, "details": details or {}},
        "request_id": get_request_id(),
    }

async def http_exception_handler(request: Request, exc: APIError):
    payload = format_error_payload(exc.code, str(exc), exc.details)
    return JSONResponse(status_code=exc.http_status, content=payload)

async def unhandled_exception_handler(request: Request, exc: Exception):
    payload = format_error_payload("internal_error", "Internal Server Error")
    return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content=payload)

def install_exception_handlers(app):
    from fastapi import HTTPException
    app.add_exception_handler(APIError, http_exception_handler)
    app.add_exception_handler(Exception, unhandled_exception_handler)

# ===== app/core/pagination.py =====
from __future__ import annotations
import base64, json
from typing import Any, Dict, Optional, Tuple

def encode_cursor(payload: Dict[str, Any]) -> str:
    return base64.urlsafe_b64encode(json.dumps(payload, separators=(",", ":")).encode()).decode()

def decode_cursor(cursor: Optional[str]) -> Optional[Dict[str, Any]]:
    if not cursor:
        return None
    try:
        data = base64.urlsafe_b64decode(cursor.encode()).decode()
        return json.loads(data)
    except Exception:
        return None

def page(items, next_cursor_payload: Optional[Dict[str, Any]]):
    return {"items": items, "next_cursor": encode_cursor(next_cursor_payload) if next_cursor_payload else None}

# ===== app/core/s3.py =====
from __future__ import annotations
from typing import BinaryIO
from urllib.parse import urlparse
from minio import Minio
from .config import settings

_client: Minio | None = None

def _mk_client() -> Minio:
    endpoint = settings.S3_ENDPOINT
    secure = False
    netloc = endpoint
    if endpoint.startswith("http://") or endpoint.startswith("https://"):
        u = urlparse(endpoint)
        secure = (u.scheme == "https")
        netloc = u.netloc
    return Minio(
        netloc,
        access_key=settings.S3_ACCESS_KEY,
        secret_key=settings.S3_SECRET_KEY,
        secure=secure,
    )

def get_minio() -> Minio:
    global _client
    if _client is None:
        _client = _mk_client()
    return _client

def reset_client():
    global _client
    _client = None

def presign_put(bucket: str, key: str, expiry_seconds: int = 3600) -> str:
    from datetime import timedelta
    url = get_minio().presigned_put_object(bucket, key, expires=timedelta(seconds=expiry_seconds))
    
    # Replace internal endpoint with public endpoint for external access
    if settings.S3_PUBLIC_ENDPOINT != settings.S3_ENDPOINT:
        internal_base = settings.S3_ENDPOINT.rstrip('/')
        public_base = settings.S3_PUBLIC_ENDPOINT.rstrip('/')
        url = url.replace(internal_base, public_base)
    
    return url

def ensure_bucket(bucket: str) -> None:
    c = get_minio()
    if not c.bucket_exists(bucket):
        c.make_bucket(bucket)

def put_object(bucket: str, key: str, data: bytes | BinaryIO, length: int | None = None, content_type: str | None = None):
    c = get_minio()
    if hasattr(data, "read"):
        return c.put_object(bucket, key, data, length=length or -1, content_type=content_type)  # type: ignore
    import io
    bio = io.BytesIO(data if isinstance(data, (bytes, bytearray)) else bytes(data))
    return c.put_object(bucket, key, bio, length=len(bio.getvalue()), content_type=content_type)

def get_object(bucket: str, key: str):
    return get_minio().get_object(bucket, key)

def stat_object(bucket: str, key: str):
    return get_minio().stat_object(bucket, key)

def list_buckets():
    return get_minio().list_buckets()

# ===== app/core/model_registry.py =====
"""
Model Registry - простой реестр моделей в Redis
Управляет конфигурацией моделей эмбеддингов и LLM
"""
from __future__ import annotations
import json
import os
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from app.core.redis import get_redis

@dataclass
class ModelConfig:
    """Конфигурация модели"""
    id: str                    # HF ID (например, "sentence-transformers/all-MiniLM-L6-v2")
    alias: str                 # Короткое имя (например, "minilm")
    rev: str                   # Ревизия/хеш
    dim: int                   # Размерность векторов
    max_seq: int               # Максимальная длина последовательности
    storage_uri: str           # Путь в MinIO (например, "s3://models/sentence-transformers/all-MiniLM-L6-v2/abc123")
    queues: Dict[str, str]     # {"rt": "embed.minilm.rt", "bulk": "embed.minilm.bulk"}
    weights: float = 1.0       # Вес для фьюзинга
    enabled: bool = True       # Включена ли модель
    health: str = "down"       # "ready" | "degraded" | "down"
    pooling: str = "mean"      # "mean" | "cls"
    tokenizer_rev: str = ""    # Ревизия токенайзера
    checksum: Dict[str, str] = None  # {"model.safetensors": "sha256:..."}
    
    def __post_init__(self):
        if self.checksum is None:
            self.checksum = {}

class ModelRegistry:
    """Реестр моделей в Redis"""
    
    def __init__(self):
        self.redis = get_redis()
        self._default_models = self._load_default_models()
    
    def _load_default_models(self) -> Dict[str, ModelConfig]:
        """Загружает дефолтные модели из переменных окружения"""
        models = {}
        
        # Читаем конфигурацию из переменных окружения
        # Формат: EMB_MODELS="minilm:sentence-transformers/all-MiniLM-L6-v2:abc123:384:256,minilm2:sentence-transformers/all-MiniLM-L6-v2:def456:384:256"
        emb_models = os.getenv("EMB_MODELS", "")
        if emb_models:
            for model_str in emb_models.split(","):
                if not model_str.strip():
                    continue
                parts = model_str.strip().split(":")
                if len(parts) >= 5:
                    alias, model_id, rev, dim, max_seq = parts[:5]
                    models[alias] = ModelConfig(
                        id=model_id,
                        alias=alias,
                        rev=rev,
                        dim=int(dim),
                        max_seq=int(max_seq),
                        storage_uri=f"s3://models/{model_id}/{rev}",
                        queues={
                            "rt": f"embed.{alias}.rt",
                            "bulk": f"embed.{alias}.bulk"
                        }
                    )
        
        # Если нет переменных, создаем дефолтную модель для тестирования
        if not models:
            models["minilm"] = ModelConfig(
                id="sentence-transformers/all-MiniLM-L6-v2",
                alias="minilm",
                rev="default",
                dim=384,
                max_seq=256,
                storage_uri="s3://models/sentence-transformers/all-MiniLM-L6-v2/default",
                queues={
                    "rt": "embed.minilm.rt",
                    "bulk": "embed.minilm.bulk"
                }
            )
        
        return models
    
    def register_model(self, config: ModelConfig) -> bool:
        """Регистрирует модель в реестре"""
        try:
            key = f"model:{config.alias}"
            self.redis.hset(key, mapping={
                "config": json.dumps(asdict(config), ensure_ascii=False)
            })
            return True
        except Exception as e:
            print(f"Failed to register model {config.alias}: {e}")
            return False
    
    def get_model(self, alias: str) -> Optional[ModelConfig]:
        """Получает конфигурацию модели по алиасу"""
        try:
            key = f"model:{alias}"
            data = self.redis.hget(key, "config")
            if not data:
                return None
            config_dict = json.loads(data)
            return ModelConfig(**config_dict)
        except Exception as e:
            print(f"Failed to get model {alias}: {e}")
            return None
    
    def list_models(self, enabled_only: bool = True) -> List[ModelConfig]:
        """Список всех моделей"""
        models = []
        try:
            # Сначала пробуем загрузить из Redis
            keys = self.redis.keys("model:*")
            for key in keys:
                data = self.redis.hget(key, "config")
                if data:
                    config_dict = json.loads(data)
                    config = ModelConfig(**config_dict)
                    if not enabled_only or config.enabled:
                        models.append(config)
            
            # Если в Redis ничего нет, используем дефолтные
            if not models:
                for config in self._default_models.values():
                    if not enabled_only or config.enabled:
                        models.append(config)
                        # Регистрируем дефолтные модели в Redis
                        self.register_model(config)
        
        except Exception as e:
            print(f"Failed to list models: {e}")
            # Fallback на дефолтные модели
            for config in self._default_models.values():
                if not enabled_only or config.enabled:
                    models.append(config)
        
        return models
    
    def get_default_models(self, profile: str = "rt") -> List[str]:
        """Получает список дефолтных моделей для профиля"""
        if profile == "rt":
            return os.getenv("EMB_DEFAULT_RT_MODELS", "minilm").split(",")
        else:
            return os.getenv("EMB_DEFAULT_BULK_MODELS", "minilm").split(",")
    
    def update_health(self, alias: str, health: str) -> bool:
        """Обновляет статус здоровья модели"""
        try:
            config = self.get_model(alias)
            if config:
                config.health = health
                return self.register_model(config)
            return False
        except Exception as e:
            print(f"Failed to update health for {alias}: {e}")
            return False
    
    def get_ready_models(self, profile: str = "rt") -> List[ModelConfig]:
        """Получает список готовых моделей для профиля"""
        models = self.list_models(enabled_only=True)
        ready_models = []
        
        for model in models:
            if model.health == "ready" and profile in model.queues:
                ready_models.append(model)
        
        return ready_models

# Глобальный экземпляр реестра
_registry = None

def get_model_registry() -> ModelRegistry:
    """Получает глобальный экземпляр реестра моделей"""
    global _registry
    if _registry is None:
        _registry = ModelRegistry()
    return _registry

# ===== app/core/s3_helpers.py =====
# app/core/s3_helpers.py
from __future__ import annotations
from datetime import timedelta
from typing import Optional
from urllib.parse import quote

from app.core.config import settings
from app.core.s3 import get_minio  # ожидается, что он уже есть в вашем проекте

def put_object(bucket: str, key: str, data, *, content_type: Optional[str] = None, part_size: int = 10 * 1024 * 1024):
    """
    Безопасная загрузка потока в MinIO.
    Использует multipart (length=-1), если длина неизвестна (UploadFile.file).
    """
    client = get_minio()
    return client.put_object(bucket, key, data, length=-1, part_size=part_size, content_type=content_type)

def _content_disposition(filename: str) -> str:
    # RFC 5987 filename*
    return f"attachment; filename*=UTF-8''{quote(filename)}"

def presign_get(bucket: str, key: str, *, seconds: int = 3600, download_name: Optional[str] = None, mime: Optional[str] = None) -> str:
    """
    Генерирует presigned GET URL. Добавляет правильный Content-Disposition и MIME.
    Если настроен публичный endpoint, подменяет на него URL.
    """
    client = get_minio()
    headers = {}
    if download_name:
        headers["response-content-disposition"] = _content_disposition(download_name)
    if mime:
        headers["response-content-type"] = mime
    url = client.presigned_get_object(bucket, key, expires=timedelta(seconds=seconds), response_headers=headers)
    if getattr(settings, "S3_PUBLIC_ENDPOINT", None) and settings.S3_PUBLIC_ENDPOINT != settings.S3_ENDPOINT:
        url = url.replace(settings.S3_ENDPOINT.rstrip("/"), settings.S3_PUBLIC_ENDPOINT.rstrip("/"))
    return url

# ===== app/repositories/chats_repo.py =====
from __future__ import annotations
from typing import List, Optional, Tuple
from sqlalchemy.orm import Session
from sqlalchemy import select, desc, asc, or_
from app.models.chat import Chats, ChatMessages

class ChatsRepo:
    def __init__(self, session: Session):
        self.s = session

    # Chats
    def create_chat(self, owner_id, name: str | None, tags: List[str] | None = None) -> Chats:
        chat = Chats(owner_id=owner_id, name=name, tags=tags or [])
        self.s.add(chat)
        self.s.flush()
        return chat

    def list_chats(self, owner_id, q: Optional[str] = None, limit: int = 100) -> List[Chats]:
        stmt = select(Chats).where(Chats.owner_id == owner_id).order_by(desc(Chats.last_message_at), desc(Chats.created_at)).limit(limit)
        if q:
            pattern = f"%{q.lower()}%"
            stmt = stmt.where(or_(Chats.name.ilike(pattern),))
        return list(self.s.scalars(stmt))

    def get(self, chat_id) -> Optional[Chats]:
        try:
            # Convert string to UUID if needed
            if isinstance(chat_id, str):
                import uuid
                chat_id = uuid.UUID(chat_id)
            return self.s.get(Chats, chat_id)
        except (ValueError, TypeError):
            return None

    def delete(self, chat_id):
        chat = self.get(chat_id)
        if chat:
            self.s.delete(chat)
            self.s.flush()

    def rename_chat(self, chat_id, name: str):
        chat = self.get(chat_id)
        if chat:
            chat.name = name
            self.s.flush()

    def update_chat_tags(self, chat_id, tags: List[str]):
        chat = self.get(chat_id)
        if chat:
            chat.tags = tags
            self.s.flush()

    # Messages
    def add_message(self, chat_id, role: str, content: dict | str, model: str | None = None) -> ChatMessages:
        if isinstance(content, str):
            payload = {"text": content}
        else:
            payload = content
        msg = ChatMessages(chat_id=chat_id, role=role, content=payload, model=model)
        self.s.add(msg)
        self.s.flush()
        return msg

    def list_messages(self, chat_id, cursor: Optional[str] = None, limit: int = 50) -> Tuple[List[ChatMessages], Optional[str]]:
        stmt = select(ChatMessages).where(ChatMessages.chat_id == chat_id).order_by(asc(ChatMessages.created_at), asc(ChatMessages.id)).limit(limit)
        if cursor:
            # cursor = created_at ISO or message id string; we simply skip <= cursor id
            try:
                stmt = stmt.where(ChatMessages.id > cursor)  # naive id cursor
            except Exception:
                pass
        rows = list(self.s.scalars(stmt))
        next_cursor = rows[-1].id.hex if rows else None
        return rows, next_cursor

# ===== app/repositories/rag_repo.py =====
from __future__ import annotations
from typing import List, Optional
from sqlalchemy.orm import Session
from sqlalchemy import select
from app.models.rag import RagDocuments, RagChunks

class RagRepo:
    def __init__(self, session: Session):
        self.s = session

    def create_document(self, **kwargs) -> RagDocuments:
        doc = RagDocuments(**kwargs)
        self.s.add(doc)
        self.s.flush()
        return doc

    def get(self, doc_id) -> Optional[RagDocuments]:
        return self.s.get(RagDocuments, doc_id)

    def list(self, limit: int = 50) -> List[RagDocuments]:
        return self.s.execute(select(RagDocuments).order_by(RagDocuments.date_upload.desc()).limit(limit)).scalars().all()

    def delete(self, doc: RagDocuments):
        self.s.delete(doc)

    def add_chunk(self, **kwargs) -> RagChunks:
        chunk = RagChunks(**kwargs)
        self.s.add(chunk)
        self.s.flush()
        return chunk

    def list_chunks(self, doc_id) -> List[RagChunks]:
        return self.s.execute(select(RagChunks).where(RagChunks.document_id == doc_id).order_by(RagChunks.chunk_idx.asc())).scalars().all()

# ===== app/repositories/__init__.py =====

# ===== app/repositories/analyze_repo.py =====
from __future__ import annotations
from typing import List, Optional
from sqlalchemy.orm import Session
from sqlalchemy import select
from app.models.analyze import AnalysisDocuments, AnalysisChunks

class AnalyzeRepo:
    def __init__(self, session: Session):
        self.s = session

    def create_document(self, **kwargs) -> AnalysisDocuments:
        doc = AnalysisDocuments(**kwargs)
        self.s.add(doc)
        self.s.flush()
        return doc

    def get(self, doc_id) -> Optional[AnalysisDocuments]:
        return self.s.get(AnalysisDocuments, doc_id)

    def list(self, limit: int = 50) -> List[AnalysisDocuments]:
        return self.s.execute(select(AnalysisDocuments).order_by(AnalysisDocuments.date_upload.desc()).limit(limit)).scalars().all()

    def delete(self, doc: AnalysisDocuments):
        self.s.delete(doc)

    def add_chunk(self, **kwargs) -> AnalysisChunks:
        chunk = AnalysisChunks(**kwargs)
        self.s.add(chunk)
        self.s.flush()
        return chunk

    def list_chunks(self, doc_id) -> List[AnalysisChunks]:
        return self.s.execute(select(AnalysisChunks).where(AnalysisChunks.document_id == doc_id).order_by(AnalysisChunks.chunk_idx.asc())).scalars().all()

# ===== app/repositories/users_repo.py =====
from __future__ import annotations
from typing import Optional, List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import select, func, and_, or_, desc
from datetime import datetime
import uuid

from app.models.user import Users, UserTokens, UserRefreshTokens, PasswordResetTokens, AuditLogs

class UsersRepo:
    def __init__(self, session: Session):
        self.s = session

    def by_login(self, login: str) -> Optional[Users]:
        return self.s.execute(select(Users).where(Users.login == login)).scalars().first()

    def get(self, user_id):
        return self.s.get(Users, user_id)

    # Refresh tokens
    def add_refresh(self, rec: UserRefreshTokens) -> None:
        self.s.add(rec)

    def get_refresh_by_hash(self, refresh_hash: str) -> Optional[UserRefreshTokens]:
        return self.s.execute(select(UserRefreshTokens).where(UserRefreshTokens.refresh_hash == refresh_hash)).scalars().first()

    def revoke_refresh(self, refresh_hash: str) -> bool:
        rec = self.get_refresh_by_hash(refresh_hash)
        if rec and not rec.revoked:
            rec.revoked = True
            return True
        return False

    # User management methods
    def create_user(self, login: str, password_hash: str, role: str = "reader", 
                   email: Optional[str] = None, is_active: bool = True) -> Users:
        """Create a new user."""
        user = Users(
            login=login,
            password_hash=password_hash,
            role=role,
            email=email,
            is_active=is_active
        )
        self.s.add(user)
        self.s.commit()
        self.s.refresh(user)
        return user

    def update_user(self, user_id: str, **updates) -> Optional[Users]:
        """Update user fields."""
        user = self.get(user_id)
        if not user:
            return None
        
        for key, value in updates.items():
            if hasattr(user, key):
                setattr(user, key, value)
        
        user.updated_at = datetime.utcnow()
        self.s.commit()
        self.s.refresh(user)
        return user

    def list_users(self, query: Optional[str] = None, role: Optional[str] = None, 
                  is_active: Optional[bool] = None, limit: int = 50, 
                  cursor: Optional[str] = None) -> tuple[List[Users], bool, Optional[str]]:
        """List users with pagination and filters."""
        stmt = select(Users)
        
        # Apply filters
        if query:
            stmt = stmt.where(
                or_(
                    Users.login.ilike(f"%{query}%"),
                    Users.email.ilike(f"%{query}%")
                )
            )
        
        if role:
            stmt = stmt.where(Users.role == role)
        
        if is_active is not None:
            stmt = stmt.where(Users.is_active == is_active)
        
        # Apply cursor-based pagination
        if cursor:
            try:
                cursor_time = datetime.fromisoformat(cursor)
                stmt = stmt.where(Users.created_at < cursor_time)
            except ValueError:
                pass  # Invalid cursor, ignore
        
        # Order and limit
        stmt = stmt.order_by(desc(Users.created_at)).limit(limit + 1)
        
        users = self.s.execute(stmt).scalars().all()
        has_more = len(users) > limit
        
        if has_more:
            users = users[:-1]
            next_cursor = users[-1].created_at.isoformat() if users else None
        else:
            next_cursor = None
        
        return users, has_more, next_cursor

    def count_users(self, query: Optional[str] = None, role: Optional[str] = None, 
                   is_active: Optional[bool] = None) -> int:
        """Count users matching filters."""
        stmt = select(func.count(Users.id))
        
        if query:
            stmt = stmt.where(
                or_(
                    Users.login.ilike(f"%{query}%"),
                    Users.email.ilike(f"%{query}%")
                )
            )
        
        if role:
            stmt = stmt.where(Users.role == role)
        
        if is_active is not None:
            stmt = stmt.where(Users.is_active == is_active)
        
        return self.s.execute(stmt).scalar() or 0

    # PAT Token methods
    def create_token(self, user_id: str, token_hash: str, name: str,
                    scopes: Optional[List[str]] = None, expires_at: Optional[datetime] = None) -> UserTokens:
        """Create a new PAT token."""
        token = UserTokens(
            user_id=user_id,
            token_hash=token_hash,
            name=name,
            scopes=scopes,
            expires_at=expires_at
        )
        self.s.add(token)
        self.s.commit()
        self.s.refresh(token)
        return token

    def get_user_tokens(self, user_id: str, include_revoked: bool = False) -> List[UserTokens]:
        """Get user's PAT tokens."""
        stmt = select(UserTokens).where(UserTokens.user_id == user_id)
        
        if not include_revoked:
            stmt = stmt.where(UserTokens.revoked_at.is_(None))
        
        return self.s.execute(stmt.order_by(desc(UserTokens.created_at))).scalars().all()

    def revoke_token(self, token_id: str) -> bool:
        """Revoke a PAT token."""
        token = self.s.get(UserTokens, token_id)
        if token and not token.revoked_at:
            token.revoked_at = datetime.utcnow()
            self.s.commit()
            return True
        return False

    def get_token_by_hash(self, token_hash: str) -> Optional[UserTokens]:
        """Get token by hash."""
        return self.s.execute(
            select(UserTokens).where(UserTokens.token_hash == token_hash)
        ).scalars().first()

    # Password reset methods
    def create_password_reset_token(self, user_id: str, token_hash: str, 
                                   expires_at: datetime) -> PasswordResetTokens:
        """Create a password reset token."""
        token = PasswordResetTokens(
            user_id=user_id,
            token_hash=token_hash,
            expires_at=expires_at
        )
        self.s.add(token)
        self.s.commit()
        self.s.refresh(token)
        return token

    def get_password_reset_token(self, token_hash: str) -> Optional[PasswordResetTokens]:
        """Get password reset token by hash."""
        return self.s.execute(
            select(PasswordResetTokens)
            .where(
                and_(
                    PasswordResetTokens.token_hash == token_hash,
                    PasswordResetTokens.used_at.is_(None),
                    PasswordResetTokens.expires_at > datetime.utcnow()
                )
            )
        ).scalars().first()

    def use_password_reset_token(self, token_hash: str) -> bool:
        """Mark password reset token as used."""
        token = self.get_password_reset_token(token_hash)
        if token:
            token.used_at = datetime.utcnow()
            self.s.commit()
            return True
        return False

    # Audit log methods
    def get_audit_logs(self, actor_user_id: Optional[str] = None, action: Optional[str] = None,
                      object_type: Optional[str] = None, limit: int = 50,
                      cursor: Optional[str] = None) -> tuple[List[AuditLogs], bool, Optional[str]]:
        """Get audit logs with pagination."""
        stmt = select(AuditLogs)
        
        if actor_user_id:
            stmt = stmt.where(AuditLogs.actor_user_id == actor_user_id)
        
        if action:
            stmt = stmt.where(AuditLogs.action == action)
        
        if object_type:
            stmt = stmt.where(AuditLogs.object_type == object_type)
        
        if cursor:
            try:
                cursor_time = datetime.fromisoformat(cursor)
                stmt = stmt.where(AuditLogs.ts < cursor_time)
            except ValueError:
                pass
        
        stmt = stmt.order_by(desc(AuditLogs.ts)).limit(limit + 1)
        
        logs = self.s.execute(stmt).scalars().all()
        has_more = len(logs) > limit
        
        if has_more:
            logs = logs[:-1]
            next_cursor = logs[-1].ts.isoformat() if logs else None
        else:
            next_cursor = None
        
        return logs, has_more, next_cursor

    def count_audit_logs(self, actor_user_id: Optional[str] = None, action: Optional[str] = None,
                        object_type: Optional[str] = None) -> int:
        """Count audit logs matching filters."""
        stmt = select(func.count(AuditLogs.id))
        
        if actor_user_id:
            stmt = stmt.where(AuditLogs.actor_user_id == actor_user_id)
        
        if action:
            stmt = stmt.where(AuditLogs.action == action)
        
        if object_type:
            stmt = stmt.where(AuditLogs.object_type == object_type)
        
        return self.s.execute(stmt).scalar() or 0

# ===== app/models/user.py =====
# app/models/user.py
from __future__ import annotations
from typing import Optional, List, Dict, Any
from datetime import datetime
import uuid

from sqlalchemy import String, Boolean, Text, DateTime, UniqueConstraint, ForeignKey, Index, CheckConstraint
from sqlalchemy.dialects.postgresql import UUID, JSON
from sqlalchemy.orm import Mapped, mapped_column, relationship

from .base import Base

class Users(Base):
    __tablename__ = "users"

    __table_args__ = (
        CheckConstraint("role IN ('admin', 'editor', 'reader')", name="ck_users_role"),
    )

    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    login: Mapped[str] = mapped_column(String(255), nullable=False, unique=True, index=True)
    password_hash: Mapped[str] = mapped_column(Text, nullable=False)
    role: Mapped[str] = mapped_column(String(20), nullable=False, default="reader")
    is_active: Mapped[bool] = mapped_column(Boolean, nullable=False, default=True)
    email: Mapped[Optional[str]] = mapped_column(String(255), nullable=True, index=True)
    require_password_change: Mapped[bool] = mapped_column(Boolean, nullable=False, default=False)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default="now()")
    updated_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default="now()")

    # Relationships
    refresh_tokens: Mapped[List["UserRefreshTokens"]] = relationship(
        back_populates="user",
        cascade="all, delete-orphan",
        passive_deletes=True,
    )
    pat_tokens: Mapped[List["UserTokens"]] = relationship(
        back_populates="user",
        cascade="all, delete-orphan",
        passive_deletes=True,
    )
    password_reset_tokens: Mapped[List["PasswordResetTokens"]] = relationship(
        back_populates="user",
        cascade="all, delete-orphan",
        passive_deletes=True,
    )
    audit_logs: Mapped[List["AuditLogs"]] = relationship(
        back_populates="actor_user",
        cascade="all, delete-orphan",
        passive_deletes=True,
    )

class UserTokens(Base):
    __tablename__ = "user_tokens"

    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    token_hash: Mapped[str] = mapped_column(Text, nullable=False)
    name: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    scopes: Mapped[Optional[List[str]]] = mapped_column(JSON, nullable=True)
    expires_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default="now()")
    last_used_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    revoked_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)

    user: Mapped["Users"] = relationship(back_populates="pat_tokens")

class UserRefreshTokens(Base):
    __tablename__ = "user_refresh_tokens"

    __table_args__ = (
        UniqueConstraint("refresh_hash", name="uq_user_refresh_tokens_refresh_hash"),
    )

    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    refresh_hash: Mapped[str] = mapped_column(Text, nullable=False)
    issued_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default="now()")
    expires_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False)
    rotating: Mapped[bool] = mapped_column(Boolean, nullable=False, default=True)
    revoked: Mapped[bool] = mapped_column(Boolean, nullable=False, default=False)
    meta: Mapped[Optional[str]] = mapped_column(Text, nullable=True)  # JSON encoded as text

    user: Mapped["Users"] = relationship(back_populates="refresh_tokens")


class PasswordResetTokens(Base):
    __tablename__ = "password_reset_tokens"

    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    token_hash: Mapped[str] = mapped_column(Text, nullable=False, unique=True)
    expires_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False)
    used_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default="now()")

    user: Mapped["Users"] = relationship(back_populates="password_reset_tokens")


class AuditLogs(Base):
    __tablename__ = "audit_logs"

    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    ts: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default="now()", index=True)
    actor_user_id: Mapped[Optional[uuid.UUID]] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("users.id", ondelete="SET NULL"),
        nullable=True,
        index=True,
    )
    action: Mapped[str] = mapped_column(String(50), nullable=False, index=True)
    object_type: Mapped[Optional[str]] = mapped_column(String(50), nullable=True, index=True)
    object_id: Mapped[Optional[str]] = mapped_column(String(255), nullable=True, index=True)
    meta: Mapped[Optional[Dict[str, Any]]] = mapped_column(JSON, nullable=True)
    ip: Mapped[Optional[str]] = mapped_column(String(45), nullable=True)  # IPv6 support
    user_agent: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    request_id: Mapped[Optional[str]] = mapped_column(String(255), nullable=True, index=True)

    actor_user: Mapped[Optional["Users"]] = relationship(back_populates="audit_logs")

# ===== app/models/__init__.py =====
from .base import Base
from .user import Users, UserTokens, UserRefreshTokens
from .chat import Chats, ChatMessages
from .rag import RagDocuments, RagChunks
from .analyze import AnalysisDocuments, AnalysisChunks

# ===== app/models/chat.py =====
from __future__ import annotations
from typing import Optional, List
from datetime import datetime
from sqlalchemy import String, Text, DateTime, Enum, Integer, ForeignKey, JSON, ARRAY
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import Mapped, mapped_column, relationship
from .base import Base
import uuid

ChatRoleEnum = Enum("system", "user", "assistant", "tool", name="chat_role_enum", create_constraint=True)

class Chats(Base):
    __tablename__ = "chats"
    
    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    owner_id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), ForeignKey("users.id", ondelete="CASCADE"), nullable=False)
    tags: Mapped[Optional[List[str]]] = mapped_column(ARRAY(String), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default="now()", nullable=False)
    updated_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default="now()", nullable=False)
    last_message_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)

    messages: Mapped[List["ChatMessages"]] = relationship(back_populates="chat", cascade="all, delete-orphan")

class ChatMessages(Base):
    __tablename__ = "chatmessages"
    
    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    chat_id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), ForeignKey("chats.id", ondelete="CASCADE"), nullable=False)
    role: Mapped[str] = mapped_column(ChatRoleEnum, nullable=False)
    content: Mapped[dict] = mapped_column(JSON, nullable=False)
    model: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    tokens_in: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    tokens_out: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default="now()", nullable=False)
    meta: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)

    chat: Mapped["Chats"] = relationship(back_populates="messages")

# ===== app/models/rag.py =====
from __future__ import annotations
from typing import Optional, List
from datetime import datetime
from sqlalchemy import String, Text, DateTime, Enum, Integer, BigInteger, ARRAY, JSON, ForeignKey
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import Mapped, mapped_column, relationship
from .base import Base
import uuid

RagStatusEnum = Enum(
    "uploaded", "normalizing", "chunking", "embedding", "indexing", "ready", "archived", "deleting", "error",
    name="rag_status_enum",
    create_constraint=True
)

class RagDocuments(Base):
    __tablename__ = "ragdocuments"
    
    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    status: Mapped[str] = mapped_column(RagStatusEnum, nullable=False, default="uploaded")
    date_upload: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default="now()", nullable=False)
    uploaded_by: Mapped[Optional[uuid.UUID]] = mapped_column(UUID(as_uuid=True), ForeignKey("users.id", ondelete="SET NULL"), nullable=True)
    url_file: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    url_canonical_file: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    source_mime: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    size_bytes: Mapped[Optional[int]] = mapped_column(BigInteger, nullable=True)
    tags: Mapped[Optional[List[str]]] = mapped_column(ARRAY(String), nullable=True)
    error: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    updated_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)

    chunks: Mapped[List["RagChunks"]] = relationship(back_populates="document", cascade="all, delete-orphan")

class RagChunks(Base):
    __tablename__ = "ragchunks"
    
    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), ForeignKey("ragdocuments.id", ondelete="CASCADE"), nullable=False)
    chunk_idx: Mapped[int] = mapped_column(Integer, nullable=False)
    text: Mapped[str] = mapped_column(Text, nullable=False)
    embedding_model: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    embedding_version: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    date_embedding: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    meta: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)
    qdrant_point_id: Mapped[Optional[uuid.UUID]] = mapped_column(UUID(as_uuid=True), nullable=True)

    document: Mapped["RagDocuments"] = relationship(back_populates="chunks")

# ===== app/models/base.py =====
from __future__ import annotations
from sqlalchemy.orm import DeclarativeBase, declared_attr, Mapped, mapped_column
from sqlalchemy import MetaData, func
from datetime import datetime

# Naming convention for Alembic-friendly constraints
metadata = MetaData(
    naming_convention={
        "ix": "ix_%(column_0_label)s",
        "uq": "uq_%(table_name)s_%(column_0_name)s",
        "ck": "ck_%(table_name)s_%(constraint_name)s",
        "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
        "pk": "pk_%(table_name)s",
    }
)

class Base(DeclarativeBase):
    metadata = metadata

    @declared_attr.directive
    def __tablename__(cls) -> str:  # type: ignore[override]
        return cls.__name__.lower()

Timestamp = datetime

# ===== app/models/analyze.py =====
from __future__ import annotations
from typing import Optional, List
from datetime import datetime
from sqlalchemy import String, Text, DateTime, Enum, Integer, JSON, ForeignKey
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import Mapped, mapped_column, relationship
from .base import Base
import uuid

AnalyzeStatusEnum = Enum("queued", "processing", "done", "error", "canceled", name="analyze_status_enum", create_constraint=True)

class AnalysisDocuments(Base):
    __tablename__ = "analysisdocuments"
    
    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    status: Mapped[str] = mapped_column(AnalyzeStatusEnum, nullable=False, default="queued")
    date_upload: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default="now()", nullable=False)
    uploaded_by: Mapped[Optional[uuid.UUID]] = mapped_column(UUID(as_uuid=True), ForeignKey("users.id", ondelete="SET NULL"), nullable=True)
    url_file: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    url_canonical_file: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    result: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)
    error: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    updated_at: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)

    chunks: Mapped[List["AnalysisChunks"]] = relationship(back_populates="document", cascade="all, delete-orphan")

class AnalysisChunks(Base):
    __tablename__ = "analysischunks"
    
    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), ForeignKey("analysisdocuments.id", ondelete="CASCADE"), nullable=False)
    chunk_idx: Mapped[int] = mapped_column(Integer, nullable=False)
    text: Mapped[str] = mapped_column(Text, nullable=False)
    embedding_model: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    embedding_version: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    date_embedding: Mapped[Optional[datetime]] = mapped_column(DateTime(timezone=True), nullable=True)
    meta: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)

    document: Mapped["AnalysisDocuments"] = relationship(back_populates="chunks")

# ===== app/schemas/auth.py =====
from __future__ import annotations
from typing import Optional, List, Dict, Any, Literal
from pydantic import BaseModel, Field
from datetime import datetime, date

class RefreshResponse(BaseModel):
    access_token: Optional[str] = Field(None)
    refresh_token: Optional[str] = Field(None)
    token_type: Optional[str] = Field(None)
    expires_in: Optional[int] = Field(None)

class LoginRequest(BaseModel):
    login: str
    password: str

class RefreshRequest(BaseModel):
    refresh_token: Optional[str] = Field(None)

class LoginResponse(BaseModel):
    access_token: Optional[str] = Field(None)
    refresh_token: Optional[str] = Field(None)
    token_type: Optional[str] = Field(None)
    expires_in: Optional[int] = Field(None)
    user: Optional[Dict[str, Any]] = Field(None)

# ===== app/schemas/__init__.py =====
from .common import ErrorResponse
from .auth import LoginRequest, LoginResponse, RefreshRequest, RefreshResponse
from .chats import ChatMessage, ChatTurnRequest, ChatTurnResponse
from .rag import RagDocument, RagSearchRequest, RagUploadRequest
from .analyze import AnalyzeRequest, AnalyzeResult

__all__ = ['ErrorResponse', 'RefreshResponse', 'LoginRequest', 'RefreshRequest', 'LoginResponse', 'ChatMessage', 'ChatTurnRequest', 'ChatTurnResponse', 'RagSearchRequest', 'RagUploadRequest', 'RagDocument', 'AnalyzeRequest', 'AnalyzeResult']

# ===== app/schemas/chat_schemas.py =====
from __future__ import annotations
from typing import Optional, List
from pydantic import BaseModel, Field, validator
from datetime import datetime

class ChatCreateRequest(BaseModel):
    name: Optional[str] = Field(None, max_length=255, description="Chat name")
    tags: Optional[List[str]] = Field(default_factory=list, description="Chat tags")

    @validator('tags')
    def validate_tags(cls, v):
        if v is None:
            return []
        if len(v) > 20:
            raise ValueError('Too many tags (max 20)')
        for tag in v:
            if not isinstance(tag, str):
                raise ValueError('All tags must be strings')
            if len(tag) > 50:
                raise ValueError('Tag too long (max 50 characters)')
            if not tag.strip():
                raise ValueError('Empty tags not allowed')
        return [tag.strip() for tag in v if tag.strip()]

class ChatUpdateRequest(BaseModel):
    name: Optional[str] = Field(None, max_length=255, description="New chat name")

class ChatTagsUpdateRequest(BaseModel):
    tags: List[str] = Field(..., description="Chat tags")

    @validator('tags')
    def validate_tags(cls, v):
        if len(v) > 20:
            raise ValueError('Too many tags (max 20)')
        return [tag.strip() for tag in v if tag.strip()]

class ChatMessageRequest(BaseModel):
    content: str
    use_rag: Optional[bool] = False
    response_stream: Optional[bool] = False

class ChatMessageResponse(BaseModel):
    message_id: str
    content: str
    answer: str

class ChatOut(BaseModel):
    id: str
    name: Optional[str]
    tags: Optional[List[str]]
    created_at: Optional[datetime]
    updated_at: Optional[datetime]
    last_message_at: Optional[datetime]

    class Config:
        from_attributes = True

class ChatMessageOut(BaseModel):
    id: str
    chat_id: str
    role: str
    content: str
    created_at: datetime

    class Config:
        from_attributes = True

# ===== app/schemas/admin.py =====
from __future__ import annotations
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field, validator
from datetime import datetime
from enum import Enum


class UserRole(str, Enum):
    ADMIN = "admin"
    EDITOR = "editor"
    READER = "reader"


class UserStatus(str, Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"


# User schemas
class UserBase(BaseModel):
    login: str = Field(..., min_length=3, max_length=255)
    role: UserRole = Field(default=UserRole.READER)
    email: Optional[str] = Field(None, max_length=255)
    is_active: bool = Field(default=True)


class UserCreate(UserBase):
    password: Optional[str] = Field(None, min_length=12)
    
    @validator('password')
    def validate_password(cls, v):
        if v is not None:
            # Basic password policy validation
            if len(v) < 12:
                raise ValueError('Password must be at least 12 characters long')
            # Add more complex validation if needed
        return v


class UserUpdate(BaseModel):
    role: Optional[UserRole] = None
    email: Optional[str] = Field(None, max_length=255)
    is_active: Optional[bool] = None
    require_password_change: Optional[bool] = None


class UserResponse(UserBase):
    id: str
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True


class UserListResponse(BaseModel):
    users: List[UserResponse]
    total: int
    has_more: bool
    next_cursor: Optional[str] = None


# Password reset schemas
class PasswordResetRequest(BaseModel):
    login_or_email: str = Field(..., min_length=1)


class PasswordResetConfirm(BaseModel):
    token: str = Field(..., min_length=1)
    new_password: str = Field(..., min_length=12)


class PasswordChange(BaseModel):
    new_password: Optional[str] = Field(None, min_length=12)
    
    @validator('new_password')
    def validate_password(cls, v):
        if v is not None and len(v) < 12:
            raise ValueError('Password must be at least 12 characters long')
        return v


# PAT Token schemas
class TokenScope(str, Enum):
    API_READ = "api:read"
    API_WRITE = "api:write"
    RAG_ALL = "rag:*"
    RAG_READ = "rag:read"
    RAG_WRITE = "rag:write"
    CHAT_ALL = "chat:*"
    CHAT_READ = "chat:read"
    CHAT_WRITE = "chat:write"


class TokenCreate(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    scopes: Optional[List[TokenScope]] = Field(default=None)
    expires_at: Optional[datetime] = None


class TokenResponse(BaseModel):
    id: str
    name: str
    scopes: Optional[List[str]] = None
    expires_at: Optional[datetime] = None
    created_at: datetime
    last_used_at: Optional[datetime] = None
    token_plain_once: Optional[str] = None  # Only returned on creation
    
    class Config:
        from_attributes = True


class TokenListResponse(BaseModel):
    tokens: List[TokenResponse]
    total: int


# Audit log schemas
class AuditAction(str, Enum):
    USER_CREATED = "user_created"
    USER_UPDATED = "user_updated"
    USER_DELETED = "user_deleted"
    USER_ACTIVATED = "user_activated"
    USER_DEACTIVATED = "user_deactivated"
    PASSWORD_RESET = "password_reset"
    TOKEN_CREATED = "token_created"
    TOKEN_REVOKED = "token_revoked"
    LOGIN = "login"
    LOGOUT = "logout"
    SYSTEM = "system"


class AuditLogResponse(BaseModel):
    id: str
    ts: datetime
    actor_user_id: Optional[str] = None
    action: str
    object_type: Optional[str] = None
    object_id: Optional[str] = None
    meta: Optional[Dict[str, Any]] = None
    ip: Optional[str] = None
    user_agent: Optional[str] = None
    request_id: Optional[str] = None
    
    class Config:
        from_attributes = True


class AuditLogListResponse(BaseModel):
    logs: List[AuditLogResponse]
    total: int
    has_more: bool
    next_cursor: Optional[str] = None


# Error response schema
class ErrorResponse(BaseModel):
    code: str
    message: str
    request_id: Optional[str] = None
    details: Optional[Dict[str, Any]] = None

# ===== app/schemas/common.py =====
from __future__ import annotations
from typing import Optional, List, Dict, Any, Literal
from pydantic import BaseModel, Field
from datetime import datetime, date

class ErrorResponse(BaseModel):
    error: Dict[str, Any]
    request_id: Optional[str] = Field(None)

# ===== app/schemas/rag.py =====
from __future__ import annotations
from typing import Optional, List, Dict, Any, Literal
from pydantic import BaseModel, Field
from datetime import datetime, date

class RagSearchRequest(BaseModel):
    query: Optional[str] = Field(None)
    top_k: Optional[int] = Field(None)
    filters: Optional[Dict[str, Any]] = Field(None)
    with_snippets: Optional[bool] = Field(None)

class RagUploadRequest(BaseModel):
    url: Optional[str] = Field(None)
    name: Optional[str] = Field(None)
    tags: Optional[List[str]] = Field(None)

class RagDocument(BaseModel):
    id: Optional[str] = Field(None)
    name: Optional[str] = Field(None)
    status: Optional[str] = Field(None)
    date_upload: Optional[datetime] = Field(None)
    url_file: Optional[str] = Field(None)
    url_canonical_file: Optional[str] = Field(None)
    tags: Optional[List[str]] = Field(None)
    progress: Optional[float] = Field(None)

# ===== app/schemas/chats.py =====
from __future__ import annotations
from typing import Optional, List, Dict, Any, Literal
from pydantic import BaseModel, Field
from datetime import datetime, date

class ChatMessage(BaseModel):
    role: Literal['system', 'user', 'assistant', 'tool']
    content: str
    created_at: Optional[datetime] = Field(None)

class ChatTurnRequest(BaseModel):
    response_stream: Optional[bool] = Field(None)
    use_rag: Optional[bool] = Field(None)
    rag_params: Optional[Dict[str, Any]] = Field(None)
    messages: Optional[List[ChatMessage]] = Field(None)
    temperature: Optional[float] = Field(None)
    max_tokens: Optional[int] = Field(None)
    idempotency_key: Optional[str] = Field(None)

class ChatTurnResponse(BaseModel):
    chat_id: Optional[str] = Field(None)
    message_id: Optional[str] = Field(None)
    created_at: Optional[datetime] = Field(None)
    assistant_message: Optional[ChatMessage] = Field(None)
    usage: Optional[Dict[str, Any]] = Field(None)
    rag: Optional[Dict[str, Any]] = Field(None)

# ===== app/schemas/rag_schemas.py =====
from __future__ import annotations
from typing import Optional, List
from pydantic import BaseModel, Field, validator
from datetime import datetime
from enum import Enum

class RagStatus(str, Enum):
    UPLOADED = "uploaded"
    NORMALIZING = "normalizing"
    CHUNKING = "chunking"
    EMBEDDING = "embedding"
    INDEXING = "indexing"
    READY = "ready"
    ARCHIVED = "archived"
    DELETING = "deleting"
    ERROR = "error"

class RagDocumentUploadRequest(BaseModel):
    tags: Optional[List[str]] = Field(default_factory=list, description="Document tags")
    
    @validator('tags')
    def validate_tags(cls, v):
        if v is None:
            return []
        if len(v) > 20:
            raise ValueError('Too many tags (max 20)')
        for tag in v:
            if not isinstance(tag, str):
                raise ValueError('All tags must be strings')
            if len(tag) > 50:
                raise ValueError('Tag too long (max 50 characters)')
            if not tag.strip():
                raise ValueError('Empty tags not allowed')
        return [tag.strip() for tag in v if tag.strip()]

class RagDocumentTagsUpdateRequest(BaseModel):
    tags: List[str] = Field(..., description="Document tags")
    
    @validator('tags')
    def validate_tags(cls, v):
        if len(v) > 20:
            raise ValueError('Too many tags (max 20)')
        for tag in v:
            if not isinstance(tag, str):
                raise ValueError('All tags must be strings')
            if len(tag) > 50:
                raise ValueError('Tag too long (max 50 characters)')
            if not tag.strip():
                raise ValueError('Empty tags not allowed')
        return [tag.strip() for tag in v if tag.strip()]

class RagSearchRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=1000, description="Search query")
    top_k: int = Field(10, ge=1, le=100, description="Number of results")
    min_score: float = Field(0.0, ge=0.0, le=1.0, description="Minimum similarity score")
    
    @validator('text')
    def validate_text(cls, v):
        if not v.strip():
            raise ValueError('Search text cannot be empty')
        return v.strip()

class RagDocumentResponse(BaseModel):
    id: str
    name: Optional[str]
    status: RagStatus
    date_upload: datetime
    url_file: Optional[str]
    url_canonical_file: Optional[str]
    tags: List[str]
    error: Optional[str]
    updated_at: Optional[datetime]
    progress: Optional[float] = None
    
    class Config:
        from_attributes = True

class RagSearchResult(BaseModel):
    id: str
    document_id: str
    text: str
    score: float
    snippet: str
    
    class Config:
        from_attributes = True

class RagMetricsResponse(BaseModel):
    total_documents: int
    total_chunks: int
    processing_documents: int
    storage_size_bytes: int
    storage_size_mb: float
    status_breakdown: dict
    ready_documents: int
    error_documents: int

# ===== app/schemas/analyze.py =====
from __future__ import annotations
from typing import Optional, List, Dict, Any, Literal
from pydantic import BaseModel, Field
from datetime import datetime, date

class AnalyzeRequest(BaseModel):
    source: Optional[Dict[str, Any]] = Field(None)
    pipeline: Optional[Dict[str, Any]] = Field(None)
    language: Optional[str] = Field(None)
    priority: Optional[Literal['low', 'normal', 'high']] = Field(None)
    idempotency_key: Optional[str] = Field(None)

class AnalyzeResult(BaseModel):
    id: Optional[str] = Field(None)
    status: Optional[str] = Field(None)
    progress: Optional[float] = Field(None)
    result: Optional[Dict[str, Any]] = Field(None)
    artifacts: Optional[Dict[str, Any]] = Field(None)

# ===== app/api/deps.py =====
from __future__ import annotations
from typing import Optional, Dict, Any, List, Union
from fastapi import Depends, HTTPException, status, Request
from sqlalchemy.orm import Session
import hashlib
import time
import uuid

from app.core.db import get_session
from app.core.redis import get_redis
from app.core.security import get_bearer_token, decode_jwt
from app.core.config import settings
from app.repositories.users_repo import UsersRepo
from app.schemas.admin import UserRole

def db_session():
    """Real DB session dependency."""
    yield from get_session()

async def rate_limit(request: Request, key: str, limit: int, window_sec: int = 60, login: str = None) -> None:
    """Simple fixed-window rate limit on IP+key+login using Redis.
    Raises 429 if the number of hits in the current window exceeds `limit`.
    """
    r = get_redis()
    ip = get_client_ip(request)
    now = int(time.time())
    window = now - (now % window_sec)
    
    # Include login in rate limit key if provided
    if login:
        rl_key = f"rl:{key}:{ip}:{login}:{window}"
    else:
        rl_key = f"rl:{key}:{ip}:{window}"
    # Use Lua for atomic INCR+EXPIRE if available; fallback to simple ops
    try:
        cur = await r.incr(rl_key)  # type: ignore[attr-defined]
        if cur == 1:
            await r.expire(rl_key, window_sec)  # type: ignore[attr-defined]
        if cur > limit:
            raise HTTPException(status_code=status.HTTP_429_TOO_MANY_REQUESTS, detail="rate_limited")
    except AttributeError:
        # In case a sync client was wired accidentally
        cur = r.incr(rl_key)  # type: ignore
        if cur == 1:
            r.expire(rl_key, window_sec)  # type: ignore
        if cur > limit:
            raise HTTPException(status_code=status.HTTP_429_TOO_MANY_REQUESTS, detail="rate_limited")

def _ensure_access(payload: Dict[str, Any]) -> None:
    if payload.get("typ") != "access":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="invalid_token_type")

def get_current_user(token: str = Depends(get_bearer_token), session: Session = Depends(db_session)) -> Dict[str, Any]:
    """Resolve user from Bearer access JWT and DB."""
    payload = decode_jwt(token)
    _ensure_access(payload)
    sub = payload.get("sub")
    if not sub:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="invalid_token")
    repo = UsersRepo(session)
    user = repo.get(sub)
    if not user or getattr(user, "is_active", True) is False:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="user_not_found_or_inactive")
    return {"id": str(user.id), "login": user.login, "role": user.role, "fio": getattr(user, "fio", None)}


def require_roles(*roles: Union[str, UserRole]) -> callable:
    """RBAC dependency factory. Returns a dependency that requires specific roles."""
    role_values = [r.value if isinstance(r, UserRole) else r for r in roles]
    
    def check_roles(current_user: Dict[str, Any] = Depends(get_current_user)) -> Dict[str, Any]:
        user_role = current_user.get("role")
        if user_role not in role_values:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Access denied. Required roles: {role_values}, user role: {user_role}"
            )
        return current_user
    
    return check_roles


def require_admin(current_user: Dict[str, Any] = Depends(require_roles(UserRole.ADMIN))) -> Dict[str, Any]:
    """Require admin role."""
    return current_user

def require_upload_permission(current_user: Dict[str, Any] = Depends(get_current_user)) -> Dict[str, Any]:
    """Require upload permission (editor, admin, or reader if enabled)."""
    user_role = current_user.get("role")
    
    # Admin and editor always have upload permission
    if user_role in ["admin", "editor"]:
        return current_user
    
    # Reader can upload if enabled
    if user_role == "reader" and settings.ALLOW_READER_UPLOADS:
        return current_user
    
    raise HTTPException(
        status_code=status.HTTP_403_FORBIDDEN,
        detail={
            "error": {
                "code": "upload_forbidden",
                "message": "Upload permission denied. Reader uploads are disabled."
            }
        }
    )


def get_request_id(request: Request) -> str:
    """Extract or generate request ID."""
    request_id = request.headers.get("X-Request-ID")
    if not request_id:
        request_id = str(uuid.uuid4())
    return request_id


def get_client_ip(request: Request) -> str:
    """Extract client IP considering X-Forwarded-For header."""
    forwarded_for = request.headers.get("X-Forwarded-For")
    if forwarded_for:
        return forwarded_for.split(",")[0].strip()
    return request.client.host if request.client else "unknown"


def get_user_agent(request: Request) -> Optional[str]:
    """Extract user agent."""
    return request.headers.get("User-Agent")

# ===== app/api/sse.py =====
import asyncio
import json
from datetime import datetime
from starlette.responses import StreamingResponse

def sse_response(source, heartbeat_interval: int = 30):
    """
    Create SSE response with heartbeat
    
    Args:
        source: Generator/async generator that yields data
        heartbeat_interval: Heartbeat interval in seconds (default 30)
    """
    async def gen():
        last_heartbeat = datetime.now()
        
        try:
            async for item in source:
                # Send data
                data = item.get("data", "")
                if isinstance(data, dict):
                    data = json.dumps(data)
                yield f"data: {data}\n\n"
                
                # Update heartbeat timestamp
                last_heartbeat = datetime.now()
                
                # Check if we need to send heartbeat
                now = datetime.now()
                if (now - last_heartbeat).total_seconds() >= heartbeat_interval:
                    yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': now.isoformat()})}\n\n"
                    last_heartbeat = now
                    
        except asyncio.CancelledError:
            # Client disconnected
            pass
        except Exception as e:
            # Send error event
            error_data = json.dumps({
                "type": "error",
                "message": str(e),
                "timestamp": datetime.now().isoformat()
            })
            yield f"data: {error_data}\n\n"
    
    return StreamingResponse(
        gen(), 
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )

def sse_heartbeat_response(heartbeat_interval: int = 30):
    """
    Create SSE response that only sends heartbeats
    
    Args:
        heartbeat_interval: Heartbeat interval in seconds
    """
    async def gen():
        while True:
            try:
                heartbeat_data = json.dumps({
                    "type": "heartbeat",
                    "timestamp": datetime.now().isoformat()
                })
                yield f"data: {heartbeat_data}\n\n"
                await asyncio.sleep(heartbeat_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                error_data = json.dumps({
                    "type": "error",
                    "message": str(e),
                    "timestamp": datetime.now().isoformat()
                })
                yield f"data: {error_data}\n\n"
                break
    
    return StreamingResponse(
        gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

# ===== app/services/auth_service.py =====
from __future__ import annotations
import hashlib, uuid
from datetime import datetime, timedelta, timezone
from typing import Tuple, Optional
from sqlalchemy.orm import Session

from app.repositories.users_repo import UsersRepo
from app.core.security import verify_password, encode_jwt, decode_jwt
from app.core.config import settings
from app.models.user import UserRefreshTokens

def _hash_refresh(token: str) -> str:
    return hashlib.sha256(token.encode()).hexdigest()

def _now() -> datetime:
    return datetime.now(timezone.utc)

def login(session: Session, login: str, password: str) -> Tuple[str, str, uuid.UUID]:
    repo = UsersRepo(session)
    user = repo.by_login(login)
    if not user or not verify_password(password, user.password_hash):
        raise ValueError("invalid_credentials")
    sub = str(user.id)
    access = encode_jwt({"sub": sub, "typ": "access"}, ttl_seconds=settings.ACCESS_TTL_SECONDS)
    refresh = encode_jwt({"sub": sub, "typ": "refresh"}, ttl_seconds=settings.REFRESH_TTL_DAYS * 86400)
    # persist refresh (hashed)
    rec = UserRefreshTokens(
        user_id=user.id,
        refresh_hash=_hash_refresh(refresh),
        issued_at=_now(),
        expires_at=_now() + timedelta(days=settings.REFRESH_TTL_DAYS),
        rotating=settings.REFRESH_ROTATING,
        revoked=False,
        meta=None,
    )
    repo.add_refresh(rec)
    return access, refresh, user.id

def refresh(session: Session, refresh_token: str) -> Tuple[str, Optional[str]]:
    payload = decode_jwt(refresh_token)
    if payload.get("typ") != "refresh":
        raise ValueError("not_refresh")
    sub = payload.get("sub")
    if not sub:
        raise ValueError("invalid_refresh")
    repo = UsersRepo(session)
    rec = repo.get_refresh_by_hash(_hash_refresh(refresh_token))
    if not rec or rec.revoked:
        raise ValueError("revoked")
    if rec.expires_at and rec.expires_at < _now():
        raise ValueError("expired")
    # rotate if configured
    access = encode_jwt({"sub": str(sub), "typ": "access"}, ttl_seconds=settings.ACCESS_TTL_SECONDS)
    if rec.rotating and settings.REFRESH_ROTATING:
        rec.revoked = True
        new_refresh = encode_jwt({"sub": str(sub), "typ": "refresh"}, ttl_seconds=settings.REFRESH_TTL_DAYS * 86400)
        new_rec = UserRefreshTokens(
            user_id=rec.user_id,
            refresh_hash=_hash_refresh(new_refresh),
            issued_at=_now(),
            expires_at=_now() + timedelta(days=settings.REFRESH_TTL_DAYS),
            rotating=True,
            revoked=False,
            meta=None,
        )
        repo.add_refresh(new_rec)
        return access, new_refresh
    # non-rotating: allow re-use
    return access, refresh_token

def revoke_refresh(session: Session, refresh_token: str) -> bool:
    return UsersRepo(session).revoke_refresh(_hash_refresh(refresh_token))

# ===== app/services/chat_service.py =====
from __future__ import annotations
from sqlalchemy.orm import Session
from app.repositories.chats_repo import ChatsRepo

def create_chat(session: Session, owner_id, name: str | None):
    return ChatsRepo(session).create_chat(owner_id, name)

def list_chats(session: Session, owner_id):
    return ChatsRepo(session).list_chats(owner_id)

def post_message(session: Session, chat_id, role: str, content: dict, model: str | None = None):
    repo = ChatsRepo(session)
    chat = repo.get(chat_id)
    if not chat:
        raise ValueError("chat_not_found")
    msg = repo.add_message(chat_id, role, content, model)
    chat.last_message_at = msg.created_at
    return msg

def list_messages(session: Session, chat_id):
    return ChatsRepo(session).list_messages(chat_id)

def delete_chat(session: Session, chat_id):
    repo = ChatsRepo(session)
    chat = repo.get(chat_id)
    if not chat:
        return False
    repo.delete(chat)
    return True

# ===== app/services/analyze_service.py =====
from __future__ import annotations
from sqlalchemy.orm import Session
from app.repositories.analyze_repo import AnalyzeRepo

def create_job(session: Session, uploaded_by=None, url_file: str | None = None):
    return AnalyzeRepo(session).create_document(uploaded_by=uploaded_by, url_file=url_file, status="queued")

def list_jobs(session: Session, limit: int = 50):
    return AnalyzeRepo(session).list(limit=limit)

def get_job(session: Session, job_id):
    return AnalyzeRepo(session).get(job_id)

def delete_job(session: Session, job_id):
    repo = AnalyzeRepo(session)
    doc = repo.get(job_id)
    if not doc:
        return False
    repo.delete(doc)
    return True

# ===== app/services/reranker.py =====
from __future__ import annotations

from typing import List, Dict, Any, Tuple
import json
import logging

logger = logging.getLogger(__name__)

class Reranker:
    """
    Reranker using cross-encoder for better search results
    """
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.model_name = model_name
        self._model = None
        self._tokenizer = None
    
    def _load_model(self):
        """Lazy load the cross-encoder model"""
        if self._model is None:
            try:
                from sentence_transformers import CrossEncoder
                self._model = CrossEncoder(self.model_name)
                logger.info(f"Loaded reranker model: {self.model_name}")
            except ImportError:
                logger.warning("sentence-transformers not available, reranking disabled")
                self._model = None
            except Exception as e:
                logger.error(f"Failed to load reranker model: {e}")
                self._model = None
    
    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        """
        Rerank documents based on query relevance
        
        Args:
            query: Search query
            documents: List of documents with 'text' field
            top_k: Number of top results to return
            
        Returns:
            Reranked list of documents with relevance scores
        """
        if not documents:
            return []
        
        self._load_model()
        
        if self._model is None:
            # Fallback: return original order with dummy scores
            return [{"document": doc, "score": 1.0} for doc in documents[:top_k]]
        
        try:
            # Prepare query-document pairs
            pairs = [(query, doc.get('text', '')) for doc in documents]
            
            # Get relevance scores
            scores = self._model.predict(pairs)
            
            # Combine documents with scores
            scored_docs = []
            for doc, score in zip(documents, scores):
                scored_docs.append({
                    "document": doc,
                    "score": float(score)
                })
            
            # Sort by score (descending)
            scored_docs.sort(key=lambda x: x["score"], reverse=True)
            
            return scored_docs[:top_k]
            
        except Exception as e:
            logger.error(f"Reranking failed: {e}")
            # Fallback: return original order
            return [{"document": doc, "score": 1.0} for doc in documents[:top_k]]

class SemanticReranker:
    """
    Semantic reranker using embedding similarity
    """
    
    def __init__(self, embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.embedding_model = embedding_model
        self._model = None
    
    def _load_model(self):
        """Lazy load the embedding model"""
        if self._model is None:
            try:
                from sentence_transformers import SentenceTransformer
                self._model = SentenceTransformer(self.embedding_model)
                logger.info(f"Loaded semantic reranker model: {self.embedding_model}")
            except ImportError:
                logger.warning("sentence-transformers not available, semantic reranking disabled")
                self._model = None
            except Exception as e:
                logger.error(f"Failed to load semantic reranker model: {e}")
                self._model = None
    
    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        """
        Rerank documents using semantic similarity
        """
        if not documents:
            return []
        
        self._load_model()
        
        if self._model is None:
            # Fallback: return original order
            return [{"document": doc, "score": 1.0} for doc in documents[:top_k]]
        
        try:
            import numpy as np
            from sklearn.metrics.pairwise import cosine_similarity
            
            # Encode query and documents
            query_embedding = self._model.encode([query])
            doc_texts = [doc.get('text', '') for doc in documents]
            doc_embeddings = self._model.encode(doc_texts)
            
            # Calculate similarities
            similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
            
            # Combine documents with scores
            scored_docs = []
            for doc, score in zip(documents, similarities):
                scored_docs.append({
                    "document": doc,
                    "score": float(score)
                })
            
            # Sort by score (descending)
            scored_docs.sort(key=lambda x: x["score"], reverse=True)
            
            return scored_docs[:top_k]
            
        except Exception as e:
            logger.error(f"Semantic reranking failed: {e}")
            # Fallback: return original order
            return [{"document": doc, "score": 1.0} for doc in documents[:top_k]]

def rerank_search_results(query: str, results: List[Dict[str, Any]], method: str = "cross-encoder", top_k: int = 10) -> List[Dict[str, Any]]:
    """
    Convenience function for reranking search results
    
    Args:
        query: Search query
        results: List of search results from Qdrant
        method: Reranking method ("cross-encoder" or "semantic")
        top_k: Number of top results to return
        
    Returns:
        Reranked results with relevance scores
    """
    if method == "cross-encoder":
        reranker = Reranker()
    elif method == "semantic":
        reranker = SemanticReranker()
    else:
        logger.warning(f"Unknown reranking method: {method}, using cross-encoder")
        reranker = Reranker()
    
    return reranker.rerank(query, results, top_k)

# ===== app/services/rag_service.py =====
# app/services/rag_service.py
# Best-practice naming for RAG + analysis:
# - store original filename only in DB metadata
# - in MinIO use doc_id as "directory":
#     RAW:       {doc_id}/source{ext}
#     CANONICAL: {doc_id}/document.json
#     PREVIEW:   {doc_id}/preview/page-{n}.png
#
# Also adds dual delete modes:
# - soft delete: sets status='archived' (keeps data)
# - hard delete: Celery task purges MinIO objects + Qdrant points + DB rows

from __future__ import annotations
from typing import Any, Dict, Optional
from datetime import datetime

from sqlalchemy.orm import Session

from app.core.config import settings
from app.core.s3 import presign_put
from app.core.metrics import rag_ingest_started_total
from app.repositories.rag_repo import RagRepo
from app.tasks.normalize import process as normalize_process
from app.tasks.chunk import split as chunk_split
from app.tasks.embed import compute as embed_compute
from app.tasks.index import finalize as index_finalize
from app.tasks.upload_watch import watch as start_upload_watch
from app.tasks.delete import hard_delete as hard_delete_task
from app.models.rag import RagDocuments, RagChunks
from . import clients

# White-list of allowed extensions
ALLOWED_EXTENSIONS = {'.txt', '.pdf', '.doc', '.docx', '.md', '.rtf', '.odt'}


def _safe_ext(filename: Optional[str]) -> str:
    if not filename or '.' not in filename:
        return ''
    ext = '.' + filename.rsplit('.', 1)[-1].lower()
    return ext if ext in ALLOWED_EXTENSIONS else ''


def create_upload(session: Session, filename: str, uploaded_by: Optional[str] = None) -> Dict[str, Any]:
    """Create a document record and return a presigned PUT URL for the RAG upload.
    New naming logic: rag/{doc_id}/origin.{ext} for original files
    """
    file_ext = _safe_ext(filename)
    if filename and file_ext == '':
        raise ValueError(f"Unsupported file type. Allowed: {', '.join(sorted(ALLOWED_EXTENSIONS))}")

    repo = RagRepo(session)
    # Persist original name in DB; storage key does NOT depend on it
    doc = repo.create_document(name=filename, uploaded_by=uploaded_by, status="uploaded")

    # New naming: rag/{doc_id}/origin.{ext}
    key = f"{doc.id}/origin{file_ext}"
    put_url = presign_put(settings.S3_BUCKET_RAG, key, 3600)

    # Save storage key for downstream tasks
    doc.url_file = key
    doc.updated_at = datetime.utcnow()
    session.commit()

    # Don't start watcher yet - wait for file to be uploaded via presigned URL
    # start_upload_watch(str(doc.id), key=key)
    
    # Запускаем цепочку обработки сразу после создания документа
    start_ingest_chain(str(doc.id))

    return {"id": str(doc.id), "put_url": put_url, "key": key}


def delete_document(session: Session, doc_id: str, *, hard: bool = False) -> bool:
    """Two deletion modes:
    - soft (default): mark as archived (keeps MinIO/Qdrant/DB children intact)
    - hard: schedule full purge task that removes MinIO objects, Qdrant points and DB rows
    """
    repo = RagRepo(session)
    doc = repo.get(doc_id)
    if not doc:
        return False

    if hard:
        # Fire-and-forget Celery task
        hard_delete_task.delay(str(doc.id))
        return True

    # Soft delete: archive only (no storage/Qdrant changes)
    doc.status = "archived"
    doc.updated_at = datetime.utcnow()
    session.commit()
    return True


def list_documents(session: Session, limit: int = 50):
    return RagRepo(session).list(limit=limit)


def get_document(session: Session, doc_id):
    return RagRepo(session).get(doc_id)


def start_ingest_chain(doc_id: str) -> None:
    rag_ingest_started_total.inc()
    # Используем правильный синтаксис для Celery цепочки
    from celery import chain
    chain(
        normalize_process.s(doc_id),
        chunk_split.s(),
        embed_compute.s(),
        index_finalize.s()
    ).apply_async()


def search(session: Session, query: str, top_k: int = 5, *, offset: int = 0, doc_id: Optional[str] = None, tags: Optional[list] = None, sort_by: str = "score_desc") -> Dict[str, Any]:
    vectors = clients.embed_texts([query])
    if not vectors:
        return {"results": [], "next_offset": None}
    vec = vectors[0]
    hits = clients.qdrant_search(vec, top_k=top_k, offset=offset, doc_id=doc_id, tags=tags, sort_by=sort_by)
    out = []
    for h in hits:
        payload = h.get("payload") or {}
        out.append({
            "score": h["score"],
            "text": payload.get("text"),
            "doc_id": payload.get("document_id"),
            "chunk_idx": payload.get("chunk_idx"),
            "tags": payload.get("tags") or [],
        })
    next_offset = offset + len(out) if len(out) == top_k else None
    return {"results": out, "next_offset": next_offset}


def progress(session: Session, doc_id: str) -> Dict[str, Any]:
    from sqlalchemy import func
    doc = session.get(RagDocuments, doc_id)
    if not doc:
        return {"id": doc_id, "status": "not_found"}
    chunks_total = session.query(func.count(RagChunks.id)).filter(RagChunks.document_id == doc.id).scalar() or 0
    vectors_total = clients.qdrant_count_by_doc(str(doc.id))
    return {
        "id": str(doc.id),
        "status": doc.status,
        "chunks_total": int(chunks_total),
        "vectors_total": int(vectors_total),
        "updated_at": (doc.updated_at.isoformat() if getattr(doc, "updated_at", None) else None),
    }


def stats(session: Session) -> Dict[str, Any]:
    from sqlalchemy import func
    rows = session.query(RagDocuments.status, func.count(RagDocuments.id)).group_by(RagDocuments.status).all()
    by_status = {k or "unknown": int(v or 0) for k, v in rows}
    total_docs = sum(by_status.values())
    return {"total_docs": total_docs, "by_status": by_status}


def get_download_url(session: Session, doc_id: str, file_type: str = "original") -> str:
    """Get download URL for a document file."""
    doc = RagRepo(session).get(doc_id)
    if not doc:
        raise ValueError("Document not found")
    
    if file_type == "original":
        key = doc.url_file
        bucket = settings.S3_BUCKET_RAG
    elif file_type == "canonical":
        key = doc.url_canonical_file
        bucket = settings.S3_BUCKET_RAG
    elif file_type == "preview":
        key = doc.url_preview_file
        bucket = settings.S3_BUCKET_RAG
    else:
        raise ValueError("Invalid file type")
    
    if not key:
        raise ValueError(f"{file_type} file not available")
    
    return presign_put(bucket, key, 3600)


def reprocess_document(session: Session, doc_id: str) -> bool:
    """Reprocess a document through the entire pipeline."""
    doc = RagRepo(session).get(doc_id)
    if not doc:
        return False
    
    # Reset status and clear previous results
    doc.status = "uploaded"
    doc.url_canonical_file = None
    doc.updated_at = datetime.utcnow()
    session.commit()
    
    # Start the processing chain
    start_ingest_chain(str(doc.id))
    return True

# ===== app/services/text_normalizer.py =====
from __future__ import annotations

import re
import unicodedata
from typing import List

ZERO_WIDTH = (
    "\u200B"  # zero width space
    "\u200C"  # zero width non-joiner
    "\u200D"  # zero width joiner
    "\u2060"  # word joiner
    "\uFEFF"  # zero width no-break space (BOM)
)
CONTROL_CHARS = "".join(map(chr, list(range(0, 32)) + [127]))
CONTROL_RE = re.compile(f"[{re.escape(CONTROL_CHARS)}]", flags=re.UNICODE)
ZEROW_RE = re.compile(f"[{re.escape(ZERO_WIDTH)}]", flags=re.UNICODE)

PUNCT_MAP = {
    "\u2018": "'", "\u2019": "'", "\u201A": "'", "\u201B": "'",
    "\u201C": '"', "\u201D": '"', "\u201E": '"', "\u201F": '"',
    "\u2013": "-", "\u2014": "-", "\u2212": "-", "\u00AD": "",  # soft hyphen
    "\u00A0": " ",  # nbsp
}
BULLET_RE = re.compile(r"^\s*([•·▪◦►▶»\-–—])\s+", re.UNICODE)
MULTISPACE_RE = re.compile(r"[ \t\f\v]+")
BLANKS_RE = re.compile(r"\n{3,}")
HYPHEN_WRAP_RE = re.compile(r"(\w)[\-­]\n(\w)", flags=re.UNICODE)
SOFT_BREAK_RE = re.compile(r"([^\S\n]*\n)(?=\S)(?<!\.\n)", flags=re.UNICODE)

def normalize_text(text: str) -> str:
    """
    Canonicalize text:
      - Unicode NFKC
      - remove zero-width/control chars
      - map fancy quotes/dashes to standard ASCII
      - fix hyphenated line wraps and soft breaks
      - normalize bullets to "- "
      - collapse spaces and blank lines
    """
    if not text:
        return ""

    t = unicodedata.normalize("NFKC", text)
    t = t.translate(str.maketrans(PUNCT_MAP))
    t = ZEROW_RE.sub("", t)
    t = CONTROL_RE.sub("", t)
    t = t.replace("\r\n", "\n").replace("\r", "\n")
    t = HYPHEN_WRAP_RE.sub(r"\1\2", t)
    t = SOFT_BREAK_RE.sub(" ", t)

    lines = []
    for line in t.split("\n"):
        if BULLET_RE.match(line):
            line = "- " + BULLET_RE.sub("", line, count=1).strip()
        else:
            line = line.strip()
        line = MULTISPACE_RE.sub(" ", line)
        lines.append(line)

    t = "\n".join(lines)
    t = BLANKS_RE.sub("\n\n", t)
    return t.strip()

# ===== app/services/text_extractor.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple
from io import BytesIO
import csv
import json

# External deps (add to requirements):
# - pdfminer.six
# - PyPDF2 (optional fallback/meta)
# - python-docx
# - openpyxl
# - charset-normalizer
#
# We import lazily inside functions to avoid import errors when a format isn't used.

@dataclass
class ExtractResult:
    text: str
    kind: str
    meta: Dict[str, Any]
    warnings: List[str]

    def to_json(self) -> str:
        return json.dumps(
            {
                "text": self.text,
                "type": "text",
                "extractor": self.kind,
                "meta": self.meta,
                "warnings": self.warnings,
            },
            ensure_ascii=False,
        )


def _detect_ext(filename: str) -> str:
    name = (filename or "").lower()
    if "." not in name:
        return ""
    return name[name.rfind(".") + 1 :]  # ext without dot


def _decode_best_effort(data: bytes) -> Tuple[str, str, List[str]]:
    """Decode bytes to str using charset-normalizer (fallback to utf-8)."""
    warnings: List[str] = []
    try:
        from charset_normalizer import from_bytes as cn_from_bytes  # type: ignore
        res = cn_from_bytes(data).best()
        if res is not None:
            return str(res), (res.encoding or "utf-8"), warnings
        warnings.append("charset-normalizer: no best match, fallback to utf-8.")
        return data.decode("utf-8", errors="replace"), "utf-8", warnings
    except Exception as e:  # optional dep may be absent in tests
        warnings.append(f"charset-normalizer unavailable ({e!r}); fallback to utf-8.")
        return data.decode("utf-8", errors="replace"), "utf-8", warnings


def _extract_pdf(data: bytes) -> ExtractResult:
    warnings: List[str] = []
    text = ""
    pages = 0
    try:
        # pdfminer.six is more accurate for text PDFs than PyPDF2
        from pdfminer.high_level import extract_text as pdf_extract_text  # type: ignore
        text = pdf_extract_text(BytesIO(data)) or ""
        # Try PyPDF2 for meta/page count
        try:
            import PyPDF2  # type: ignore
            r = PyPDF2.PdfReader(BytesIO(data))
            pages = len(r.pages)
        except Exception:
            pages = 0
        if not text.strip():
            warnings.append("PDF appears to have no extractable text (maybe scanned). Consider OCR later.")
    except Exception as e:
        warnings.append(f"PDF extraction failed via pdfminer: {e!r}")
        # last resort: try PyPDF2.extract_text
        try:
            import PyPDF2  # type: ignore
            r = PyPDF2.PdfReader(BytesIO(data))
            pages = len(r.pages)
            text = ""
            for p in r.pages:
                try:
                    text += (p.extract_text() or "") + "\n"
                except Exception:
                    continue
            if not text.strip():
                warnings.append("PyPDF2 yielded empty text. Consider OCR.")
        except Exception as e2:
            warnings.append(f"PyPDF2 fallback failed: {e2!r}")
            text = ""
    return ExtractResult(text=text, kind="pdf", meta={"pages": pages}, warnings=warnings)


def _extract_docx(data: bytes) -> ExtractResult:
    warnings: List[str] = []
    text = ""
    try:
        from docx import Document  # type: ignore
        doc = Document(BytesIO(data))
        parts: List[str] = []
        # paragraphs
        for p in doc.paragraphs:
            if p.text:
                parts.append(p.text)
        # tables
        for t in doc.tables:
            for row in t.rows:
                cells = [c.text.strip() for c in row.cells]
                if any(cells):
                    parts.append("\t".join(cells))
        text = "\n".join(parts).strip()
        if not text:
            warnings.append("DOCX parsed but no visible text was found.")
    except Exception as e:
        warnings.append(f"DOCX extraction failed: {e!r}")
        text = ""
    return ExtractResult(text=text, kind="docx", meta={}, warnings=warnings)


def _extract_txt(data: bytes) -> ExtractResult:
    text, enc, warn = _decode_best_effort(data)
    return ExtractResult(text=text, kind=f"txt({enc})", meta={"encoding": enc}, warnings=warn)


def _extract_csv(data: bytes) -> ExtractResult:
    text, enc, warn = _decode_best_effort(data)
    # Try sniffing CSV dialect; render as TSV-like plain text
    out_lines: List[str] = []
    try:
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(text[:10000])
    except Exception:
        dialect = csv.excel
    reader = csv.reader(text.splitlines(), dialect=dialect)
    for row in reader:
        out_lines.append("\t".join("" if v is None else str(v) for v in row))
    return ExtractResult(text="\n".join(out_lines), kind=f"csv({enc})", meta={"encoding": enc}, warnings=warn)


def _extract_xlsx(data: bytes) -> ExtractResult:
    warnings: List[str] = []
    text = ""
    try:
        from openpyxl import load_workbook  # type: ignore
        wb = load_workbook(BytesIO(data), data_only=True, read_only=True)
        out_lines: List[str] = []
        for ws in wb.worksheets:
            out_lines.append(f"# Sheet: {ws.title}")
            for row in ws.iter_rows(values_only=True):
                out_lines.append("\t".join("" if v is None else str(v) for v in row))
            out_lines.append("")
        text = "\n".join(out_lines).strip()
    except Exception as e:
        warnings.append(f"XLSX extraction failed: {e!r}")
        text = ""
    return ExtractResult(text=text, kind="xlsx", meta={}, warnings=warnings)


def extract_text(data: bytes, filename: str) -> ExtractResult:
    """
    Universal text extractor for: PDF, DOCX, TXT, CSV, XLSX.
    Returns ExtractResult(text, kind, meta, warnings).
    """
    ext = _detect_ext(filename)
    if ext in ("pdf",):
        return _extract_pdf(data)
    if ext in ("docx",):
        return _extract_docx(data)
    if ext in ("txt", "log", ""):
        return _extract_txt(data)
    if ext in ("csv",):
        return _extract_csv(data)
    if ext in ("xlsx",):
        return _extract_xlsx(data)
    # Unknown → try text decode
    res = _extract_txt(data)
    res.warnings.append(f"Unknown extension '.{ext}', treated as text.")
    return res

# ===== app/services/__init__.py =====
# services package init

# ===== app/services/enhanced_text_extractor.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional
from io import BytesIO
import csv
import json
import base64

# External deps (add to requirements):
# - pdfminer.six
# - PyPDF2 (optional fallback/meta)
# - python-docx
# - openpyxl
# - charset-normalizer
# - pdfplumber (for tables)
# - pdf2image (for OCR)
# - pytesseract (for OCR)
# - mammoth (for DOCX)
# - pandas (for CSV/Excel)

@dataclass
class TableData:
    name: str
    csv_data: str
    rows: int
    cols: int

@dataclass
class ExtractResult:
    text: str
    kind: str
    meta: Dict[str, Any]
    warnings: List[str]
    tables: List[TableData]

    def to_json(self) -> str:
        return json.dumps(
            {
                "text": self.text,
                "tables": [{"name": t.name, "csv": t.csv_data, "rows": t.rows, "cols": t.cols} for t in self.tables],
                "meta": self.meta,
                "extractor": self.kind,
                "warnings": self.warnings,
            },
            ensure_ascii=False
        )

def extract_text_enhanced(content: bytes, filename: str) -> ExtractResult:
    """
    Enhanced text extraction with OCR fallback and table support
    """
    if not content:
        return ExtractResult("", "empty", {}, [], [])
    
    ext = filename.lower().split('.')[-1] if '.' in filename else ''
    warnings = []
    tables = []
    
    # Text files
    if ext in ('txt', 'md', 'rtf'):
        return _extract_text_file(content, ext, warnings)
    
    # CSV files
    elif ext == 'csv':
        return _extract_csv_file(content, warnings)
    
    # PDF files
    elif ext == 'pdf':
        return _extract_pdf_enhanced(content, warnings)
    
    # DOCX files
    elif ext == 'docx':
        return _extract_docx_enhanced(content, warnings)
    
    # XLSX files
    elif ext == 'xlsx':
        return _extract_xlsx_enhanced(content, warnings)
    
    # DOC files (legacy)
    elif ext == 'doc':
        return _extract_doc_file(content, warnings)
    
    # Unknown format - try as text
    else:
        warnings.append(f"Unknown extension '.{ext}', treated as text.")
        return _extract_text_file(content, 'txt', warnings)

def _extract_text_file(content: bytes, ext: str, warnings: List[str]) -> ExtractResult:
    """Extract text from plain text files"""
    try:
        # Try to detect encoding
        import charset_normalizer
        detected = charset_normalizer.detect(content)
        encoding = detected.get('encoding', 'utf-8')
        confidence = detected.get('confidence', 0.0)
        
        if confidence < 0.7:
            warnings.append(f"Low confidence encoding detection: {encoding} ({confidence:.2f})")
        
        text = content.decode(encoding, errors='replace')
        
        # Handle RTF
        if ext == 'rtf':
            text = _clean_rtf(text)
        
        return ExtractResult(
            text=text,
            kind=f"txt({encoding})",
            meta={"encoding": encoding, "confidence": confidence},
            warnings=warnings,
            tables=[]
        )
    except Exception as e:
        warnings.append(f"Text extraction failed: {e}")
        return ExtractResult(
            text=content.decode('utf-8', errors='replace'),
            kind="txt(utf_8_fallback)",
            meta={"encoding": "utf-8", "error": str(e)},
            warnings=warnings,
            tables=[]
        )

def _extract_csv_file(content: bytes, warnings: List[str]) -> ExtractResult:
    """Extract text and tables from CSV files"""
    try:
        import pandas as pd
        import io
        
        # Detect encoding
        import charset_normalizer
        detected = charset_normalizer.detect(content)
        encoding = detected.get('encoding', 'utf-8')
        
        # Read CSV
        df = pd.read_csv(io.BytesIO(content), encoding=encoding)
        
        # Convert to text
        text = df.to_string(index=False)
        
        # Create table data
        csv_data = df.to_csv(index=False)
        table = TableData(
            name="main_table",
            csv_data=csv_data,
            rows=len(df),
            cols=len(df.columns)
        )
        
        return ExtractResult(
            text=text,
            kind="csv(enhanced)",
            meta={"encoding": encoding, "rows": len(df), "cols": len(df.columns)},
            warnings=warnings,
            tables=[table]
        )
    except Exception as e:
        warnings.append(f"Enhanced CSV extraction failed: {e}")
        # Fallback to simple CSV
        return _extract_simple_csv(content, warnings)

def _extract_simple_csv(content: bytes, warnings: List[str]) -> ExtractResult:
    """Simple CSV extraction fallback"""
    try:
        import csv
        import io
        
        # Detect encoding
        import charset_normalizer
        detected = charset_normalizer.detect(content)
        encoding = detected.get('encoding', 'utf-8')
        
        # Read CSV
        text_content = content.decode(encoding, errors='replace')
        csv_reader = csv.reader(io.StringIO(text_content))
        rows = list(csv_reader)
        
        # Format as text
        text = '\n'.join(['\t'.join(row) for row in rows])
        
        # Create table data
        csv_data = text_content
        table = TableData(
            name="main_table",
            csv_data=csv_data,
            rows=len(rows),
            cols=len(rows[0]) if rows else 0
        )
        
        return ExtractResult(
            text=text,
            kind="csv(simple)",
            meta={"encoding": encoding, "rows": len(rows)},
            warnings=warnings,
            tables=[table]
        )
    except Exception as e:
        warnings.append(f"CSV extraction failed: {e}")
        return ExtractResult(
            text=content.decode('utf-8', errors='replace'),
            kind="csv(fallback)",
            meta={"encoding": "utf-8", "error": str(e)},
            warnings=warnings,
            tables=[]
        )

def _extract_pdf_enhanced(content: bytes, warnings: List[str]) -> ExtractResult:
    """Enhanced PDF extraction with OCR fallback and table support"""
    text = ""
    tables = []
    meta = {}
    
    # Try pdfplumber first (better for tables)
    try:
        import pdfplumber
        with pdfplumber.open(BytesIO(content)) as pdf:
            pages_text = []
            all_tables = []
            
            for page_num, page in enumerate(pdf.pages):
                # Extract text
                page_text = page.extract_text()
                if page_text:
                    pages_text.append(page_text)
                
                # Extract tables
                page_tables = page.extract_tables()
                for table_num, table in enumerate(page_tables):
                    if table and len(table) > 1:  # Skip empty tables
                        # Convert to CSV
                        csv_data = '\n'.join([','.join([str(cell or '') for cell in row]) for row in table])
                        table_data = TableData(
                            name=f"page_{page_num+1}_table_{table_num+1}",
                            csv_data=csv_data,
                            rows=len(table),
                            cols=len(table[0]) if table else 0
                        )
                        all_tables.append(table_data)
            
            text = '\n\n'.join(pages_text)
            tables = all_tables
            meta = {"pages": len(pdf.pages), "tables_found": len(tables)}
            
            if text.strip():
                return ExtractResult(
                    text=text,
                    kind="pdf(pdfplumber)",
                    meta=meta,
                    warnings=warnings,
                    tables=tables
                )
    except Exception as e:
        warnings.append(f"PDF extraction via pdfplumber failed: {e}")
    
    # Try pdfminer.six
    try:
        from pdfminer.high_level import extract_text
        text = extract_text(BytesIO(content))
        if text.strip():
            return ExtractResult(
                text=text,
                kind="pdf(pdfminer)",
                meta={"pages": 1, "method": "pdfminer"},
                warnings=warnings,
                tables=tables
            )
    except Exception as e:
        warnings.append(f"PDF extraction via pdfminer failed: {e}")
    
    # Try PyPDF2
    try:
        import PyPDF2
        pdf_reader = PyPDF2.PdfReader(BytesIO(content))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        
        if text.strip():
            return ExtractResult(
                text=text,
                kind="pdf(pypdf2)",
                meta={"pages": len(pdf_reader.pages), "method": "pypdf2"},
                warnings=warnings,
                tables=tables
            )
    except Exception as e:
        warnings.append(f"PDF extraction via PyPDF2 failed: {e}")
    
    # OCR fallback
    try:
        return _extract_pdf_ocr(content, warnings)
    except Exception as e:
        warnings.append(f"OCR extraction failed: {e}")
    
    # Final fallback
    return ExtractResult(
        text="",
        kind="pdf(failed)",
        meta={"pages": 0, "error": "All extraction methods failed"},
        warnings=warnings,
        tables=[]
    )

def _extract_pdf_ocr(content: bytes, warnings: List[str]) -> ExtractResult:
    """OCR extraction for scanned PDFs"""
    try:
        from pdf2image import convert_from_bytes
        import pytesseract
        
        # Convert PDF to images
        images = convert_from_bytes(content, dpi=300)
        
        text_parts = []
        for i, image in enumerate(images):
            # OCR each page
            page_text = pytesseract.image_to_string(image, lang='rus+eng')
            text_parts.append(page_text)
        
        text = '\n\n'.join(text_parts)
        
        return ExtractResult(
            text=text,
            kind="pdf(ocr)",
            meta={"pages": len(images), "method": "ocr", "lang": "rus+eng"},
            warnings=warnings,
            tables=[]
        )
    except Exception as e:
        raise Exception(f"OCR processing failed: {e}")

def _extract_docx_enhanced(content: bytes, warnings: List[str]) -> ExtractResult:
    """Enhanced DOCX extraction with table support"""
    try:
        from docx import Document
        from docx.table import Table
        
        doc = Document(BytesIO(content))
        
        # Extract text
        text_parts = []
        tables = []
        
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_parts.append(paragraph.text)
        
        # Extract tables
        for table_num, table in enumerate(doc.tables):
            table_data = []
            for row in table.rows:
                row_data = [cell.text.strip() for cell in row.cells]
                table_data.append(row_data)
            
            if table_data:
                # Convert to CSV
                csv_data = '\n'.join([','.join(row) for row in table_data])
                table_obj = TableData(
                    name=f"table_{table_num+1}",
                    csv_data=csv_data,
                    rows=len(table_data),
                    cols=len(table_data[0]) if table_data else 0
                )
                tables.append(table_obj)
        
        text = '\n'.join(text_parts)
        
        return ExtractResult(
            text=text,
            kind="docx(enhanced)",
            meta={"tables_found": len(tables)},
            warnings=warnings,
            tables=tables
        )
    except Exception as e:
        warnings.append(f"Enhanced DOCX extraction failed: {e}")
        # Fallback to mammoth
        return _extract_docx_mammoth(content, warnings)

def _extract_docx_mammoth(content: bytes, warnings: List[str]) -> ExtractResult:
    """DOCX extraction using mammoth"""
    try:
        import mammoth
        
        result = mammoth.extract_raw_text(BytesIO(content))
        text = result.value
        
        return ExtractResult(
            text=text,
            kind="docx(mammoth)",
            meta={"method": "mammoth"},
            warnings=warnings,
            tables=[]
        )
    except Exception as e:
        warnings.append(f"DOCX extraction via mammoth failed: {e}")
        return ExtractResult(
            text="",
            kind="docx(failed)",
            meta={"error": str(e)},
            warnings=warnings,
            tables=[]
        )

def _extract_xlsx_enhanced(content: bytes, warnings: List[str]) -> ExtractResult:
    """Enhanced XLSX extraction with table support"""
    try:
        import openpyxl
        import pandas as pd
        import io
        
        # Load workbook
        workbook = openpyxl.load_workbook(BytesIO(content))
        
        text_parts = []
        tables = []
        
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]
            
            # Extract text from sheet
            sheet_text = []
            for row in sheet.iter_rows(values_only=True):
                row_text = [str(cell or '') for cell in row]
                if any(cell.strip() for cell in row_text):
                    sheet_text.append('\t'.join(row_text))
            
            if sheet_text:
                text_parts.append(f"Sheet: {sheet_name}\n" + '\n'.join(sheet_text))
            
            # Create table data
            if sheet_text:
                # Convert to DataFrame for CSV
                df = pd.read_excel(BytesIO(content), sheet_name=sheet_name)
                csv_data = df.to_csv(index=False)
                
                table = TableData(
                    name=sheet_name,
                    csv_data=csv_data,
                    rows=len(df),
                    cols=len(df.columns)
                )
                tables.append(table)
        
        text = '\n\n'.join(text_parts)
        
        return ExtractResult(
            text=text,
            kind="xlsx(enhanced)",
            meta={"sheets": len(workbook.sheetnames), "tables_found": len(tables)},
            warnings=warnings,
            tables=tables
        )
    except Exception as e:
        warnings.append(f"Enhanced XLSX extraction failed: {e}")
        return ExtractResult(
            text="",
            kind="xlsx(failed)",
            meta={"error": str(e)},
            warnings=warnings,
            tables=[]
        )

def _extract_doc_file(content: bytes, warnings: List[str]) -> ExtractResult:
    """Extract from legacy DOC files"""
    try:
        import mammoth
        
        result = mammoth.extract_raw_text(BytesIO(content))
        text = result.value
        
        return ExtractResult(
            text=text,
            kind="doc(mammoth)",
            meta={"method": "mammoth"},
            warnings=warnings,
            tables=[]
        )
    except Exception as e:
        warnings.append(f"DOC extraction failed: {e}")
        return ExtractResult(
            text="",
            kind="doc(failed)",
            meta={"error": str(e)},
            warnings=warnings,
            tables=[]
        )

def _clean_rtf(text: str) -> str:
    """Basic RTF cleaning"""
    import re
    
    # Remove RTF control codes
    text = re.sub(r'\\[a-z]+\d*\s?', '', text)
    text = re.sub(r'[{}]', '', text)
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

# ===== app/services/clients.py =====
from __future__ import annotations
import os, time, httpx
from typing import List, Dict, Any, Optional
from app.core.qdrant import get_qdrant
from app.core.metrics import external_request_total, external_request_seconds
from qdrant_client.http.models import Filter, FieldCondition, MatchValue

EMB_URL = os.getenv("EMBEDDINGS_URL", "http://emb:8001")
LLM_URL = os.getenv("LLM_URL", "http://llm:8002")
COLLECTION = os.getenv("QDRANT_COLLECTION", "rag_chunks")

def _timed(name: str):
    class _Ctx:
        def __enter__(self):
            self.t0 = time.perf_counter()
            return self
        def __exit__(self, exc_type, exc, tb):
            dt = time.perf_counter() - self.t0
            external_request_total.labels(target=name, status=("ok" if exc is None else "fail")).inc()
            external_request_seconds.labels(target=name).observe(dt)
    return _Ctx()

def embed_texts(texts: List[str], profile: str = "rt", models: Optional[List[str]] = None) -> List[List[float]]:
    """Эмбеддинг текстов через диспетчер или HTTP fallback"""
    try:
        # Пробуем использовать новый диспетчер
        from app.services.embedding_dispatcher import embed_texts_dispatcher
        return embed_texts_dispatcher(texts, profile, models)
    except Exception as e:
        print(f"Dispatcher failed, falling back to HTTP: {e}")
        # Fallback на старый HTTP API
        with _timed("emb"):
            with httpx.Client(timeout=60) as client:
                r = client.post(f"{EMB_URL}/embed", json={"inputs": texts})
                r.raise_for_status()
                return r.json().get("vectors", [])

def llm_chat(messages: List[Dict[str, str]], temperature: float = 0.2, max_tokens: Optional[int] = None) -> str:
    """Обычный чат с LLM (не стриминг)"""
    payload = {"messages": messages, "temperature": temperature}
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens
    with _timed("llm"):
        with httpx.Client(timeout=120) as client:
            r = client.post(f"{LLM_URL}/chat", json=payload)
            r.raise_for_status()
            
            # Проверяем, является ли ответ стримингом или обычным JSON
            content_type = r.headers.get("content-type", "")
            if "text/event-stream" in content_type:
                # Обрабатываем стриминг ответ
                content = ""
                for line in r.text.split('\n'):
                    if line.startswith('data: '):
                        data = line[6:]  # Убираем "data: "
                        if data.strip() == "[DONE]":
                            break
                        try:
                            chunk = r.json() if not line.startswith('data: ') else __import__('json').loads(data)
                            if "choices" in chunk and len(chunk["choices"]) > 0:
                                delta = chunk["choices"][0].get("delta", {})
                                chunk_content = delta.get("content", "")
                                if chunk_content:
                                    content += chunk_content
                        except:
                            continue
            else:
                # Обрабатываем обычный JSON ответ (заглушка)
                try:
                    data = r.json()
                    content = data.get("content", "")
                except:
                    content = r.text
                    
            return content

async def llm_chat_stream(messages: List[Dict[str, str]], temperature: float = 0.2, max_tokens: Optional[int] = None):
    """Настоящий стриминг чат с LLM"""
    payload = {"messages": messages, "temperature": temperature, "stream": True}
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens
    
    with _timed("llm"):
        async with httpx.AsyncClient(timeout=120) as client:
            async with client.stream("POST", f"{LLM_URL}/v1/chat/completions", json=payload) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.startswith('data: '):
                        data = line[6:]  # Убираем "data: "
                        if data.strip() == "[DONE]":
                            break
                        try:
                            chunk = __import__('json').loads(data)
                            if "choices" in chunk and len(chunk["choices"]) > 0:
                                delta = chunk["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    yield content
                        except:
                            continue

def qdrant_search(vector: List[float], top_k: int, offset: Optional[int] = None,
                  doc_id: Optional[str] = None, tags: Optional[List[str]] = None,
                  sort_by: str = "score_desc"):
    client = get_qdrant()
    must = []
    if doc_id:
        must.append(FieldCondition(key="document_id", match=MatchValue(value=doc_id)))
    if tags:
        must.append(FieldCondition(key="tags", match=MatchValue(value=tags)))
    f = Filter(must=must) if must else None
    with _timed("qdrant.search"):
        hits = client.search(collection_name=COLLECTION, query_vector=vector, limit=top_k, offset=offset or 0, with_payload=True, query_filter=f)
    out = []
    for h in hits:
        out.append({"score": float(h.score), "id": str(h.id), "payload": h.payload or {}})
    return out

def qdrant_count_by_doc(doc_id: str) -> int:
    client = get_qdrant()
    f = Filter(must=[FieldCondition(key="document_id", match=MatchValue(value=doc_id))])
    with _timed("qdrant.count"):
        try:
            res = client.count(collection_name=COLLECTION, count_filter=f, exact=True)
            return int(getattr(res, "count", None) or (res.get("count") if isinstance(res, dict) else 0) or 0)
        except Exception:
            total = 0
            next_page = None
            while True:
                points, next_page = client.scroll(
                    collection_name=COLLECTION,
                    scroll_filter=f,
                    limit=1024,
                    with_payload=False,
                    with_vectors=False,
                    offset=next_page,
                )
                total += len(points or [])
                if not next_page:
                    break
            return total

# ===== app/services/audit_service.py =====
from __future__ import annotations
from typing import Optional, Dict, Any
from datetime import datetime
from sqlalchemy.orm import Session
from fastapi import Request

from app.models.user import AuditLogs
from app.schemas.admin import AuditAction


class AuditService:
    def __init__(self, session: Session):
        self.session = session

    def log_action(
        self,
        action: str,
        actor_user_id: Optional[str] = None,
        object_type: Optional[str] = None,
        object_id: Optional[str] = None,
        meta: Optional[Dict[str, Any]] = None,
        request: Optional[Request] = None,
        request_id: Optional[str] = None,
    ) -> None:
        """Log an audit action."""
        audit_log = AuditLogs(
            actor_user_id=actor_user_id,
            action=action,
            object_type=object_type,
            object_id=object_id,
            meta=meta,
            request_id=request_id,
        )
        
        if request:
            # Extract IP and User-Agent from request
            forwarded_for = request.headers.get("X-Forwarded-For")
            if forwarded_for:
                audit_log.ip = forwarded_for.split(",")[0].strip()
            else:
                audit_log.ip = request.client.host if request.client else None
            
            audit_log.user_agent = request.headers.get("User-Agent")
        
        self.session.add(audit_log)
        self.session.commit()

    def log_user_action(
        self,
        action: AuditAction,
        target_user_id: str,
        actor_user_id: Optional[str] = None,
        request: Optional[Request] = None,
        request_id: Optional[str] = None,
        **meta
    ) -> None:
        """Log a user-related action."""
        self.log_action(
            action=action.value,
            actor_user_id=actor_user_id,
            object_type="user",
            object_id=target_user_id,
            meta=meta,
            request=request,
            request_id=request_id,
        )

    def log_token_action(
        self,
        action: AuditAction,
        token_id: str,
        user_id: str,
        actor_user_id: Optional[str] = None,
        request: Optional[Request] = None,
        request_id: Optional[str] = None,
        **meta
    ) -> None:
        """Log a token-related action."""
        self.log_action(
            action=action.value,
            actor_user_id=actor_user_id,
            object_type="token",
            object_id=token_id,
            meta={"user_id": user_id, **meta},
            request=request,
            request_id=request_id,
        )

    def log_auth_action(
        self,
        action: AuditAction,
        user_id: str,
        request: Optional[Request] = None,
        request_id: Optional[str] = None,
        **meta
    ) -> None:
        """Log an authentication-related action."""
        self.log_action(
            action=action.value,
            actor_user_id=user_id,
            object_type="auth",
            object_id=user_id,
            meta=meta,
            request=request,
            request_id=request_id,
        )

# ===== app/services/embedding_dispatcher.py =====
"""
Embedding Dispatcher - маршрутизатор задач эмбеддинга
Принимает логическую задачу и распределяет по моделям
"""
from __future__ import annotations
import asyncio
import time
import uuid
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from celery import Celery
from app.core.model_registry import get_model_registry, ModelConfig
from app.core.config import settings

@dataclass
class EmbeddingRequest:
    """Запрос на эмбеддинг"""
    request_id: str
    texts: List[str]
    profile: str  # "rt" | "bulk"
    models: Optional[List[str]] = None
    tenant_id: Optional[str] = None
    reply_to: Optional[str] = None
    correlation_id: Optional[str] = None
    idempotency_key: Optional[str] = None

@dataclass
class ModelResult:
    """Результат от одной модели"""
    model: str
    dim: int
    vectors: List[List[float]]
    warnings: List[str]
    duration_ms: int

@dataclass
class EmbeddingResponse:
    """Ответ диспетчера"""
    request_id: str
    model_results: List[ModelResult]
    errors: List[str]
    used_profile: str

class EmbeddingDispatcher:
    """Диспетчер эмбеддингов"""
    
    def __init__(self, celery_app: Celery):
        self.celery = celery_app
        self.registry = get_model_registry()
    
    def dispatch_embedding(self, request: EmbeddingRequest) -> EmbeddingResponse:
        """Основной метод диспетчера"""
        start_time = time.perf_counter()
        
        # Определяем модели для использования
        if request.models:
            target_models = request.models
        else:
            target_models = self.registry.get_default_models(request.profile)
        
        # Получаем готовые модели
        ready_models = []
        for alias in target_models:
            model = self.registry.get_model(alias)
            if model and model.health == "ready":
                ready_models.append(model)
        
        if not ready_models:
            return EmbeddingResponse(
                request_id=request.request_id,
                model_results=[],
                errors=[f"No ready models found for profile {request.profile}"],
                used_profile=request.profile
            )
        
        # Отправляем задачи в очереди моделей
        tasks = []
        for model in ready_models:
            queue_name = model.queues[request.profile]
            task = self._send_embedding_task(request, model, queue_name)
            tasks.append((model, task))
        
        # Собираем результаты
        model_results = []
        errors = []
        
        for model, task in tasks:
            try:
                result = self._wait_for_result(task, request.profile)
                if result:
                    model_results.append(result)
                else:
                    errors.append(f"Model {model.alias} failed to respond")
            except Exception as e:
                errors.append(f"Model {model.alias} error: {str(e)}")
        
        return EmbeddingResponse(
            request_id=request.request_id,
            model_results=model_results,
            errors=errors,
            used_profile=request.profile
        )
    
    def _send_embedding_task(self, request: EmbeddingRequest, model: ModelConfig, queue_name: str) -> str:
        """Отправляет задачу в очередь модели"""
        task_payload = {
            "request_id": request.request_id,
            "texts": request.texts,
            "profile": request.profile,
            "model_alias": model.alias,
            "tenant_id": request.tenant_id,
            "reply_to": request.reply_to or f"embed.dispatch.{request.request_id}",
            "correlation_id": request.correlation_id,
            "idempotency_key": request.idempotency_key
        }
        
        # Отправляем задачу в очередь модели
        task = self.celery.send_task(
            "embedding_worker.process_embedding",
            args=[task_payload],
            queue=queue_name
        )
        
        return task.id
    
    def _wait_for_result(self, task_id: str, profile: str) -> Optional[ModelResult]:
        """Ждет результат от задачи"""
        timeout = 600 if profile == "bulk" else 800  # мс
        
        try:
            result = self.celery.AsyncResult(task_id)
            # Ждем результат с таймаутом
            response = result.get(timeout=timeout / 1000.0)
            
            if response and "error" not in response:
                return ModelResult(
                    model=response.get("model_alias", "unknown"),
                    dim=response.get("dim", 0),
                    vectors=response.get("vectors", []),
                    warnings=response.get("warnings", []),
                    duration_ms=response.get("duration_ms", 0)
                )
            else:
                return None
                
        except Exception as e:
            print(f"Task {task_id} failed: {e}")
            return None

# Celery задачи для диспетчера
def create_dispatcher_tasks(celery_app: Celery):
    """Создает Celery задачи для диспетчера"""
    
    @celery_app.task(name="embedding_dispatcher.dispatch")
    def dispatch_embedding_task(request_data: Dict[str, Any]) -> Dict[str, Any]:
        """Celery задача для диспетчера"""
        request = EmbeddingRequest(**request_data)
        dispatcher = EmbeddingDispatcher(celery_app)
        response = dispatcher.dispatch_embedding(request)
        
        # Конвертируем в словарь для сериализации
        return {
            "request_id": response.request_id,
            "model_results": [
                {
                    "model": mr.model,
                    "dim": mr.dim,
                    "vectors": mr.vectors,
                    "warnings": mr.warnings,
                    "duration_ms": mr.duration_ms
                }
                for mr in response.model_results
            ],
            "errors": response.errors,
            "used_profile": response.used_profile
        }
    
    return dispatch_embedding_task

# Удобная функция для использования в API
def embed_texts_dispatcher(texts: List[str], profile: str = "rt", models: Optional[List[str]] = None) -> List[List[float]]:
    """Упрощенный интерфейс для эмбеддинга через диспетчер"""
    from app.celery_app import celery_app
    
    request = EmbeddingRequest(
        request_id=str(uuid.uuid4()),
        texts=texts,
        profile=profile,
        models=models
    )
    
    dispatcher = EmbeddingDispatcher(celery_app)
    response = dispatcher.dispatch_embedding(request)
    
    # Возвращаем векторы от первой успешной модели
    if response.model_results:
        return response.model_results[0].vectors
    else:
        raise Exception(f"Embedding failed: {response.errors}")

# ===== app/services/adaptive_chunker.py =====
from __future__ import annotations

import re
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass

@dataclass
class Chunk:
    text: str
    chunk_idx: int
    metadata: Dict[str, Any]
    is_header: bool = False
    is_table: bool = False
    parent_section: str = ""

class AdaptiveChunker:
    """
    Adaptive text chunking that preserves structure and context
    """
    
    def __init__(self, max_chars: int = 1200, overlap: int = 100):
        self.max_chars = max_chars
        self.overlap = overlap
        
        # Patterns for structure detection
        self.header_patterns = [
            r'^#{1,6}\s+.+',  # Markdown headers
            r'^\d+\.\s+.+',   # Numbered lists
            r'^[A-Z][A-Z\s]+$',  # ALL CAPS headers
            r'^[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*:$',  # Title Case headers
        ]
        
        self.section_patterns = [
            r'^(?:Chapter|Section|Part)\s+\d+',  # Chapter markers
            r'^\d+\.\d+',  # Subsection numbers
            r'^[IVX]+\.',  # Roman numerals
        ]
        
        self.table_indicators = [
            r'^\s*\|.*\|',  # Markdown tables
            r'^\s*\w+\s+\w+\s+\w+',  # Space-separated columns
            r'^\s*\w+,\s*\w+',  # CSV-like data
        ]

    def chunk_text(self, text: str, document_meta: Dict[str, Any] = None) -> List[Chunk]:
        """
        Chunk text while preserving structure
        """
        if not text.strip():
            return []
        
        # Detect document structure
        structure = self._analyze_structure(text)
        
        # Create chunks based on structure
        chunks = []
        current_chunk = ""
        current_metadata = {}
        chunk_idx = 0
        
        lines = text.split('\n')
        i = 0
        
        while i < len(lines):
            line = lines[i].strip()
            
            # Check if this is a structural element
            if self._is_header(line):
                # Save current chunk if it exists
                if current_chunk.strip():
                    chunks.append(Chunk(
                        text=current_chunk.strip(),
                        chunk_idx=chunk_idx,
                        metadata=current_metadata.copy(),
                        is_header=False,
                        is_table=self._is_table(current_chunk),
                        parent_section=current_metadata.get('section', '')
                    ))
                    chunk_idx += 1
                    current_chunk = ""
                
                # Start new chunk with header
                header_chunk = self._create_header_chunk(line, i, lines, chunk_idx)
                chunks.append(header_chunk)
                chunk_idx += 1
                
                # Set metadata for following chunks
                current_metadata = {
                    'section': line,
                    'header_line': i,
                    'document_meta': document_meta or {}
                }
                
            elif self._is_table_start(line):
                # Handle table as separate chunk
                table_chunk = self._extract_table_chunk(lines, i, chunk_idx, current_metadata)
                if table_chunk:
                    chunks.append(table_chunk)
                    chunk_idx += 1
                    i = table_chunk.metadata.get('end_line', i)
            
            else:
                # Regular text line
                if len(current_chunk + line) > self.max_chars:
                    # Save current chunk
                    if current_chunk.strip():
                        chunks.append(Chunk(
                            text=current_chunk.strip(),
                            chunk_idx=chunk_idx,
                            metadata=current_metadata.copy(),
                            is_header=False,
                            is_table=self._is_table(current_chunk),
                            parent_section=current_metadata.get('section', '')
                        ))
                        chunk_idx += 1
                    
                    # Start new chunk with overlap
                    current_chunk = self._create_overlap(current_chunk) + line
                else:
                    current_chunk += '\n' + line if current_chunk else line
            
            i += 1
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append(Chunk(
                text=current_chunk.strip(),
                chunk_idx=chunk_idx,
                metadata=current_metadata.copy(),
                is_header=False,
                is_table=self._is_table(current_chunk),
                parent_section=current_metadata.get('section', '')
            ))
        
        # Apply overlap between chunks
        return self._apply_overlap(chunks)

    def _analyze_structure(self, text: str) -> Dict[str, Any]:
        """Analyze document structure"""
        lines = text.split('\n')
        
        structure = {
            'headers': [],
            'sections': [],
            'tables': [],
            'lists': []
        }
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            if self._is_header(line):
                structure['headers'].append({'line': i, 'text': line})
            elif self._is_section(line):
                structure['sections'].append({'line': i, 'text': line})
            elif self._is_table_start(line):
                structure['tables'].append({'line': i, 'text': line})
        
        return structure

    def _is_header(self, line: str) -> bool:
        """Check if line is a header"""
        for pattern in self.header_patterns:
            if re.match(pattern, line):
                return True
        return False

    def _is_section(self, line: str) -> bool:
        """Check if line is a section marker"""
        for pattern in self.section_patterns:
            if re.match(pattern, line):
                return True
        return False

    def _is_table_start(self, line: str) -> bool:
        """Check if line starts a table"""
        for pattern in self.table_indicators:
            if re.match(pattern, line):
                return True
        return False

    def _is_table(self, text: str) -> bool:
        """Check if text contains table data"""
        lines = text.split('\n')
        table_lines = 0
        
        for line in lines:
            if self._is_table_start(line):
                table_lines += 1
        
        return table_lines >= 2  # At least 2 table lines

    def _create_header_chunk(self, header: str, line_idx: int, lines: List[str], chunk_idx: int) -> Chunk:
        """Create a chunk for a header with some following context"""
        # Include a few lines after header for context
        context_lines = []
        for i in range(line_idx + 1, min(line_idx + 3, len(lines))):
            if lines[i].strip():
                context_lines.append(lines[i])
                break
        
        text = header
        if context_lines:
            text += '\n' + '\n'.join(context_lines)
        
        return Chunk(
            text=text,
            chunk_idx=chunk_idx,
            metadata={
                'is_header': True,
                'header_line': line_idx,
                'header_text': header
            },
            is_header=True,
            is_table=False,
            parent_section=""
        )

    def _extract_table_chunk(self, lines: List[str], start_idx: int, chunk_idx: int, metadata: Dict) -> Chunk:
        """Extract a complete table as a chunk"""
        table_lines = []
        i = start_idx
        
        # Collect table lines
        while i < len(lines):
            line = lines[i].strip()
            if self._is_table_start(line) or (table_lines and line and not line.startswith(' ')):
                table_lines.append(line)
            elif not line and table_lines:
                # Empty line might end table
                break
            elif not table_lines:
                # No table started yet
                break
            else:
                table_lines.append(line)
            i += 1
        
        if len(table_lines) >= 2:
            return Chunk(
                text='\n'.join(table_lines),
                chunk_idx=chunk_idx,
                metadata={
                    **metadata,
                    'is_table': True,
                    'table_lines': len(table_lines),
                    'start_line': start_idx,
                    'end_line': i
                },
                is_header=False,
                is_table=True,
                parent_section=metadata.get('section', '')
            )
        
        return None

    def _create_overlap(self, text: str) -> str:
        """Create overlap text from the end of current chunk"""
        if not text:
            return ""
        
        # Take last few sentences or words
        sentences = re.split(r'[.!?]+', text)
        if len(sentences) > 1:
            # Take last sentence
            return sentences[-2].strip() + " "
        else:
            # Take last few words
            words = text.split()
            if len(words) > 10:
                return ' '.join(words[-10:]) + " "
            return text[-self.overlap:] + " " if len(text) > self.overlap else text

    def _apply_overlap(self, chunks: List[Chunk]) -> List[Chunk]:
        """Apply overlap between chunks"""
        if len(chunks) <= 1:
            return chunks
        
        result = []
        
        for i, chunk in enumerate(chunks):
            if i > 0:
                # Add overlap from previous chunk
                prev_chunk = result[-1]
                overlap_text = self._create_overlap(prev_chunk.text)
                
                if overlap_text:
                    chunk.text = overlap_text + chunk.text
                    chunk.metadata['has_overlap'] = True
                    chunk.metadata['overlap_from'] = i - 1
            
            result.append(chunk)
        
        return result

def chunk_text_adaptive(text: str, max_chars: int = 1200, overlap: int = 100, document_meta: Dict[str, Any] = None) -> List[Chunk]:
    """
    Convenience function for adaptive chunking
    """
    chunker = AdaptiveChunker(max_chars=max_chars, overlap=overlap)
    return chunker.chunk_text(text, document_meta)

# ===== app/api/routers/auth.py =====
from __future__ import annotations
from fastapi import APIRouter, Depends, HTTPException, status, Request, Response
from sqlalchemy.orm import Session
import hashlib

from app.api.deps import db_session, rate_limit, get_current_user
from app.core.security import get_bearer_token
from app.services.auth_service import login as do_login, refresh as do_refresh, revoke_refresh as do_revoke
from app.repositories.users_repo import UsersRepo

router = APIRouter(prefix="/auth", tags=["auth"])

@router.post("/login")
async def login(request: Request, payload: dict, session: Session = Depends(db_session)):
    # payload: {"login":"...", "password":"..."}
    login_ = (payload or {}).get("login", "").strip()
    password = (payload or {}).get("password", "")
    
    # Rate limit by IP + login
    await rate_limit(request, "auth_login", limit=10, window_sec=60, login=login_)
    if not login_ or not password:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="missing_credentials")
    try:
        access, refresh, user_id = do_login(session, login_, password)
    except ValueError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="invalid_credentials")
    # enrich with user for convenience (per OpenAPI)
    u = UsersRepo(session).get(user_id)
    return {
        "access_token": access,
        "refresh_token": refresh,
        "token_type": "bearer",
        "expires_in": 3600,
        "user": {"id": str(u.id), "fio": getattr(u, "fio", None), "login": u.login, "role": u.role} if u else None,
    }

@router.post("/refresh")
def refresh(payload: dict, session: Session = Depends(db_session)):
    rt = (payload or {}).get("refresh_token")
    if not rt:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="missing_refresh_token")
    try:
        access, maybe_new_refresh = do_refresh(session, rt)
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=str(e))
    return {
        "access_token": access,
        "refresh_token": maybe_new_refresh,
        "token_type": "bearer",
        "expires_in": 3600,
    }

@router.get("/me")
def me(user = Depends(get_current_user)):
    return user

@router.post("/logout", status_code=204)
def logout(payload: dict | None = None, session: Session = Depends(db_session)) -> Response:
    # Best-effort: revoke provided refresh token; if absent, just return 204 (contract allows empty body).
    rt = (payload or {}).get("refresh_token") if isinstance(payload, dict) else None
    if rt:
        do_revoke(session, rt)
    return Response(status_code=204)

# ===== app/api/routers/password_reset.py =====
from __future__ import annotations
from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, status, Request
from sqlalchemy.orm import Session
import secrets
import hashlib
from datetime import datetime, timedelta

from app.api.deps import db_session, get_request_id, get_client_ip, get_user_agent, rate_limit
from app.core.security import hash_password, validate_password_strength
from app.repositories.users_repo import UsersRepo
from app.services.audit_service import AuditService
from app.schemas.admin import PasswordResetRequest, PasswordResetConfirm, ErrorResponse, AuditAction

router = APIRouter(prefix="/auth", tags=["password-reset"])


def create_error_response(code: str, message: str, request_id: str, 
                         details: Optional[dict] = None) -> ErrorResponse:
    """Create standardized error response."""
    return ErrorResponse(
        code=code,
        message=message,
        request_id=request_id,
        details=details
    )


@router.post("/password/forgot", status_code=status.HTTP_200_OK)
async def forgot_password(
    request_data: PasswordResetRequest,
    session: Session = Depends(db_session),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Request password reset. Always returns 200 for security."""
    # Rate limiting for password reset requests
    await rate_limit(request, "password_reset", limit=5, window_sec=300)  # 5 attempts per 5 minutes
    
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    # Find user by login or email
    user = repo.by_login(request_data.login_or_email)
    if not user and "@" in request_data.login_or_email:
        # Try to find by email
        users = repo.s.execute(
            repo.s.query(repo.Users).filter(repo.Users.email == request_data.login_or_email)
        ).scalars().all()
        if users:
            user = users[0]
    
    if not user or not user.is_active:
        # Always return 200 for security (don't reveal if user exists)
        return {"message": "If the account exists, a password reset link has been sent"}
    
    # Generate reset token
    token_plain = secrets.token_urlsafe(32)
    token_hash = hashlib.sha256(token_plain.encode()).hexdigest()
    expires_at = datetime.utcnow() + timedelta(minutes=60)  # 1 hour expiry
    
    # Create reset token
    repo.create_password_reset_token(
        user_id=str(user.id),
        token_hash=token_hash,
        expires_at=expires_at
    )
    
    # Log audit action
    audit.log_auth_action(
        action=AuditAction.PASSWORD_RESET,
        user_id=str(user.id),
        request=request,
        request_id=request_id,
        method="email_request"
    )
    
    # TODO: Send email with reset link
    # For now, we'll just return the token in development
    # In production, this should be sent via email
    if not user.email:
        return {
            "message": "Password reset requested. Contact administrator for reset token.",
            "token": token_plain  # Only for development
        }
    
    # TODO: Implement email sending
    # send_password_reset_email(user.email, token_plain)
    
    return {"message": "If the account exists, a password reset link has been sent"}


@router.post("/password/reset", status_code=status.HTTP_200_OK)
async def reset_password(
    request_data: PasswordResetConfirm,
    session: Session = Depends(db_session),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Reset password using token."""
    # Rate limiting for password reset attempts
    await rate_limit(request, "password_reset_confirm", limit=10, window_sec=300)  # 10 attempts per 5 minutes
    
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    # Hash the provided token
    token_hash = hashlib.sha256(request_data.token.encode()).hexdigest()
    
    # Find valid reset token
    reset_token = repo.get_password_reset_token(token_hash)
    if not reset_token:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=create_error_response(
                "invalid_token",
                "Invalid or expired reset token",
                request_id
            ).dict()
        )
    
    # Validate new password
    is_valid, error_msg = validate_password_strength(request_data.new_password)
    if not is_valid:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=create_error_response(
                "invalid_password",
                f"Password validation failed: {error_msg}",
                request_id
            ).dict()
        )
    
    # Update password
    password_hash = hash_password(request_data.new_password)
    repo.update_user(str(reset_token.user_id), password_hash=password_hash)
    
    # Mark token as used
    repo.use_password_reset_token(token_hash)
    
    # Revoke all refresh tokens for security
    user = repo.get(str(reset_token.user_id))
    if user:
        for token in user.refresh_tokens:
            if not token.revoked:
                token.revoked = True
        session.commit()
    
    # Log audit action
    audit.log_auth_action(
        action=AuditAction.PASSWORD_RESET,
        user_id=str(reset_token.user_id),
        request=request,
        request_id=request_id,
        method="email_reset"
    )
    
    return {"message": "Password reset successfully"}

# ===== app/api/routers/__init__.py =====

# ===== app/api/routers/admin.py =====
from __future__ import annotations
from typing import Optional, List
from fastapi import APIRouter, Depends, HTTPException, status, Request, Response
from sqlalchemy.orm import Session
import secrets
import hashlib
from datetime import datetime, timedelta

from app.api.deps import (
    db_session, require_admin, get_request_id, get_client_ip, get_user_agent
)
from app.core.security import hash_password, verify_password, validate_password_strength
from app.core.pat_validation import validate_scopes, check_scope_permission
from app.repositories.users_repo import UsersRepo
from app.services.audit_service import AuditService
from app.schemas.admin import (
    UserCreate, UserUpdate, UserResponse, UserListResponse,
    PasswordResetRequest, PasswordResetConfirm, PasswordChange,
    TokenCreate, TokenResponse, TokenListResponse,
    AuditLogResponse, AuditLogListResponse,
    ErrorResponse, UserRole, AuditAction
)

router = APIRouter(prefix="/api/admin", tags=["admin"])


def create_error_response(code: str, message: str, request_id: str, 
                         details: Optional[dict] = None) -> dict:
    """Create standardized error response."""
    return {
        "error": {
            "code": code,
            "message": message,
            "details": details or {}
        },
        "request_id": request_id
    }


# User Management Endpoints
@router.get("/users", response_model=UserListResponse)
def list_users(
    query: Optional[str] = None,
    role: Optional[str] = None,
    is_active: Optional[bool] = None,
    limit: int = 50,
    cursor: Optional[str] = None,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """List users with pagination and filters."""
    if limit > 100:
        limit = 100
    
    repo = UsersRepo(session)
    users, has_more, next_cursor = repo.list_users(
        query=query, role=role, is_active=is_active, 
        limit=limit, cursor=cursor
    )
    
    total = repo.count_users(query=query, role=role, is_active=is_active)
    
    return UserListResponse(
        users=[UserResponse.from_orm(user) for user in users],
        total=total,
        has_more=has_more,
        next_cursor=next_cursor
    )


@router.post("/users", response_model=UserResponse, status_code=status.HTTP_201_CREATED)
def create_user(
    user_data: UserCreate,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Create a new user."""
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    # Check if user already exists
    existing_user = repo.by_login(user_data.login)
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=create_error_response(
                "user_exists", 
                f"User with login '{user_data.login}' already exists",
                request_id
            ).dict()
        )
    
    # Generate password if not provided
    password = user_data.password
    if not password:
        password = secrets.token_urlsafe(16)
    else:
        # Validate password strength if provided
        is_valid, error_msg = validate_password_strength(password)
        if not is_valid:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=create_error_response(
                    "invalid_password",
                    f"Password validation failed: {error_msg}",
                    request_id
                ).dict()
            )
    
    password_hash = hash_password(password)
    
    # Create user
    user = repo.create_user(
        login=user_data.login,
        password_hash=password_hash,
        role=user_data.role.value,
        email=user_data.email,
        is_active=user_data.is_active
    )
    
    # Log audit action
    audit.log_user_action(
        action=AuditAction.USER_CREATED,
        target_user_id=str(user.id),
        actor_user_id=current_user["id"],
        request=request,
        request_id=request_id,
        role=user_data.role.value,
        email=user_data.email
    )
    
    response = UserResponse.from_orm(user)
    
    # Include generated password in response only if EMAIL_ENABLED=false
    if not user_data.password and not settings.EMAIL_ENABLED:
        response_dict = response.dict()
        response_dict["generated_password"] = password
        return response_dict
    
    return response


@router.get("/users/{user_id}", response_model=UserResponse)
def get_user(
    user_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Get user by ID."""
    repo = UsersRepo(session)
    user = repo.get(user_id)
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                "user_not_found",
                f"User with ID '{user_id}' not found",
                request_id
            ).dict()
        )
    
    return UserResponse.from_orm(user)


@router.patch("/users/{user_id}", response_model=UserResponse)
def update_user(
    user_id: str,
    user_data: UserUpdate,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Update user."""
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    user = repo.get(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                "user_not_found",
                f"User with ID '{user_id}' not found",
                request_id
            ).dict()
        )
    
    # Prepare updates
    updates = {}
    if user_data.role is not None:
        updates["role"] = user_data.role.value
    if user_data.email is not None:
        updates["email"] = user_data.email
    if user_data.is_active is not None:
        updates["is_active"] = user_data.is_active
    if user_data.require_password_change is not None:
        updates["require_password_change"] = user_data.require_password_change
    
    # Update user
    updated_user = repo.update_user(user_id, **updates)
    
    # Log audit action
    audit.log_user_action(
        action=AuditAction.USER_UPDATED,
        target_user_id=user_id,
        actor_user_id=current_user["id"],
        request=request,
        request_id=request_id,
        changes=updates
    )
    
    return UserResponse.from_orm(updated_user)


@router.post("/users/{user_id}/password", response_model=dict)
def reset_user_password(
    user_id: str,
    password_data: PasswordChange,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Reset user password."""
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    user = repo.get(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                "user_not_found",
                f"User with ID '{user_id}' not found",
                request_id
            ).dict()
        )
    
    # Generate new password if not provided
    new_password = password_data.new_password
    if not new_password:
        new_password = secrets.token_urlsafe(16)
    
    password_hash = hash_password(new_password)
    
    # Update password
    repo.update_user(user_id, password_hash=password_hash)
    
    # Revoke all refresh tokens
    for token in user.refresh_tokens:
        if not token.revoked:
            token.revoked = True
    
    session.commit()
    
    # Log audit action
    audit.log_user_action(
        action=AuditAction.PASSWORD_RESET,
        target_user_id=user_id,
        actor_user_id=current_user["id"],
        request=request,
        request_id=request_id
    )
    
    response = {"message": "Password reset successfully"}
    
    # Include new password only if EMAIL_ENABLED=false
    if not settings.EMAIL_ENABLED:
        response["new_password"] = new_password
    
    return response


@router.delete("/users/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
def delete_user(
    user_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Soft delete user (deactivate)."""
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    user = repo.get(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                "user_not_found",
                f"User with ID '{user_id}' not found",
                request_id
            ).dict()
        )
    
    # Soft delete (deactivate)
    repo.update_user(user_id, is_active=False)
    
    # Log audit action
    audit.log_user_action(
        action=AuditAction.USER_DEACTIVATED,
        target_user_id=user_id,
        actor_user_id=current_user["id"],
        request=request,
        request_id=request_id
    )
    
    return Response(status_code=status.HTTP_204_NO_CONTENT)


# PAT Token Management Endpoints
@router.get("/users/{user_id}/tokens", response_model=TokenListResponse)
def list_user_tokens(
    user_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """List user's PAT tokens."""
    repo = UsersRepo(session)
    
    user = repo.get(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                "user_not_found",
                f"User with ID '{user_id}' not found",
                request_id
            ).dict()
        )
    
    tokens = repo.get_user_tokens(user_id)
    total = len(tokens)
    
    return TokenListResponse(
        tokens=[TokenResponse.from_orm(token) for token in tokens],
        total=total
    )


@router.post("/users/{user_id}/tokens", response_model=TokenResponse, status_code=status.HTTP_201_CREATED)
def create_user_token(
    user_id: str,
    token_data: TokenCreate,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Create a new PAT token for user."""
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    user = repo.get(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                "user_not_found",
                f"User with ID '{user_id}' not found",
                request_id
            ).dict()
        )
    
    # Validate scopes
    scopes = [scope.value for scope in token_data.scopes] if token_data.scopes else []
    try:
        validated_scopes = validate_scopes(scopes)
    except HTTPException as e:
        raise e
    
    # Generate token
    token_plain = secrets.token_urlsafe(32)
    token_hash = hashlib.sha256(token_plain.encode()).hexdigest()
    
    # Create token
    token = repo.create_token(
        user_id=user_id,
        token_hash=token_hash,
        name=token_data.name,
        scopes=validated_scopes,
        expires_at=token_data.expires_at
    )
    
    # Log audit action
    audit.log_token_action(
        action=AuditAction.TOKEN_CREATED,
        token_id=str(token.id),
        user_id=user_id,
        actor_user_id=current_user["id"],
        request=request,
        request_id=request_id,
        name=token_data.name
    )
    
    response = TokenResponse.from_orm(token)
    response.token_plain_once = token_plain
    
    return response


@router.delete("/tokens/{token_id}", status_code=status.HTTP_204_NO_CONTENT)
def revoke_token(
    token_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """Revoke a PAT token."""
    repo = UsersRepo(session)
    audit = AuditService(session)
    
    token = repo.s.get(repo.UserTokens, token_id)
    if not token:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                "token_not_found",
                f"Token with ID '{token_id}' not found",
                request_id
            ).dict()
        )
    
    # Revoke token
    success = repo.revoke_token(token_id)
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=create_error_response(
                "token_already_revoked",
                "Token is already revoked",
                request_id
            ).dict()
        )
    
    # Log audit action
    audit.log_token_action(
        action=AuditAction.TOKEN_REVOKED,
        token_id=token_id,
        user_id=str(token.user_id),
        actor_user_id=current_user["id"],
        request=request,
        request_id=request_id
    )
    
    return Response(status_code=status.HTTP_204_NO_CONTENT)


# Audit Logs Endpoints
@router.get("/audit-logs", response_model=AuditLogListResponse)
def list_audit_logs(
    actor_user_id: Optional[str] = None,
    action: Optional[str] = None,
    object_type: Optional[str] = None,
    limit: int = 50,
    cursor: Optional[str] = None,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_admin),
    request: Request = None,
    request_id: str = Depends(get_request_id)
):
    """List audit logs with pagination and filters."""
    if limit > 100:
        limit = 100
    
    repo = UsersRepo(session)
    logs, has_more, next_cursor = repo.get_audit_logs(
        actor_user_id=actor_user_id,
        action=action,
        object_type=object_type,
        limit=limit,
        cursor=cursor
    )
    
    total = repo.count_audit_logs(
        actor_user_id=actor_user_id,
        action=action,
        object_type=object_type
    )
    
    return AuditLogListResponse(
        logs=[AuditLogResponse.from_orm(log) for log in logs],
        total=total,
        has_more=has_more,
        next_cursor=next_cursor
    )

# ===== app/api/routers/rag.py =====
# app/api/routers/rag.py
"""
RAG роутер по best‑practice:
- Принимаем multipart файл
- Создаём UUID-документ (ORM/Repo)
- Кладём файл в MinIO: rag/{uuid}/origin.{ext}
- Сохраняем метаинфо в БД (оригинальное имя, mime, путь до файла без префикса бакета)
- Триггерим пайплайн (по желанию)
- Скачивание: presigned GET с оригинальным именем
"""
from __future__ import annotations
from typing import Optional
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Query, Body, Form
from sqlalchemy.orm import Session

from app.api.deps import db_session, require_roles, get_current_user
from app.schemas.admin import UserRole

# RBAC helpers
def require_editor_or_admin():
    """Require editor or admin role for write operations."""
    return require_roles(UserRole.EDITOR, UserRole.ADMIN)

def require_reader_or_above():
    """Require reader role or above for read operations."""
    return require_roles(UserRole.READER, UserRole.EDITOR, UserRole.ADMIN)
from app.core.config import settings
from app.core.s3_helpers import put_object, presign_get
from app.repositories.rag_repo import RagRepo
from app.services.rag_service import progress, stats, search, reprocess_document
from app.models.rag import RagDocuments

router = APIRouter(prefix="/rag", tags=["rag"])

ALLOWED_EXTENSIONS = {'.txt', '.pdf', '.doc', '.docx', '.md', '.rtf', '.odt'}

def _safe_ext(filename: Optional[str]) -> str:
    if not filename or '.' not in filename:
        return ''
    ext = '.' + filename.rsplit('.', 1)[-1].lower()
    return ext if ext in ALLOWED_EXTENSIONS else ''

@router.post("/upload")
async def upload_rag_file(
    file: UploadFile = File(...),
    tags: str = Form("[]"),  # JSON string of tags
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_editor_or_admin()),
):
    # Валидация размера файла (50MB)
    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    if hasattr(file, 'size') and file.size and file.size > MAX_FILE_SIZE:
        raise HTTPException(status_code=413, detail="File too large. Maximum size is 50MB")
    
    # Валидация типа файла
    repo = RagRepo(session)
    ext = _safe_ext(file.filename)
    if file.filename and not ext:
        raise HTTPException(status_code=400, detail=f"Unsupported file type. Allowed: {', '.join(sorted(ALLOWED_EXTENSIONS))}")
    
    # Валидация MIME типа
    if file.content_type and not file.content_type.startswith(('text/', 'application/pdf', 'application/msword', 'application/vnd.openxmlformats-officedocument')):
        raise HTTPException(status_code=400, detail="Unsupported MIME type")
    
    # Парсим теги
    try:
        import json
        parsed_tags = json.loads(tags) if tags else []
        if not isinstance(parsed_tags, list):
            parsed_tags = []
    except (json.JSONDecodeError, TypeError):
        parsed_tags = []
    
    try:
        # 1) создаём документ (UUID генерится в базе)
        doc = repo.create_document(
            name=file.filename,
            uploaded_by=None,
            status="uploaded",
            source_mime=file.content_type,
            tags=parsed_tags,
        )
        # 2) ключ origin.{ext} под UUID
        key = f"{doc.id}/origin{ext}"
        # 3) сохраняем в MinIO
        put_object(settings.S3_BUCKET_RAG, key, file.file, content_type=file.content_type)
        # 4) метаданные в БД
        doc.url_file = key
        doc.updated_at = datetime.utcnow()
        session.commit()
        # 5) триггерим пайплайн с задержкой (файл должен быть полностью загружен)
        try:
            from app.tasks.upload_watch import watch as upload_watch
            # Запускаем с задержкой 5 секунд, чтобы файл успел загрузиться
            upload_watch.apply_async(args=[str(doc.id)], kwargs={'key': key}, countdown=5)
        except Exception as e:
            # Логируем ошибку, но не прерываем процесс
            print(f"Warning: Failed to trigger upload watch: {e}")
        return {"id": str(doc.id), "key": key, "status": "uploaded", "tags": parsed_tags}
    except Exception as e:
        session.rollback()
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@router.get("/")
def list_rag_documents(
    page: int = Query(1, ge=1, description="Page number"),
    size: int = Query(20, ge=1, le=100, description="Page size"),
    status: Optional[str] = Query(None, description="Filter by status"),
    search: Optional[str] = Query(None, description="Search in document names"),
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_reader_or_above()),
):
    repo = RagRepo(session)
    
    # Применяем фильтры
    query = session.query(RagDocuments)
    
    if status:
        query = query.filter(RagDocuments.status == status)
    
    if search:
        query = query.filter(RagDocuments.name.ilike(f"%{search}%"))
    
    # Подсчитываем общее количество
    total = query.count()
    
    # Применяем пагинацию
    offset = (page - 1) * size
    docs = query.offset(offset).limit(size).all()
    
    # Вычисляем метаданные пагинации
    total_pages = (total + size - 1) // size
    has_next = page < total_pages
    has_prev = page > 1
    
    return {
        "items": [{"id": str(doc.id), "name": doc.name, "status": doc.status, "created_at": doc.date_upload.isoformat() if doc.date_upload else None, "url_file": doc.url_file, "url_canonical_file": doc.url_canonical_file, "tags": doc.tags, "progress": None, "updated_at": doc.updated_at.isoformat() if doc.updated_at else None} for doc in docs],
        "pagination": {
            "page": page,
            "size": size,
            "total": total,
            "total_pages": total_pages,
            "has_next": has_next,
            "has_prev": has_prev
        }
    }

@router.get("/{doc_id}")
def get_rag_document(
    doc_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_reader_or_above()),
):
    doc = RagRepo(session).get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="not_found")
    return {"id": str(doc.id), "name": doc.name, "status": doc.status, "date_upload": doc.date_upload, "url_file": doc.url_file, "url_canonical_file": doc.url_canonical_file, "tags": doc.tags, "progress": None, "updated_at": doc.updated_at}

@router.get("/{doc_id}/download")
def download_rag_file(
    doc_id: str,
    kind: str = Query("original", regex="^(original|canonical)$"),
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_reader_or_above()),
):
    repo = RagRepo(session)
    doc = repo.get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="not_found")

    bucket = settings.S3_BUCKET_RAG
    if kind == "original":
        key = getattr(doc, "url_file", None)
        if not key:
            raise HTTPException(status_code=404, detail="original_not_ready")
        download_name = doc.name or "document"
        mime = getattr(doc, "source_mime", None)
    else:
        # canonical лежит рядом: rag/{uuid}/canonical.txt
        key = getattr(doc, "url_canonical_file", None) or f"{doc.id}/canonical.txt"
        base = (doc.name or "document").rsplit('.', 1)[0]
        download_name = f"{base}.txt"
        mime = "text/plain"

    url = presign_get(bucket, key, download_name=download_name, mime=mime)
    return {"url": url}

@router.get("/{doc_id}/progress")
def get_rag_progress(
    doc_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_reader_or_above()),
):
    """Получить прогресс обработки RAG документа"""
    return progress(session, doc_id)


@router.get("/stats")
def get_rag_stats(
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_reader_or_above()),
):
    """Получить статистику RAG документов"""
    return stats(session)

@router.post("/{doc_id}/archive")
def archive_rag_document(
    doc_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_editor_or_admin()),
):
    doc = RagRepo(session).get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="not_found")
    
    doc.status = "archived"
    doc.updated_at = datetime.utcnow()
    session.commit()
    
    return {"id": str(doc.id), "status": doc.status}

@router.put("/{doc_id}/tags")
def update_rag_document_tags(
    doc_id: str,
    tags: list = Body(...),
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_editor_or_admin()),
):
    doc = RagRepo(session).get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    
    doc.tags = tags
    doc.updated_at = datetime.utcnow()
    session.commit()
    
    return {"id": str(doc.id), "tags": tags}

@router.delete("/{doc_id}")
def delete_rag_document(
    doc_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_editor_or_admin()),
):
    doc = RagRepo(session).get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="not_found")
    
    # Удаляем файлы из MinIO
    if doc.url_file:
        try:
            from app.core.s3 import get_minio
            client = get_minio()
            client.remove_object(settings.S3_BUCKET_RAG, doc.url_file)
        except Exception:
            pass
    
    if doc.url_canonical_file:
        try:
            from app.core.s3 import get_minio
            client = get_minio()
            client.remove_object(settings.S3_BUCKET_RAG, doc.url_canonical_file)
        except Exception:
            pass
    
    # Удаляем из БД
    session.delete(doc)
    session.commit()
    
    return {"id": str(doc.id), "deleted": True}

@router.post("/search")
def search_rag(
    query: str = Body(..., description="Search query"),
    top_k: int = Body(10, ge=1, le=100),
    min_score: float = Body(0.0, ge=0.0, le=1.0),
    offset: int = Body(0, ge=0),
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_reader_or_above()),
):
    """Поиск в RAG документах"""
    results = search(session, query, top_k=top_k, offset=offset)
    # Фильтруем по min_score на уровне приложения
    filtered_results = [r for r in results.get("results", []) if r.get("score", 0) >= min_score]
    return {
        "results": filtered_results,
        "next_offset": results.get("next_offset")
    }

@router.post("/{doc_id}/reindex")
def reindex_rag_document(
    doc_id: str,
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_editor_or_admin()),
):
    """Переиндексация RAG документа"""
    repo = RagRepo(session)
    doc = repo.get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    
    success = reprocess_document(session, doc_id)
    if not success:
        raise HTTPException(status_code=500, detail="Failed to start reindexing")
    
    return {"id": doc_id, "status": "reindexing_started"}

@router.post("/reindex")
def reindex_all_rag_documents(
    session: Session = Depends(db_session),
    current_user: dict = Depends(require_editor_or_admin()),
):
    """Массовая переиндексация всех RAG документов"""
    # Получаем все документы со статусом ready
    docs = session.query(RagDocuments).filter(RagDocuments.status == "ready").all()
    
    reindexed_count = 0
    for doc in docs:
        if reprocess_document(session, str(doc.id)):
            reindexed_count += 1
    
    return {"reindexed_count": reindexed_count, "total_documents": len(docs)}

# ===== app/api/routers/chats.py =====
from __future__ import annotations
from typing import Dict, Any, AsyncGenerator
import asyncio
import json
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from app.api.deps import db_session, get_current_user
from app.repositories.chats_repo import ChatsRepo
from app.schemas.chat_schemas import (
    ChatCreateRequest, ChatUpdateRequest, ChatTagsUpdateRequest,
    ChatMessageRequest, ChatMessageResponse, ChatOut, ChatMessageOut
)
from app.services.clients import llm_chat

router = APIRouter(prefix="/chats", tags=["chats"])

def _ser_chat(c) -> Dict[str, Any]:
    return {
        "id": str(c.id),
        "name": c.name,
        "tags": c.tags or [],
        "created_at": c.created_at.isoformat() if c.created_at else None,
        "updated_at": c.updated_at.isoformat() if c.updated_at else None,
        "last_message_at": c.last_message_at.isoformat() if c.last_message_at else None,
    }

def _ser_msg(m) -> Dict[str, Any]:
    content = m.content if isinstance(m.content, str) else (m.content.get("text") if isinstance(m.content, dict) else str(m.content))
    return {
        "id": str(m.id),
        "chat_id": str(m.chat_id),
        "role": m.role,
        "content": content,
        "created_at": m.created_at.isoformat() if m.created_at else None,
    }

@router.get("")
def list_chats(
    limit: int = Query(100, ge=1, le=200),
    cursor: str | None = None,
    q: str | None = None,
    session: Session = Depends(db_session),
    user: Dict[str, Any] = Depends(get_current_user),
):
    repo = ChatsRepo(session)
    items = repo.list_chats(user["id"], q=q, limit=limit)
    return {"items": [_ser_chat(c) for c in items], "next_cursor": None}

@router.post("")
def create_chat(
    request: ChatCreateRequest,
    session: Session = Depends(db_session),
    user: Dict[str, Any] = Depends(get_current_user),
):
    repo = ChatsRepo(session)
    chat = repo.create_chat(user["id"], request.name, request.tags)
    return {"chat_id": str(chat.id)}

@router.patch("/{chat_id}")
def rename_chat(
    chat_id: str,
    request: ChatUpdateRequest,
    session: Session = Depends(db_session),
    user: Dict[str, Any] = Depends(get_current_user),
):
    repo = ChatsRepo(session)
    chat = repo.get(chat_id)
    if not chat or str(chat.owner_id) != str(user["id"]):
        raise HTTPException(status_code=404, detail="not_found")
    if request.name is not None:
        repo.rename_chat(chat_id, request.name or None)
    return _ser_chat(repo.get(chat_id))

@router.put("/{chat_id}/tags")
def update_tags(
    chat_id: str,
    request: ChatTagsUpdateRequest,
    session: Session = Depends(db_session),
    user: Dict[str, Any] = Depends(get_current_user),
):
    repo = ChatsRepo(session)
    chat = repo.get(chat_id)
    if not chat or str(chat.owner_id) != str(user["id"]):
        raise HTTPException(status_code=404, detail="not_found")
    repo.update_chat_tags(chat_id, request.tags)
    return {"id": chat_id, "tags": request.tags}

@router.get("/{chat_id}/messages")
def list_messages(
    chat_id: str,
    limit: int = Query(50, ge=1, le=200),
    cursor: str | None = None,
    session: Session = Depends(db_session),
    user: Dict[str, Any] = Depends(get_current_user),
):
    repo = ChatsRepo(session)
    chat = repo.get(chat_id)
    if not chat or str(chat.owner_id) != str(user["id"]):
        raise HTTPException(status_code=404, detail="not_found")
    rows, next_cursor = repo.list_messages(chat_id, cursor=cursor, limit=limit)
    items = [_ser_msg(m) for m in rows]
    return {"items": items, "next_cursor": next_cursor}

@router.post("/{chat_id}/messages")
async def post_message(
    chat_id: str,
    request: ChatMessageRequest,
    session: Session = Depends(db_session),
    user: Dict[str, Any] = Depends(get_current_user),
):
    repo = ChatsRepo(session)
    chat = repo.get(chat_id)
    if not chat or str(chat.owner_id) != str(user["id"]):
        raise HTTPException(status_code=404, detail="not_found")

    # Store user message
    user_msg = repo.add_message(chat_id, "user", {"text": request.content})

    async def stream_resp() -> AsyncGenerator[bytes, None]:
      # Generate response with LLM (non-stream), then stream chunks to client
      messages = [{"role": "system", "content": "You are a helpful assistant."}]
      
      # Add RAG context if requested
      if request.use_rag:
          try:
              from app.services.rag_service import search
              rag_results = search(session, request.content, top_k=3)
              if rag_results.get("results"):
                  context = "\n\n".join([r.get("snippet", "") for r in rag_results["results"][:3]])
                  messages.append({"role": "system", "content": f"Context from knowledge base:\n{context}"})
          except Exception as e:
              print(f"RAG search failed: {e}")
      
      messages.append({"role": "user", "content": request.content})
      answer = llm_chat(messages)
      
      # Save assistant message
      repo.add_message(chat_id, "assistant", {"text": answer})
      # Stream by small chunks
      for i in range(0, len(answer), 100):
          chunk = answer[i:i+100]
          yield f"data: {chunk}\n\n".encode("utf-8")
          await asyncio.sleep(0)  # yield control

    if request.response_stream:
        return StreamingResponse(stream_resp(), media_type="text/event-stream")
    else:
        # non-streaming
        messages = [{"role": "system", "content": "You are a helpful assistant."}]
        
        # Add RAG context if requested
        if request.use_rag:
            try:
                from app.services.rag_service import search
                rag_results = search(session, request.content, top_k=3)
                if rag_results.get("results"):
                    context = "\n\n".join([r.get("snippet", "") for r in rag_results["results"][:3]])
                    messages.append({"role": "system", "content": f"Context from knowledge base:\n{context}"})
            except Exception as e:
                print(f"RAG search failed: {e}")
        
        messages.append({"role": "user", "content": request.content})
        answer = llm_chat(messages)
        repo.add_message(chat_id, "assistant", {"text": answer})
        return ChatMessageResponse(message_id=str(user_msg.id), content=request.content, answer=answer)

@router.delete("/{chat_id}")
def delete_chat(
    chat_id: str,
    session: Session = Depends(db_session),
    user: Dict[str, Any] = Depends(get_current_user),
):
    repo = ChatsRepo(session)
    chat = repo.get(chat_id)
    if not chat or str(chat.owner_id) != str(user["id"]):
        raise HTTPException(status_code=404, detail="not_found")
    repo.delete(chat_id)
    return {"id": chat_id, "deleted": True}

# ===== app/api/routers/analyze.py =====
# app/api/routers/analyze.py
"""
Аналитический роутер — те же правила, другой бакет:
- analysis/{uuid}/origin.{ext}, рядом canonical.txt и preview/*
- Метаданные в БД; скачивание presigned GET с оригинальным именем
"""
from __future__ import annotations
from typing import Optional
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Query
from sqlalchemy.orm import Session

from app.api.deps import db_session
from app.core.config import settings
from app.core.s3_helpers import put_object, presign_get
from app.repositories.analyze_repo import AnalyzeRepo

router = APIRouter(prefix="/analyze", tags=["analyze"])

ALLOWED_EXTENSIONS = {'.txt', '.pdf', '.doc', '.docx', '.md', '.rtf', '.odt'}

def _safe_ext(filename: Optional[str]) -> str:
    if not filename or '.' not in filename:
        return ''
    ext = '.' + filename.rsplit('.', 1)[-1].lower()
    return ext if ext in ALLOWED_EXTENSIONS else ''

@router.post("/upload")
async def upload_analysis_file(
    file: UploadFile = File(...),
    session: Session = Depends(db_session),
):
    repo = AnalyzeRepo(session)
    ext = _safe_ext(file.filename)
    if file.filename and not ext:
        raise HTTPException(status_code=400, detail=f"Unsupported file type. Allowed: {', '.join(sorted(ALLOWED_EXTENSIONS))}")
    doc = repo.create_document(
        uploaded_by=None,
        status="queued",
    )
    key = f"{doc.id}/origin{ext}"
    put_object(settings.S3_BUCKET_ANALYSIS, key, file.file, content_type=file.content_type)

    doc.url_file = key
    doc.updated_at = datetime.utcnow()
    session.commit()

    try:
        from app.tasks.upload_watch import watch as upload_watch
        upload_watch(str(doc.id), key)
    except Exception:
        pass

    return {"id": str(doc.id), "key": key, "status": "uploaded"}

@router.get("/")
def list_analysis_documents(
    session: Session = Depends(db_session),
):
    repo = AnalyzeRepo(session)
    docs = repo.list()
    return {"items": [{"id": str(doc.id), "status": doc.status, "created_at": doc.date_upload.isoformat() if doc.date_upload else None, "url_file": doc.url_file, "url_canonical_file": doc.url_canonical_file, "result": doc.result, "error": doc.error, "updated_at": doc.updated_at.isoformat() if doc.updated_at else None} for doc in docs]}

@router.get("/{doc_id}")
def get_analysis_document(
    doc_id: str,
    session: Session = Depends(db_session),
):
    """Получить информацию о документе анализа"""
    repo = AnalyzeRepo(session)
    doc = repo.get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    
    return {
        "id": str(doc.id),
        "status": doc.status,
        "created_at": doc.date_upload.isoformat() if doc.date_upload else None,
        "url_file": doc.url_file,
        "url_canonical_file": doc.url_canonical_file,
        "result": doc.result,
        "error": doc.error,
        "updated_at": doc.updated_at.isoformat() if doc.updated_at else None
    }

@router.get("/{doc_id}/download")
def download_analysis_file(
    doc_id: str,
    kind: str = Query("original", description="File type: original or canonical"),
    session: Session = Depends(db_session),
):
    """Скачать файл анализа"""
    repo = AnalyzeRepo(session)
    doc = repo.get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    
    if kind == "canonical" and not doc.url_canonical_file:
        raise HTTPException(status_code=404, detail="Canonical file not available")
    
    file_key = doc.url_canonical_file if kind == "canonical" else doc.url_file
    if not file_key:
        raise HTTPException(status_code=404, detail="File not found")
    
    url = presign_get(settings.S3_BUCKET_ANALYSIS, file_key, 3600)
    return {"url": url}

@router.delete("/{doc_id}")
def delete_analysis_document(
    doc_id: str,
    session: Session = Depends(db_session),
):
    """Удалить документ анализа"""
    repo = AnalyzeRepo(session)
    doc = repo.get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Удаляем из S3
    if doc.url_file:
        try:
            from app.core.s3 import get_minio
            minio = get_minio()
            minio.remove_object(settings.S3_BUCKET_ANALYSIS, doc.url_file)
        except Exception:
            pass  # Игнорируем ошибки удаления из S3
    
    if doc.url_canonical_file:
        try:
            from app.core.s3 import get_minio
            minio = get_minio()
            minio.remove_object(settings.S3_BUCKET_RAG, doc.url_canonical_file)
        except Exception:
            pass
    
    # Удаляем из БД
    session.delete(doc)
    session.commit()
    
    return {"id": str(doc.id), "deleted": True}

@router.post("/{doc_id}/reanalyze")
def reanalyze_document(
    doc_id: str,
    session: Session = Depends(db_session),
):
    """Повторно проанализировать документ"""
    repo = AnalyzeRepo(session)
    doc = repo.get(doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Сбрасываем статус и запускаем заново
    doc.status = "queued"
    doc.result = None
    doc.error = None
    doc.updated_at = datetime.utcnow()
    session.commit()
    
    # Запускаем задачу анализа
    try:
        from app.tasks.analyze import run as analyze_task
        analyze_task.delay(str(doc.id))
    except Exception as e:
        doc.status = "error"
        doc.error = f"Failed to start reanalysis: {str(e)}"
        session.commit()
        raise HTTPException(status_code=500, detail="Failed to start reanalysis")
    
    return {"id": str(doc.id), "status": "reanalysis_started"}
# ===== app/llm/prompts/__init__.py =====

# ===== app/migrations/versions/__init__.py =====

# ===== app/migrations/versions/20250115_000001_add_require_password_change.py =====
"""Add require_password_change field to users table

Revision ID: 20250115_000001
Revises: 20250115_000000
Create Date: 2025-01-15 00:00:01.000000

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = '20250115_000001'
down_revision = '20250115_000000'
branch_labels = None
depends_on = None


def upgrade() -> None:
    # Add require_password_change field to users table
    op.add_column('users', sa.Column('require_password_change', sa.Boolean(), nullable=False, server_default='false'))


def downgrade() -> None:
    # Remove require_password_change field from users table
    op.drop_column('users', 'require_password_change')

# ===== app/migrations/versions/20250912_104656_add_chat_tags.py =====
"""add chat tags column

Revision ID: 20250912_104656_add_chat_tags
Revises: 20250910_175628_initial
Create Date: 2025-09-12 10:46:56
"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '20250912_104656_add_chat_tags'
down_revision = '20250910_175628_initial'
branch_labels = None
depends_on = None

def upgrade() -> None:
    op.add_column('chats', sa.Column('tags', postgresql.ARRAY(sa.Text()), nullable=False, server_default='{}'))
    op.alter_column('chats', 'tags', server_default=None)

def downgrade() -> None:
    op.drop_column('chats', 'tags')

# ===== app/migrations/versions/20250115_000000_rbac_admin_models.py =====
"""rbac admin models

Revision ID: 20250115_000000_rbac_admin_models
Revises: 20250912_104656_add_chat_tags
Create Date: 2025-01-15 00:00:00
"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '20250115_000000_rbac_admin_models'
down_revision = '20250912_104656_add_chat_tags'
branch_labels = None
depends_on = None


def upgrade() -> None:
    # Drop existing ENUM type and replace with VARCHAR + CHECK constraint
    op.execute("DROP TYPE IF EXISTS role_enum CASCADE")
    
    # Add email column to users
    op.add_column('users', sa.Column('email', sa.String(255), nullable=True))
    op.create_index('ix_users_email', 'users', ['email'])
    
    # Change role column from ENUM to VARCHAR with CHECK constraint
    op.alter_column('users', 'role', type_=sa.String(20), existing_type=postgresql.ENUM('admin', 'editor', 'reader', name='role_enum'))
    op.create_check_constraint('ck_users_role', 'users', "role IN ('admin', 'editor', 'reader')")
    
    # Update user_tokens table
    op.add_column('user_tokens', sa.Column('scopes', postgresql.JSON(astext_type=sa.Text()), nullable=True))
    op.add_column('user_tokens', sa.Column('expires_at', sa.DateTime(timezone=True), nullable=True))
    op.alter_column('user_tokens', 'revoked', new_column_name='revoked_at', type_=sa.DateTime(timezone=True))
    op.alter_column('user_tokens', 'revoked_at', nullable=True)
    
    # Create password_reset_tokens table
    op.create_table('password_reset_tokens',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('token_hash', sa.Text(), nullable=False),
        sa.Column('expires_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('used_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('token_hash')
    )
    op.create_index('ix_password_reset_tokens_user_id', 'password_reset_tokens', ['user_id'])
    op.create_index('ix_password_reset_tokens_expires_at', 'password_reset_tokens', ['expires_at'])
    op.create_index('ix_password_reset_tokens_used_at', 'password_reset_tokens', ['used_at'])
    
    # Add TTL cleanup function for password reset tokens (expires after 1 hour)
    op.execute("""
        CREATE OR REPLACE FUNCTION cleanup_expired_password_reset_tokens()
        RETURNS void AS $$
        BEGIN
            DELETE FROM password_reset_tokens 
            WHERE expires_at < NOW() OR used_at IS NOT NULL;
        END;
        $$ LANGUAGE plpgsql;
    """)
    
    # Create audit_logs table
    op.create_table('audit_logs',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('ts', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
        sa.Column('actor_user_id', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('action', sa.String(50), nullable=False),
        sa.Column('object_type', sa.String(50), nullable=True),
        sa.Column('object_id', sa.String(255), nullable=True),
        sa.Column('meta', postgresql.JSON(astext_type=sa.Text()), nullable=True),
        sa.Column('ip', sa.String(45), nullable=True),
        sa.Column('user_agent', sa.Text(), nullable=True),
        sa.Column('request_id', sa.String(255), nullable=True),
        sa.ForeignKeyConstraint(['actor_user_id'], ['users.id'], ondelete='SET NULL'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_audit_logs_ts', 'audit_logs', ['ts'])
    op.create_index('ix_audit_logs_actor_user_id', 'audit_logs', ['actor_user_id'])
    op.create_index('ix_audit_logs_action', 'audit_logs', ['action'])
    op.create_index('ix_audit_logs_object_type', 'audit_logs', ['object_type'])
    op.create_index('ix_audit_logs_object_id', 'audit_logs', ['object_id'])
    op.create_index('ix_audit_logs_request_id', 'audit_logs', ['request_id'])


def downgrade() -> None:
    # Drop new tables
    op.drop_table('audit_logs')
    op.drop_table('password_reset_tokens')
    
    # Revert user_tokens changes
    op.alter_column('user_tokens', 'revoked_at', new_column_name='revoked', type_=sa.Boolean())
    op.alter_column('user_tokens', 'revoked', nullable=False, server_default=sa.text('false'))
    op.drop_column('user_tokens', 'expires_at')
    op.drop_column('user_tokens', 'scopes')
    
    # Revert users changes
    op.drop_constraint('ck_users_role', 'users', type_='check')
    op.alter_column('users', 'role', type_=postgresql.ENUM('admin', 'editor', 'reader', name='role_enum'))
    op.drop_index('ix_users_email', 'users')
    op.drop_column('users', 'email')
    
    # Recreate ENUM type
    op.execute("CREATE TYPE role_enum AS ENUM ('admin', 'editor', 'reader')")

# ===== app/migrations/versions/20250910_175628_initial.py =====
# app/migrations/versions/20250910_175628_initial.py
from __future__ import annotations
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = "20250910_175628_initial"
down_revision = "20250908_154500_add_tokens"
branch_labels = None
depends_on = None

def upgrade():
    # Use string columns instead of ENUMs to avoid conflicts
    # ENUMs will be handled by SQLAlchemy models

    # --- users ---
    op.create_table(
        "users",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("login", sa.String(length=255), nullable=False),
        sa.Column("password_hash", sa.Text(), nullable=False),
        sa.Column("role", sa.String(20), nullable=False, server_default=sa.text("'reader'")),
        sa.Column("is_active", sa.Boolean(), nullable=False, server_default=sa.text("true")),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("updated_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
    )
    op.create_index("ix_users_login", "users", ["login"], unique=True)

    # --- user_tokens ---
    op.create_table(
        "user_tokens",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("user_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("token_hash", sa.Text(), nullable=False),
        sa.Column("name", sa.String(length=255), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("last_used_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("revoked", sa.Boolean(), nullable=False, server_default=sa.text("false")),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
    )
    op.create_index("ix_user_tokens_user_id", "user_tokens", ["user_id"])

    # --- user_refresh_tokens ---
    op.create_table(
        "user_refresh_tokens",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("user_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("refresh_hash", sa.Text(), nullable=False),
        sa.Column("issued_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("expires_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("rotating", sa.Boolean(), nullable=False, server_default=sa.text("true")),
        sa.Column("revoked", sa.Boolean(), nullable=False, server_default=sa.text("false")),
        sa.Column("meta", sa.Text(), nullable=True),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.UniqueConstraint("refresh_hash", name="uq_user_refresh_tokens_refresh_hash"),
    )
    op.create_index("ix_user_refresh_tokens_user_id", "user_refresh_tokens", ["user_id"])

    # --- chats ---
    op.create_table(
        "chats",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("name", sa.String(length=255), nullable=True),
        sa.Column("owner_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("updated_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("last_message_at", sa.DateTime(timezone=True), nullable=True),
    )
    op.create_foreign_key("fk_chats_owner_id_users", "chats", "users", ["owner_id"], ["id"], ondelete="CASCADE")

    # --- chat messages ---
    op.create_table(
        "chatmessages",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("chat_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("role", sa.String(20), nullable=False),
        sa.Column("content", sa.JSON(), nullable=False),
        sa.Column("model", sa.String(length=255), nullable=True),
        sa.Column("tokens_in", sa.Integer(), nullable=True),
        sa.Column("tokens_out", sa.Integer(), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("meta", sa.JSON(), nullable=True),
        sa.ForeignKeyConstraint(["chat_id"], ["chats.id"], ondelete="CASCADE"),
    )
    op.create_index("ix_chatmessages_chat_id_created_at", "chatmessages", ["chat_id", "created_at"])

    # --- RAG documents ---
    op.create_table(
        "ragdocuments",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("name", sa.Text(), nullable=True),
        sa.Column("status", sa.String(20), nullable=False, server_default=sa.text("'uploaded'")),
        sa.Column("date_upload", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("uploaded_by", postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column("url_file", sa.Text(), nullable=True),
        sa.Column("url_canonical_file", sa.Text(), nullable=True),
        sa.Column("source_mime", sa.String(length=255), nullable=True),
        sa.Column("size_bytes", sa.BigInteger(), nullable=True),
        sa.Column("tags", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("error", sa.Text(), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=True),
    )
    op.create_foreign_key("fk_ragdocuments_uploaded_by_users", "ragdocuments", "users", ["uploaded_by"], ["id"], ondelete="SET NULL")

    # --- RAG chunks ---
    op.create_table(
        "ragchunks",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("document_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("chunk_idx", sa.Integer(), nullable=False),
        sa.Column("text", sa.Text(), nullable=False),
        sa.Column("embedding_model", sa.String(length=255), nullable=True),
        sa.Column("embedding_version", sa.String(length=255), nullable=True),
        sa.Column("date_embedding", sa.DateTime(timezone=True), nullable=True),
        sa.Column("meta", sa.JSON(), nullable=True),
        sa.Column("qdrant_point_id", postgresql.UUID(as_uuid=True), nullable=True),
        sa.ForeignKeyConstraint(["document_id"], ["ragdocuments.id"], ondelete="CASCADE"),
    )
    op.create_index("ix_ragchunks_document_id_chunk_idx", "ragchunks", ["document_id", "chunk_idx"])

    # --- Analysis documents ---
    op.create_table(
        "analysisdocuments",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("status", sa.String(20), nullable=False, server_default=sa.text("'queued'")),
        sa.Column("date_upload", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
        sa.Column("uploaded_by", postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column("url_file", sa.Text(), nullable=True),
        sa.Column("url_canonical_file", sa.Text(), nullable=True),
        sa.Column("result", sa.JSON(), nullable=True),
        sa.Column("error", sa.Text(), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=True),
    )
    op.create_foreign_key("fk_analysisdocuments_uploaded_by_users", "analysisdocuments", "users", ["uploaded_by"], ["id"], ondelete="SET NULL")

    # --- Analysis chunks ---
    op.create_table(
        "analysischunks",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column("document_id", postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column("chunk_idx", sa.Integer(), nullable=False),
        sa.Column("text", sa.Text(), nullable=False),
        sa.Column("embedding_model", sa.String(length=255), nullable=True),
        sa.Column("embedding_version", sa.String(length=255), nullable=True),
        sa.Column("date_embedding", sa.DateTime(timezone=True), nullable=True),
        sa.Column("meta", sa.JSON(), nullable=True),
        sa.ForeignKeyConstraint(["document_id"], ["analysisdocuments.id"], ondelete="CASCADE"),
    )
    op.create_index("ix_analysischunks_document_id_chunk_idx", "analysischunks", ["document_id", "chunk_idx"])

def downgrade():
    # Drop in reverse dependency order
    op.drop_index("ix_analysischunks_document_id_chunk_idx", table_name="analysischunks")
    op.drop_table("analysischunks")
    op.drop_constraint("fk_analysisdocuments_uploaded_by_users", "analysisdocuments", type_="foreignkey")
    op.drop_table("analysisdocuments")

    op.drop_index("ix_ragchunks_document_id_chunk_idx", table_name="ragchunks")
    op.drop_table("ragchunks")
    op.drop_constraint("fk_ragdocuments_uploaded_by_users", "ragdocuments", type_="foreignkey")
    op.drop_table("ragdocuments")

    op.drop_index("ix_chatmessages_chat_id_created_at", table_name="chatmessages")
    op.drop_table("chatmessages")
    op.drop_constraint("fk_chats_owner_id_users", "chats", type_="foreignkey")
    op.drop_table("chats")

    # Drop user-related tables
    op.drop_index("ix_user_refresh_tokens_user_id", table_name="user_refresh_tokens")
    op.drop_table("user_refresh_tokens")
    op.drop_index("ix_user_tokens_user_id", table_name="user_tokens")
    op.drop_table("user_tokens")
    op.drop_index("ix_users_login", table_name="users")
    op.drop_table("users")

    # ENUMs will be dropped automatically when tables are dropped

# ===== app/migrations/versions/20250908_154500_add_tokens.py =====
# app/migrations/versions/20250908_154500_add_tokens.py
"""Compatibility shim: previously existed in DB history.
This migration is intentionally a no-op to satisfy databases already stamped
with this revision id.
"""
from __future__ import annotations
from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision = "20250908_154500_add_tokens"
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    # No operation on purpose
    pass

def downgrade():
    # No operation on purpose
    pass
